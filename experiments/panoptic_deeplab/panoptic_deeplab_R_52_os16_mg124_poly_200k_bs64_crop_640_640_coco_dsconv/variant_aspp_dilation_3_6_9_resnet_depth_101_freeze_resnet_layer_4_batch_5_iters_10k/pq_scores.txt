env: DETECTRON2_DATASETS=/content/datasets
Command Line Args: Namespace(config_file='./detectron2/projects/Panoptic-DeepLab/configs/COCO-PanopticSegmentation/panoptic_deeplab_R_52_os16_mg124_poly_200k_bs64_crop_640_640_coco_dsconv.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.RESNETS.NORM', 'BN', 'MODEL.INS_EMBED_HEAD.NORM', 'BN', 'MODEL.SEM_SEG_HEAD.NORM', 'BN', 'SOLVER.MAX_ITER', '10000', 'SOLVER.IMS_PER_BATCH', '5', 'SOLVER.BASE_LR', '0.0025', 'MODEL.BACKBONE.FREEZE_AT', '4', 'MODEL.RESNETS.DEPTH', '101', 'MODEL.WEIGHTS', 'detectron2://DeepLab/R-103.pkl', 'MODEL.SEM_SEG_HEAD.ASPP_DILATIONS', '[3, 6, 9]', 'MODEL.INS_EMBED_HEAD.ASPP_DILATIONS', '[3, 6, 9]'], resume=False)
Loading config ./detectron2/projects/Panoptic-DeepLab/configs/COCO-PanopticSegmentation/panoptic_deeplab_R_52_os16_mg124_poly_200k_bs64_crop_640_640_coco_dsconv.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
[12/11 23:03:19 detectron2]: Rank of current process: 0. World size: 1
[12/11 23:03:20 detectron2]: Environment info:
----------------------  ----------------------------------------------------------------
sys.platform            linux
Python                  3.7.12 (default, Jan 15 2022, 18:48:18) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/content/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.1
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.10.0+cu111 @/usr/local/lib/python3.7/dist-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   Tesla V100-SXM2-16GB (arch=7.0)
Driver version          460.32.03
CUDA_HOME               /usr/local/cuda
Pillow                  7.1.2
torchvision             0.11.1+cu111 @/usr/local/lib/python3.7/dist-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.1.2
----------------------  ----------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[12/11 23:03:20 detectron2]: Command line arguments: Namespace(config_file='./detectron2/projects/Panoptic-DeepLab/configs/COCO-PanopticSegmentation/panoptic_deeplab_R_52_os16_mg124_poly_200k_bs64_crop_640_640_coco_dsconv.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.RESNETS.NORM', 'BN', 'MODEL.INS_EMBED_HEAD.NORM', 'BN', 'MODEL.SEM_SEG_HEAD.NORM', 'BN', 'SOLVER.MAX_ITER', '10000', 'SOLVER.IMS_PER_BATCH', '5', 'SOLVER.BASE_LR', '0.0025', 'MODEL.BACKBONE.FREEZE_AT', '4', 'MODEL.RESNETS.DEPTH', '101', 'MODEL.WEIGHTS', 'detectron2://DeepLab/R-103.pkl', 'MODEL.SEM_SEG_HEAD.ASPP_DILATIONS', '[3, 6, 9]', 'MODEL.INS_EMBED_HEAD.ASPP_DILATIONS', '[3, 6, 9]'], resume=False)
[12/11 23:03:20 detectron2]: Contents of args.config_file=./detectron2/projects/Panoptic-DeepLab/configs/COCO-PanopticSegmentation/panoptic_deeplab_R_52_os16_mg124_poly_200k_bs64_crop_640_640_coco_dsconv.yaml:
_BASE_: ../Cityscapes-PanopticSegmentation/Base-PanopticDeepLab-OS16.yaml
MODEL:
  WEIGHTS: "detectron2://DeepLab/R-52.pkl"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  BACKBONE:
    NAME: "build_resnet_deeplab_backbone"
  RESNETS:
    DEPTH: 50
    NORM: "SyncBN"
    RES5_MULTI_GRID: [1, 2, 4]
    STEM_TYPE: "deeplab"
    STEM_OUT_CHANNELS: 128
    STRIDE_IN_1X1: False
  SEM_SEG_HEAD:
    NUM_CLASSES: 133
    LOSS_TOP_K: 1.0
    USE_DEPTHWISE_SEPARABLE_CONV: True
  PANOPTIC_DEEPLAB:
    STUFF_AREA: 4096
    NMS_KERNEL: 41
    SIZE_DIVISIBILITY: 640
    USE_DEPTHWISE_SEPARABLE_CONV: True
DATASETS:
  TRAIN: ("coco_2017_train_panoptic",)
  TEST: ("coco_2017_val_panoptic",)
SOLVER:
  BASE_LR: 0.0005
  MAX_ITER: 200000
  IMS_PER_BATCH: 64
INPUT:
  FORMAT: "RGB"
  GAUSSIAN_SIGMA: 8
  MIN_SIZE_TRAIN: !!python/object/apply:eval ["[int(x * 0.1 * 640) for x in range(5, 16)]"]
  MIN_SIZE_TRAIN_SAMPLING: "choice"
  MIN_SIZE_TEST: 640
  MAX_SIZE_TRAIN: 960
  MAX_SIZE_TEST: 640
  CROP:
    ENABLED: True
    TYPE: "absolute"
    SIZE: (640, 640)

[12/11 23:03:20 detectron2]: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 10
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_val_panoptic
  TRAIN:
  - coco_2017_train_panoptic
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: true
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 640
    - 640
    TYPE: absolute
  FORMAT: RGB
  GAUSSIAN_SIGMA: 8
  IGNORE_CROWD_IN_SEMANTIC: false
  IGNORE_STUFF_IN_OFFSET: true
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 640
  MAX_SIZE_TRAIN: 960
  MIN_SIZE_TEST: 640
  MIN_SIZE_TRAIN:
  - 320
  - 384
  - 448
  - 512
  - 576
  - 640
  - 704
  - 768
  - 832
  - 896
  - 960
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SMALL_INSTANCE_AREA: 4096
  SMALL_INSTANCE_WEIGHT: 3
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 4
    NAME: build_resnet_deeplab_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  INS_EMBED_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 3
    - 6
    - 9
    ASPP_DROPOUT: 0.1
    CENTER_LOSS_WEIGHT: 200.0
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    HEAD_CHANNELS: 32
    IN_FEATURES:
    - res2
    - res3
    - res5
    NAME: PanopticDeepLabInsEmbedHead
    NORM: BN
    OFFSET_LOSS_WEIGHT: 0.01
    PROJECT_CHANNELS:
    - 32
    - 64
    PROJECT_FEATURES:
    - res2
    - res3
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: PanopticDeepLab
  PANOPTIC_DEEPLAB:
    BENCHMARK_NETWORK_SPEED: false
    CENTER_THRESHOLD: 0.1
    NMS_KERNEL: 41
    PREDICT_INSTANCES: true
    SIZE_DIVISIBILITY: 640
    STUFF_AREA: 4096
    TOP_K_INSTANCE: 200
    USE_DEPTHWISE_SEPARABLE_CONV: true
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 101
    NORM: BN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 2
    RES5_MULTI_GRID:
    - 1
    - 2
    - 4
    STEM_OUT_CHANNELS: 128
    STEM_TYPE: deeplab
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id001
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 3
    - 6
    - 9
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    HEAD_CHANNELS: 256
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res5
    LOSS_TOP_K: 1.0
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    NAME: PanopticDeepLabSemSegHead
    NORM: BN
    NUM_CLASSES: 133
    PROJECT_CHANNELS:
    - 32
    - 64
    PROJECT_FEATURES:
    - res2
    - res3
    USE_DEPTHWISE_SEPARABLE_CONV: true
  WEIGHTS: detectron2://DeepLab/R-103.pkl
OUTPUT_DIR: ./output
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.0025
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: false
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 5
  LR_SCHEDULER_NAME: WarmupPolyLR
  MAX_ITER: 10000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAM
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[12/11 23:03:20 detectron2]: Full config saved to ./output/config.yaml
[12/11 23:03:20 d2.utils.env]: Using a generated random seed 20655427
[12/11 23:03:25 d2.engine.defaults]: Model:
PanopticDeepLab(
  (backbone): ResNet(
    (stem): DeepLabStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv3): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False
          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False
          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8), bias=False
          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): PanopticDeepLabSemSegHead(
    (decoder): ModuleDict(
      (res2): ModuleDict(
        (project_conv): Conv2d(
          256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (fuse_conv): DepthwiseSeparableConv2d(
          (depthwise): Conv2d(
            288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False
            (norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pointwise): Conv2d(
            288, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (res3): ModuleDict(
        (project_conv): Conv2d(
          512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (fuse_conv): DepthwiseSeparableConv2d(
          (depthwise): Conv2d(
            320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320, bias=False
            (norm): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pointwise): Conv2d(
            320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (res5): ModuleDict(
        (project_conv): ASPP(
          (convs): ModuleList(
            (0): Conv2d(
              2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): DepthwiseSeparableConv2d(
              (depthwise): Conv2d(
                2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3), groups=2048, bias=False
                (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (pointwise): Conv2d(
                2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): DepthwiseSeparableConv2d(
              (depthwise): Conv2d(
                2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), groups=2048, bias=False
                (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (pointwise): Conv2d(
                2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (3): DepthwiseSeparableConv2d(
              (depthwise): Conv2d(
                2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(9, 9), dilation=(9, 9), groups=2048, bias=False
                (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (pointwise): Conv2d(
                2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (4): Sequential(
              (0): AvgPool2d(kernel_size=(40, 40), stride=1, padding=0)
              (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (project): Conv2d(
            1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (fuse_conv): None
      )
    )
    (head): DepthwiseSeparableConv2d(
      (depthwise): Conv2d(
        256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256, bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (pointwise): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (predictor): Conv2d(256, 133, kernel_size=(1, 1), stride=(1, 1))
    (loss): DeepLabCE(
      (criterion): CrossEntropyLoss()
    )
  )
  (ins_embed_head): PanopticDeepLabInsEmbedHead(
    (decoder): ModuleDict(
      (res2): ModuleDict(
        (project_conv): Conv2d(
          256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (fuse_conv): DepthwiseSeparableConv2d(
          (depthwise): Conv2d(
            160, 160, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=160, bias=False
            (norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pointwise): Conv2d(
            160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (res3): ModuleDict(
        (project_conv): Conv2d(
          512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (fuse_conv): DepthwiseSeparableConv2d(
          (depthwise): Conv2d(
            320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320, bias=False
            (norm): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pointwise): Conv2d(
            320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (res5): ModuleDict(
        (project_conv): ASPP(
          (convs): ModuleList(
            (0): Conv2d(
              2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): DepthwiseSeparableConv2d(
              (depthwise): Conv2d(
                2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3), groups=2048, bias=False
                (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (pointwise): Conv2d(
                2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): DepthwiseSeparableConv2d(
              (depthwise): Conv2d(
                2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), groups=2048, bias=False
                (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (pointwise): Conv2d(
                2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (3): DepthwiseSeparableConv2d(
              (depthwise): Conv2d(
                2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(9, 9), dilation=(9, 9), groups=2048, bias=False
                (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (pointwise): Conv2d(
                2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (4): Sequential(
              (0): AvgPool2d(kernel_size=(40, 40), stride=1, padding=0)
              (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (project): Conv2d(
            1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (fuse_conv): None
      )
    )
    (center_head): Sequential(
      (0): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): Conv2d(
        128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (center_predictor): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))
    (offset_head): DepthwiseSeparableConv2d(
      (depthwise): Conv2d(
        128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=128, bias=False
        (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (pointwise): Conv2d(
        128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (offset_predictor): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))
    (center_loss): MSELoss()
    (offset_loss): L1Loss()
  )
)
[12/11 23:03:25 d2.projects.panoptic_deeplab.dataset_mapper]: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(320, 384, 448, 512, 576, 640, 704, 768, 832, 896, 960), max_size=960, sample_style='choice'), RandomCrop(crop_type='absolute', crop_size=[640, 640]), RandomFlip()]
[12/11 23:03:32 d2.data.build]: Using training sampler TrainingSampler
[12/11 23:03:32 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[12/11 23:03:32 d2.data.common]: Serializing 118287 elements to byte tensors and concatenating them all ...
[12/11 23:03:33 d2.data.common]: Serialized dataset takes 80.32 MiB
/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
[12/11 23:03:36 fvcore.common.checkpoint]: [Checkpointer] Loading from detectron2://DeepLab/R-103.pkl ...
[12/11 23:03:36 fvcore.common.checkpoint]: Reading a file from 'torchvision'
[12/11 23:03:36 d2.checkpoint.c2_model_loading]: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint                                                                                        | Shapes                                             |
|:------------------|:-----------------------------------------------------------------------------------------------------------|:---------------------------------------------------|
| res2.0.conv1.*    | res2.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (64,) (64,) (64,) (64,) (64,128,1,1)               |
| res2.0.conv2.*    | res2.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (64,) (64,) (64,) (64,) (64,64,3,3)                |
| res2.0.conv3.*    | res2.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,64,1,1)           |
| res2.0.shortcut.* | res2.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                          | (256,) (256,) (256,) (256,) (256,128,1,1)          |
| res2.1.conv1.*    | res2.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (64,) (64,) (64,) (64,) (64,256,1,1)               |
| res2.1.conv2.*    | res2.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (64,) (64,) (64,) (64,) (64,64,3,3)                |
| res2.1.conv3.*    | res2.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,64,1,1)           |
| res2.2.conv1.*    | res2.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (64,) (64,) (64,) (64,) (64,256,1,1)               |
| res2.2.conv2.*    | res2.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (64,) (64,) (64,) (64,) (64,64,3,3)                |
| res2.2.conv3.*    | res2.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,64,1,1)           |
| res3.0.conv1.*    | res3.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (128,) (128,) (128,) (128,) (128,256,1,1)          |
| res3.0.conv2.*    | res3.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (128,) (128,) (128,) (128,) (128,128,3,3)          |
| res3.0.conv3.*    | res3.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (512,) (512,) (512,) (512,) (512,128,1,1)          |
| res3.0.shortcut.* | res3.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                          | (512,) (512,) (512,) (512,) (512,256,1,1)          |
| res3.1.conv1.*    | res3.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (128,) (128,) (128,) (128,) (128,512,1,1)          |
| res3.1.conv2.*    | res3.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (128,) (128,) (128,) (128,) (128,128,3,3)          |
| res3.1.conv3.*    | res3.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (512,) (512,) (512,) (512,) (512,128,1,1)          |
| res3.2.conv1.*    | res3.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (128,) (128,) (128,) (128,) (128,512,1,1)          |
| res3.2.conv2.*    | res3.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (128,) (128,) (128,) (128,) (128,128,3,3)          |
| res3.2.conv3.*    | res3.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (512,) (512,) (512,) (512,) (512,128,1,1)          |
| res3.3.conv1.*    | res3.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (128,) (128,) (128,) (128,) (128,512,1,1)          |
| res3.3.conv2.*    | res3.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (128,) (128,) (128,) (128,) (128,128,3,3)          |
| res3.3.conv3.*    | res3.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (512,) (512,) (512,) (512,) (512,128,1,1)          |
| res4.0.conv1.*    | res4.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,512,1,1)          |
| res4.0.conv2.*    | res4.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.0.conv3.*    | res4.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.0.shortcut.* | res4.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                          | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)     |
| res4.1.conv1.*    | res4.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.1.conv2.*    | res4.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.1.conv3.*    | res4.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.10.conv1.*   | res4.10.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.10.conv2.*   | res4.10.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.10.conv3.*   | res4.10.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.11.conv1.*   | res4.11.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.11.conv2.*   | res4.11.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.11.conv3.*   | res4.11.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.12.conv1.*   | res4.12.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.12.conv2.*   | res4.12.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.12.conv3.*   | res4.12.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.13.conv1.*   | res4.13.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.13.conv2.*   | res4.13.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.13.conv3.*   | res4.13.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.14.conv1.*   | res4.14.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.14.conv2.*   | res4.14.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.14.conv3.*   | res4.14.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.15.conv1.*   | res4.15.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.15.conv2.*   | res4.15.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.15.conv3.*   | res4.15.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.16.conv1.*   | res4.16.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.16.conv2.*   | res4.16.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.16.conv3.*   | res4.16.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.17.conv1.*   | res4.17.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.17.conv2.*   | res4.17.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.17.conv3.*   | res4.17.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.18.conv1.*   | res4.18.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.18.conv2.*   | res4.18.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.18.conv3.*   | res4.18.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.19.conv1.*   | res4.19.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.19.conv2.*   | res4.19.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.19.conv3.*   | res4.19.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.2.conv1.*    | res4.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.2.conv2.*    | res4.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.2.conv3.*    | res4.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.20.conv1.*   | res4.20.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.20.conv2.*   | res4.20.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.20.conv3.*   | res4.20.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.21.conv1.*   | res4.21.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.21.conv2.*   | res4.21.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.21.conv3.*   | res4.21.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.22.conv1.*   | res4.22.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.22.conv2.*   | res4.22.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.22.conv3.*   | res4.22.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.3.conv1.*    | res4.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.3.conv2.*    | res4.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.3.conv3.*    | res4.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.4.conv1.*    | res4.4.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.4.conv2.*    | res4.4.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.4.conv3.*    | res4.4.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.5.conv1.*    | res4.5.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.5.conv2.*    | res4.5.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.5.conv3.*    | res4.5.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.6.conv1.*    | res4.6.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.6.conv2.*    | res4.6.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.6.conv3.*    | res4.6.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.7.conv1.*    | res4.7.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.7.conv2.*    | res4.7.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.7.conv3.*    | res4.7.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.8.conv1.*    | res4.8.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.8.conv2.*    | res4.8.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.8.conv3.*    | res4.8.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.9.conv1.*    | res4.9.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.9.conv2.*    | res4.9.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.9.conv3.*    | res4.9.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res5.0.conv1.*    | res5.0.conv1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) () (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5.0.conv2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) () (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5.0.conv3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) () (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5.0.shortcut.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight} | (2048,) () (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5.1.conv1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) () (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5.1.conv2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) () (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5.1.conv3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) () (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5.2.conv1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) () (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5.2.conv2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) () (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5.2.conv3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) () (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.*      | stem.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                               | (64,) (64,) (64,) (64,) (64,3,3,3)                 |
| stem.conv2.*      | stem.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                               | (64,) (64,) (64,) (64,) (64,64,3,3)                |
| stem.conv3.*      | stem.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                               | (128,) (128,) (128,) (128,) (128,64,3,3)           |
WARNING [12/11 23:03:37 fvcore.common.checkpoint]: Some model parameters or buffers are not found in the checkpoint:
ins_embed_head.center_head.0.norm.{bias, running_mean, running_var, weight}
ins_embed_head.center_head.0.weight
ins_embed_head.center_head.1.norm.{bias, running_mean, running_var, weight}
ins_embed_head.center_head.1.weight
ins_embed_head.center_predictor.{bias, weight}
ins_embed_head.decoder.res2.fuse_conv.depthwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res2.fuse_conv.depthwise.weight
ins_embed_head.decoder.res2.fuse_conv.pointwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res2.fuse_conv.pointwise.weight
ins_embed_head.decoder.res2.project_conv.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res2.project_conv.weight
ins_embed_head.decoder.res3.fuse_conv.depthwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res3.fuse_conv.depthwise.weight
ins_embed_head.decoder.res3.fuse_conv.pointwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res3.fuse_conv.pointwise.weight
ins_embed_head.decoder.res3.project_conv.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res3.project_conv.weight
ins_embed_head.decoder.res5.project_conv.convs.0.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res5.project_conv.convs.0.weight
ins_embed_head.decoder.res5.project_conv.convs.1.depthwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res5.project_conv.convs.1.depthwise.weight
ins_embed_head.decoder.res5.project_conv.convs.1.pointwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res5.project_conv.convs.1.pointwise.weight
ins_embed_head.decoder.res5.project_conv.convs.2.depthwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res5.project_conv.convs.2.depthwise.weight
ins_embed_head.decoder.res5.project_conv.convs.2.pointwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res5.project_conv.convs.2.pointwise.weight
ins_embed_head.decoder.res5.project_conv.convs.3.depthwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res5.project_conv.convs.3.depthwise.weight
ins_embed_head.decoder.res5.project_conv.convs.3.pointwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res5.project_conv.convs.3.pointwise.weight
ins_embed_head.decoder.res5.project_conv.convs.4.1.{bias, weight}
ins_embed_head.decoder.res5.project_conv.project.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res5.project_conv.project.weight
ins_embed_head.offset_head.depthwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.offset_head.depthwise.weight
ins_embed_head.offset_head.pointwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.offset_head.pointwise.weight
ins_embed_head.offset_predictor.{bias, weight}
sem_seg_head.decoder.res2.fuse_conv.depthwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res2.fuse_conv.depthwise.weight
sem_seg_head.decoder.res2.fuse_conv.pointwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res2.fuse_conv.pointwise.weight
sem_seg_head.decoder.res2.project_conv.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res2.project_conv.weight
sem_seg_head.decoder.res3.fuse_conv.depthwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res3.fuse_conv.depthwise.weight
sem_seg_head.decoder.res3.fuse_conv.pointwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res3.fuse_conv.pointwise.weight
sem_seg_head.decoder.res3.project_conv.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res3.project_conv.weight
sem_seg_head.decoder.res5.project_conv.convs.0.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res5.project_conv.convs.0.weight
sem_seg_head.decoder.res5.project_conv.convs.1.depthwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res5.project_conv.convs.1.depthwise.weight
sem_seg_head.decoder.res5.project_conv.convs.1.pointwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res5.project_conv.convs.1.pointwise.weight
sem_seg_head.decoder.res5.project_conv.convs.2.depthwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res5.project_conv.convs.2.depthwise.weight
sem_seg_head.decoder.res5.project_conv.convs.2.pointwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res5.project_conv.convs.2.pointwise.weight
sem_seg_head.decoder.res5.project_conv.convs.3.depthwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res5.project_conv.convs.3.depthwise.weight
sem_seg_head.decoder.res5.project_conv.convs.3.pointwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res5.project_conv.convs.3.pointwise.weight
sem_seg_head.decoder.res5.project_conv.convs.4.1.{bias, weight}
sem_seg_head.decoder.res5.project_conv.project.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res5.project_conv.project.weight
sem_seg_head.head.depthwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.head.depthwise.weight
sem_seg_head.head.pointwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.head.pointwise.weight
sem_seg_head.predictor.{bias, weight}
WARNING [12/11 23:03:37 fvcore.common.checkpoint]: The checkpoint state_dict contains keys that are not used by the model:
  res2.0.conv1.norm.num_batches_tracked
  res2.0.conv2.norm.num_batches_tracked
  res2.0.conv3.norm.num_batches_tracked
  res2.0.shortcut.norm.num_batches_tracked
  res2.1.conv1.norm.num_batches_tracked
  res2.1.conv2.norm.num_batches_tracked
  res2.1.conv3.norm.num_batches_tracked
  res2.2.conv1.norm.num_batches_tracked
  res2.2.conv2.norm.num_batches_tracked
  res2.2.conv3.norm.num_batches_tracked
  res3.0.conv1.norm.num_batches_tracked
  res3.0.conv2.norm.num_batches_tracked
  res3.0.conv3.norm.num_batches_tracked
  res3.0.shortcut.norm.num_batches_tracked
  res3.1.conv1.norm.num_batches_tracked
  res3.1.conv2.norm.num_batches_tracked
  res3.1.conv3.norm.num_batches_tracked
  res3.2.conv1.norm.num_batches_tracked
  res3.2.conv2.norm.num_batches_tracked
  res3.2.conv3.norm.num_batches_tracked
  res3.3.conv1.norm.num_batches_tracked
  res3.3.conv2.norm.num_batches_tracked
  res3.3.conv3.norm.num_batches_tracked
  res4.0.conv1.norm.num_batches_tracked
  res4.0.conv2.norm.num_batches_tracked
  res4.0.conv3.norm.num_batches_tracked
  res4.0.shortcut.norm.num_batches_tracked
  res4.1.conv1.norm.num_batches_tracked
  res4.1.conv2.norm.num_batches_tracked
  res4.1.conv3.norm.num_batches_tracked
  res4.10.conv1.norm.num_batches_tracked
  res4.10.conv2.norm.num_batches_tracked
  res4.10.conv3.norm.num_batches_tracked
  res4.11.conv1.norm.num_batches_tracked
  res4.11.conv2.norm.num_batches_tracked
  res4.11.conv3.norm.num_batches_tracked
  res4.12.conv1.norm.num_batches_tracked
  res4.12.conv2.norm.num_batches_tracked
  res4.12.conv3.norm.num_batches_tracked
  res4.13.conv1.norm.num_batches_tracked
  res4.13.conv2.norm.num_batches_tracked
  res4.13.conv3.norm.num_batches_tracked
  res4.14.conv1.norm.num_batches_tracked
  res4.14.conv2.norm.num_batches_tracked
  res4.14.conv3.norm.num_batches_tracked
  res4.15.conv1.norm.num_batches_tracked
  res4.15.conv2.norm.num_batches_tracked
  res4.15.conv3.norm.num_batches_tracked
  res4.16.conv1.norm.num_batches_tracked
  res4.16.conv2.norm.num_batches_tracked
  res4.16.conv3.norm.num_batches_tracked
  res4.17.conv1.norm.num_batches_tracked
  res4.17.conv2.norm.num_batches_tracked
  res4.17.conv3.norm.num_batches_tracked
  res4.18.conv1.norm.num_batches_tracked
  res4.18.conv2.norm.num_batches_tracked
  res4.18.conv3.norm.num_batches_tracked
  res4.19.conv1.norm.num_batches_tracked
  res4.19.conv2.norm.num_batches_tracked
  res4.19.conv3.norm.num_batches_tracked
  res4.2.conv1.norm.num_batches_tracked
  res4.2.conv2.norm.num_batches_tracked
  res4.2.conv3.norm.num_batches_tracked
  res4.20.conv1.norm.num_batches_tracked
  res4.20.conv2.norm.num_batches_tracked
  res4.20.conv3.norm.num_batches_tracked
  res4.21.conv1.norm.num_batches_tracked
  res4.21.conv2.norm.num_batches_tracked
  res4.21.conv3.norm.num_batches_tracked
  res4.22.conv1.norm.num_batches_tracked
  res4.22.conv2.norm.num_batches_tracked
  res4.22.conv3.norm.num_batches_tracked
  res4.3.conv1.norm.num_batches_tracked
  res4.3.conv2.norm.num_batches_tracked
  res4.3.conv3.norm.num_batches_tracked
  res4.4.conv1.norm.num_batches_tracked
  res4.4.conv2.norm.num_batches_tracked
  res4.4.conv3.norm.num_batches_tracked
  res4.5.conv1.norm.num_batches_tracked
  res4.5.conv2.norm.num_batches_tracked
  res4.5.conv3.norm.num_batches_tracked
  res4.6.conv1.norm.num_batches_tracked
  res4.6.conv2.norm.num_batches_tracked
  res4.6.conv3.norm.num_batches_tracked
  res4.7.conv1.norm.num_batches_tracked
  res4.7.conv2.norm.num_batches_tracked
  res4.7.conv3.norm.num_batches_tracked
  res4.8.conv1.norm.num_batches_tracked
  res4.8.conv2.norm.num_batches_tracked
  res4.8.conv3.norm.num_batches_tracked
  res4.9.conv1.norm.num_batches_tracked
  res4.9.conv2.norm.num_batches_tracked
  res4.9.conv3.norm.num_batches_tracked
  stem.conv1.norm.num_batches_tracked
  stem.conv2.norm.num_batches_tracked
  stem.conv3.norm.num_batches_tracked
  stem.fc.{bias, weight}
[12/11 23:03:37 d2.engine.train_loop]: Starting training from iteration 0
/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
[12/11 23:03:45 d2.utils.events]:  eta: 0:58:05  iter: 19  total_loss: 6.259  loss_sem_seg: 3.731  loss_center: 0.7928  loss_offset: 1.65  time: 0.3506  data_time: 0.0629  lr: 4.9867e-05  max_mem: 7418M
[12/11 23:03:52 d2.utils.events]:  eta: 0:58:00  iter: 39  total_loss: 6.219  loss_sem_seg: 3.89  loss_center: 0.6086  loss_offset: 1.559  time: 0.3517  data_time: 0.0283  lr: 9.9552e-05  max_mem: 7418M
[12/11 23:03:59 d2.utils.events]:  eta: 0:57:59  iter: 59  total_loss: 5.926  loss_sem_seg: 3.618  loss_center: 0.7036  loss_offset: 1.741  time: 0.3501  data_time: 0.0242  lr: 0.00014906  max_mem: 7418M
[12/11 23:04:06 d2.utils.events]:  eta: 0:57:48  iter: 79  total_loss: 5.807  loss_sem_seg: 3.242  loss_center: 0.7246  loss_offset: 1.661  time: 0.3494  data_time: 0.0259  lr: 0.00019838  max_mem: 7418M
[12/11 23:04:13 d2.utils.events]:  eta: 0:57:46  iter: 99  total_loss: 5.569  loss_sem_seg: 3.378  loss_center: 0.7002  loss_offset: 1.531  time: 0.3498  data_time: 0.0259  lr: 0.00024753  max_mem: 7418M
[12/11 23:04:20 d2.utils.events]:  eta: 0:57:38  iter: 119  total_loss: 5.49  loss_sem_seg: 3.216  loss_center: 0.6118  loss_offset: 1.579  time: 0.3499  data_time: 0.0254  lr: 0.00029649  max_mem: 7418M
[12/11 23:04:27 d2.utils.events]:  eta: 0:57:23  iter: 139  total_loss: 5.189  loss_sem_seg: 2.842  loss_center: 0.7565  loss_offset: 1.585  time: 0.3492  data_time: 0.0243  lr: 0.00034528  max_mem: 7418M
[12/11 23:04:34 d2.utils.events]:  eta: 0:57:16  iter: 159  total_loss: 4.772  loss_sem_seg: 2.55  loss_center: 0.5256  loss_offset: 1.455  time: 0.3492  data_time: 0.0248  lr: 0.00039388  max_mem: 7418M
[12/11 23:04:41 d2.utils.events]:  eta: 0:57:00  iter: 179  total_loss: 4.654  loss_sem_seg: 2.587  loss_center: 0.5881  loss_offset: 1.701  time: 0.3488  data_time: 0.0240  lr: 0.0004423  max_mem: 7418M
[12/11 23:04:48 d2.utils.events]:  eta: 0:56:59  iter: 199  total_loss: 5.106  loss_sem_seg: 2.679  loss_center: 0.725  loss_offset: 1.649  time: 0.3489  data_time: 0.0265  lr: 0.00049055  max_mem: 7418M
[12/11 23:04:55 d2.utils.events]:  eta: 0:56:50  iter: 219  total_loss: 4.254  loss_sem_seg: 2.543  loss_center: 0.6612  loss_offset: 1.34  time: 0.3488  data_time: 0.0248  lr: 0.00053861  max_mem: 7418M
[12/11 23:05:01 d2.utils.events]:  eta: 0:56:37  iter: 239  total_loss: 4.717  loss_sem_seg: 2.371  loss_center: 0.5722  loss_offset: 1.627  time: 0.3487  data_time: 0.0240  lr: 0.00058649  max_mem: 7418M
[12/11 23:05:08 d2.utils.events]:  eta: 0:56:33  iter: 259  total_loss: 4.745  loss_sem_seg: 2.489  loss_center: 0.6043  loss_offset: 1.548  time: 0.3486  data_time: 0.0262  lr: 0.0006342  max_mem: 7418M
[12/11 23:05:15 d2.utils.events]:  eta: 0:56:26  iter: 279  total_loss: 4.348  loss_sem_seg: 2.215  loss_center: 0.6442  loss_offset: 1.481  time: 0.3486  data_time: 0.0255  lr: 0.00068172  max_mem: 7418M
[12/11 23:05:22 d2.utils.events]:  eta: 0:56:16  iter: 299  total_loss: 3.969  loss_sem_seg: 2.126  loss_center: 0.5965  loss_offset: 1.405  time: 0.3485  data_time: 0.0257  lr: 0.00072906  max_mem: 7418M
[12/11 23:05:29 d2.utils.events]:  eta: 0:56:09  iter: 319  total_loss: 4.32  loss_sem_seg: 2.132  loss_center: 0.8104  loss_offset: 1.313  time: 0.3484  data_time: 0.0242  lr: 0.00077622  max_mem: 7418M
[12/11 23:05:36 d2.utils.events]:  eta: 0:56:02  iter: 339  total_loss: 4.08  loss_sem_seg: 2.09  loss_center: 0.6189  loss_offset: 1.188  time: 0.3485  data_time: 0.0258  lr: 0.0008232  max_mem: 7418M
[12/11 23:05:43 d2.utils.events]:  eta: 0:55:58  iter: 359  total_loss: 4.222  loss_sem_seg: 2.336  loss_center: 0.695  loss_offset: 1.281  time: 0.3487  data_time: 0.0255  lr: 0.00087  max_mem: 7418M
[12/11 23:05:50 d2.utils.events]:  eta: 0:55:49  iter: 379  total_loss: 3.982  loss_sem_seg: 2.163  loss_center: 0.635  loss_offset: 1.283  time: 0.3487  data_time: 0.0249  lr: 0.00091662  max_mem: 7418M
[12/11 23:05:57 d2.utils.events]:  eta: 0:55:41  iter: 399  total_loss: 4.088  loss_sem_seg: 2.076  loss_center: 0.5527  loss_offset: 1.312  time: 0.3487  data_time: 0.0249  lr: 0.00096306  max_mem: 7418M
[12/11 23:06:04 d2.utils.events]:  eta: 0:55:35  iter: 419  total_loss: 3.771  loss_sem_seg: 2.025  loss_center: 0.6101  loss_offset: 1.167  time: 0.3487  data_time: 0.0258  lr: 0.0010093  max_mem: 7418M
[12/11 23:06:11 d2.utils.events]:  eta: 0:55:31  iter: 439  total_loss: 3.88  loss_sem_seg: 2.056  loss_center: 0.6271  loss_offset: 1.22  time: 0.3488  data_time: 0.0256  lr: 0.0010554  max_mem: 7418M
[12/11 23:06:18 d2.utils.events]:  eta: 0:55:25  iter: 459  total_loss: 3.853  loss_sem_seg: 2.06  loss_center: 0.5816  loss_offset: 1.144  time: 0.3489  data_time: 0.0258  lr: 0.0011013  max_mem: 7418M
[12/11 23:06:25 d2.utils.events]:  eta: 0:55:17  iter: 479  total_loss: 4.155  loss_sem_seg: 2.344  loss_center: 0.6629  loss_offset: 1.078  time: 0.3488  data_time: 0.0244  lr: 0.001147  max_mem: 7418M
[12/11 23:06:32 d2.utils.events]:  eta: 0:55:07  iter: 499  total_loss: 3.667  loss_sem_seg: 1.959  loss_center: 0.5713  loss_offset: 1.114  time: 0.3487  data_time: 0.0249  lr: 0.0011925  max_mem: 7418M
[12/11 23:06:39 d2.utils.events]:  eta: 0:55:00  iter: 519  total_loss: 3.761  loss_sem_seg: 2.112  loss_center: 0.495  loss_offset: 0.9869  time: 0.3488  data_time: 0.0269  lr: 0.0012379  max_mem: 7418M
[12/11 23:06:46 d2.utils.events]:  eta: 0:54:56  iter: 539  total_loss: 3.539  loss_sem_seg: 1.816  loss_center: 0.4657  loss_offset: 1.091  time: 0.3489  data_time: 0.0249  lr: 0.001283  max_mem: 7418M
[12/11 23:06:53 d2.utils.events]:  eta: 0:54:46  iter: 559  total_loss: 3.5  loss_sem_seg: 1.794  loss_center: 0.6029  loss_offset: 0.941  time: 0.3488  data_time: 0.0249  lr: 0.001328  max_mem: 7418M
[12/11 23:07:00 d2.utils.events]:  eta: 0:54:40  iter: 579  total_loss: 3.74  loss_sem_seg: 1.826  loss_center: 0.5923  loss_offset: 1.066  time: 0.3489  data_time: 0.0258  lr: 0.0013728  max_mem: 7418M
[12/11 23:07:07 d2.utils.events]:  eta: 0:54:36  iter: 599  total_loss: 3.749  loss_sem_seg: 1.946  loss_center: 0.6517  loss_offset: 1.027  time: 0.3489  data_time: 0.0254  lr: 0.0014175  max_mem: 7418M
[12/11 23:07:14 d2.utils.events]:  eta: 0:54:28  iter: 619  total_loss: 3.396  loss_sem_seg: 1.727  loss_center: 0.5799  loss_offset: 0.9041  time: 0.3489  data_time: 0.0242  lr: 0.0014619  max_mem: 7418M
[12/11 23:07:21 d2.utils.events]:  eta: 0:54:23  iter: 639  total_loss: 3.526  loss_sem_seg: 1.954  loss_center: 0.5881  loss_offset: 0.9229  time: 0.3490  data_time: 0.0264  lr: 0.0015062  max_mem: 7418M
[12/11 23:07:28 d2.utils.events]:  eta: 0:54:16  iter: 659  total_loss: 3.498  loss_sem_seg: 2.022  loss_center: 0.6713  loss_offset: 0.8593  time: 0.3491  data_time: 0.0265  lr: 0.0015503  max_mem: 7418M
[12/11 23:07:35 d2.utils.events]:  eta: 0:54:09  iter: 679  total_loss: 3.468  loss_sem_seg: 1.658  loss_center: 0.6763  loss_offset: 0.8912  time: 0.3491  data_time: 0.0260  lr: 0.0015942  max_mem: 7418M
[12/11 23:07:42 d2.utils.events]:  eta: 0:54:04  iter: 699  total_loss: 3.367  loss_sem_seg: 1.801  loss_center: 0.5746  loss_offset: 0.8363  time: 0.3492  data_time: 0.0260  lr: 0.0016379  max_mem: 7418M
[12/11 23:07:49 d2.utils.events]:  eta: 0:53:57  iter: 719  total_loss: 3.485  loss_sem_seg: 1.921  loss_center: 0.5506  loss_offset: 1.039  time: 0.3492  data_time: 0.0259  lr: 0.0016814  max_mem: 7418M
[12/11 23:07:56 d2.utils.events]:  eta: 0:53:49  iter: 739  total_loss: 3.216  loss_sem_seg: 1.726  loss_center: 0.5128  loss_offset: 0.7515  time: 0.3491  data_time: 0.0250  lr: 0.0017248  max_mem: 7418M
[12/11 23:08:03 d2.utils.events]:  eta: 0:53:43  iter: 759  total_loss: 3.219  loss_sem_seg: 1.64  loss_center: 0.5693  loss_offset: 0.9987  time: 0.3492  data_time: 0.0251  lr: 0.0017679  max_mem: 7418M
[12/11 23:08:10 d2.utils.events]:  eta: 0:53:36  iter: 779  total_loss: 3.337  loss_sem_seg: 1.651  loss_center: 0.5554  loss_offset: 0.9221  time: 0.3491  data_time: 0.0262  lr: 0.0018109  max_mem: 7418M
[12/11 23:08:17 d2.utils.events]:  eta: 0:53:29  iter: 799  total_loss: 3.685  loss_sem_seg: 1.989  loss_center: 0.5632  loss_offset: 0.9381  time: 0.3492  data_time: 0.0261  lr: 0.0018537  max_mem: 7418M
[12/11 23:08:24 d2.utils.events]:  eta: 0:53:22  iter: 819  total_loss: 3.233  loss_sem_seg: 1.696  loss_center: 0.6101  loss_offset: 0.8699  time: 0.3491  data_time: 0.0251  lr: 0.0018964  max_mem: 7418M
[12/11 23:08:31 d2.utils.events]:  eta: 0:53:15  iter: 839  total_loss: 3.515  loss_sem_seg: 1.887  loss_center: 0.7072  loss_offset: 0.8152  time: 0.3492  data_time: 0.0259  lr: 0.0019388  max_mem: 7418M
[12/11 23:08:38 d2.utils.events]:  eta: 0:53:09  iter: 859  total_loss: 3.215  loss_sem_seg: 1.724  loss_center: 0.5712  loss_offset: 0.8905  time: 0.3492  data_time: 0.0248  lr: 0.0019811  max_mem: 7418M
[12/11 23:08:45 d2.utils.events]:  eta: 0:53:02  iter: 879  total_loss: 3.436  loss_sem_seg: 1.892  loss_center: 0.7189  loss_offset: 0.7656  time: 0.3492  data_time: 0.0257  lr: 0.0020231  max_mem: 7418M
[12/11 23:08:52 d2.utils.events]:  eta: 0:52:54  iter: 899  total_loss: 3.258  loss_sem_seg: 1.797  loss_center: 0.6746  loss_offset: 0.8321  time: 0.3492  data_time: 0.0244  lr: 0.002065  max_mem: 7418M
[12/11 23:08:59 d2.utils.events]:  eta: 0:52:47  iter: 919  total_loss: 3.28  loss_sem_seg: 1.811  loss_center: 0.5036  loss_offset: 0.8658  time: 0.3492  data_time: 0.0268  lr: 0.0021068  max_mem: 7418M
[12/11 23:09:06 d2.utils.events]:  eta: 0:52:41  iter: 939  total_loss: 2.93  loss_sem_seg: 1.494  loss_center: 0.651  loss_offset: 0.7834  time: 0.3493  data_time: 0.0264  lr: 0.0021483  max_mem: 7418M
[12/11 23:09:13 d2.utils.events]:  eta: 0:52:34  iter: 959  total_loss: 3.342  loss_sem_seg: 1.658  loss_center: 0.6902  loss_offset: 0.8413  time: 0.3493  data_time: 0.0255  lr: 0.0021896  max_mem: 7418M
[12/11 23:09:20 d2.utils.events]:  eta: 0:52:27  iter: 979  total_loss: 3.054  loss_sem_seg: 1.418  loss_center: 0.598  loss_offset: 0.8131  time: 0.3493  data_time: 0.0246  lr: 0.0022308  max_mem: 7418M
[12/11 23:09:27 d2.utils.events]:  eta: 0:52:20  iter: 999  total_loss: 3.281  loss_sem_seg: 2.026  loss_center: 0.5756  loss_offset: 0.8673  time: 0.3494  data_time: 0.0262  lr: 0.0022718  max_mem: 7418M
[12/11 23:09:34 d2.utils.events]:  eta: 0:52:13  iter: 1019  total_loss: 3.076  loss_sem_seg: 1.573  loss_center: 0.6192  loss_offset: 0.8384  time: 0.3494  data_time: 0.0256  lr: 0.0022695  max_mem: 7418M
[12/11 23:09:41 d2.utils.events]:  eta: 0:52:06  iter: 1039  total_loss: 3.081  loss_sem_seg: 1.612  loss_center: 0.5105  loss_offset: 0.815  time: 0.3494  data_time: 0.0251  lr: 0.002265  max_mem: 7418M
[12/11 23:09:49 d2.utils.events]:  eta: 0:51:59  iter: 1059  total_loss: 2.941  loss_sem_seg: 1.6  loss_center: 0.621  loss_offset: 0.7188  time: 0.3495  data_time: 0.0265  lr: 0.0022604  max_mem: 7418M
[12/11 23:09:56 d2.utils.events]:  eta: 0:51:52  iter: 1079  total_loss: 3.127  loss_sem_seg: 1.577  loss_center: 0.6252  loss_offset: 0.7586  time: 0.3495  data_time: 0.0265  lr: 0.0022559  max_mem: 7418M
[12/11 23:10:03 d2.utils.events]:  eta: 0:51:45  iter: 1099  total_loss: 3.588  loss_sem_seg: 2.116  loss_center: 0.5614  loss_offset: 0.88  time: 0.3495  data_time: 0.0268  lr: 0.0022513  max_mem: 7418M
[12/11 23:10:10 d2.utils.events]:  eta: 0:51:38  iter: 1119  total_loss: 3.443  loss_sem_seg: 1.973  loss_center: 0.4327  loss_offset: 0.938  time: 0.3496  data_time: 0.0256  lr: 0.0022468  max_mem: 7418M
[12/11 23:10:17 d2.utils.events]:  eta: 0:51:31  iter: 1139  total_loss: 3.107  loss_sem_seg: 1.584  loss_center: 0.574  loss_offset: 0.783  time: 0.3496  data_time: 0.0249  lr: 0.0022422  max_mem: 7418M
[12/11 23:10:24 d2.utils.events]:  eta: 0:51:24  iter: 1159  total_loss: 3.342  loss_sem_seg: 1.84  loss_center: 0.5612  loss_offset: 0.8958  time: 0.3495  data_time: 0.0249  lr: 0.0022376  max_mem: 7418M
[12/11 23:10:31 d2.utils.events]:  eta: 0:51:19  iter: 1179  total_loss: 3.463  loss_sem_seg: 1.884  loss_center: 0.7112  loss_offset: 0.7773  time: 0.3495  data_time: 0.0276  lr: 0.0022331  max_mem: 7418M
[12/11 23:10:38 d2.utils.events]:  eta: 0:51:11  iter: 1199  total_loss: 3.194  loss_sem_seg: 1.564  loss_center: 0.696  loss_offset: 0.7555  time: 0.3495  data_time: 0.0249  lr: 0.0022285  max_mem: 7418M
[12/11 23:10:45 d2.utils.events]:  eta: 0:51:04  iter: 1219  total_loss: 2.944  loss_sem_seg: 1.529  loss_center: 0.6193  loss_offset: 0.8891  time: 0.3495  data_time: 0.0253  lr: 0.002224  max_mem: 7418M
[12/11 23:10:52 d2.utils.events]:  eta: 0:50:59  iter: 1239  total_loss: 3.106  loss_sem_seg: 1.595  loss_center: 0.5549  loss_offset: 0.7293  time: 0.3495  data_time: 0.0264  lr: 0.0022194  max_mem: 7418M
[12/11 23:10:59 d2.utils.events]:  eta: 0:50:52  iter: 1259  total_loss: 3.014  loss_sem_seg: 1.571  loss_center: 0.5033  loss_offset: 0.7355  time: 0.3495  data_time: 0.0259  lr: 0.0022149  max_mem: 7418M
[12/11 23:11:06 d2.utils.events]:  eta: 0:50:44  iter: 1279  total_loss: 2.921  loss_sem_seg: 1.67  loss_center: 0.4793  loss_offset: 0.7282  time: 0.3495  data_time: 0.0245  lr: 0.0022103  max_mem: 7418M
[12/11 23:11:13 d2.utils.events]:  eta: 0:50:38  iter: 1299  total_loss: 2.955  loss_sem_seg: 1.624  loss_center: 0.4253  loss_offset: 0.7107  time: 0.3495  data_time: 0.0263  lr: 0.0022057  max_mem: 7418M
[12/11 23:11:20 d2.utils.events]:  eta: 0:50:31  iter: 1319  total_loss: 3.15  loss_sem_seg: 1.581  loss_center: 0.5578  loss_offset: 0.7457  time: 0.3495  data_time: 0.0254  lr: 0.0022012  max_mem: 7418M
[12/11 23:11:27 d2.utils.events]:  eta: 0:50:25  iter: 1339  total_loss: 3.177  loss_sem_seg: 1.532  loss_center: 0.6179  loss_offset: 0.8838  time: 0.3495  data_time: 0.0247  lr: 0.0021966  max_mem: 7418M
[12/11 23:11:34 d2.utils.events]:  eta: 0:50:16  iter: 1359  total_loss: 3.027  loss_sem_seg: 1.598  loss_center: 0.5705  loss_offset: 0.7839  time: 0.3495  data_time: 0.0260  lr: 0.002192  max_mem: 7418M
[12/11 23:11:41 d2.utils.events]:  eta: 0:50:08  iter: 1379  total_loss: 3.054  loss_sem_seg: 1.52  loss_center: 0.6209  loss_offset: 0.8007  time: 0.3495  data_time: 0.0257  lr: 0.0021875  max_mem: 7418M
[12/11 23:11:48 d2.utils.events]:  eta: 0:50:03  iter: 1399  total_loss: 2.847  loss_sem_seg: 1.48  loss_center: 0.4761  loss_offset: 0.7427  time: 0.3495  data_time: 0.0254  lr: 0.0021829  max_mem: 7418M
[12/11 23:11:55 d2.utils.events]:  eta: 0:49:56  iter: 1419  total_loss: 2.959  loss_sem_seg: 1.416  loss_center: 0.6378  loss_offset: 0.8644  time: 0.3495  data_time: 0.0275  lr: 0.0021783  max_mem: 7418M
[12/11 23:12:02 d2.utils.events]:  eta: 0:49:49  iter: 1439  total_loss: 3.14  loss_sem_seg: 1.546  loss_center: 0.6425  loss_offset: 0.7703  time: 0.3495  data_time: 0.0268  lr: 0.0021738  max_mem: 7418M
[12/11 23:12:09 d2.utils.events]:  eta: 0:49:41  iter: 1459  total_loss: 2.847  loss_sem_seg: 1.35  loss_center: 0.6117  loss_offset: 0.689  time: 0.3495  data_time: 0.0268  lr: 0.0021692  max_mem: 7418M
[12/11 23:12:16 d2.utils.events]:  eta: 0:49:35  iter: 1479  total_loss: 2.809  loss_sem_seg: 1.479  loss_center: 0.4756  loss_offset: 0.7319  time: 0.3495  data_time: 0.0252  lr: 0.0021646  max_mem: 7418M
[12/11 23:12:23 d2.utils.events]:  eta: 0:49:28  iter: 1499  total_loss: 2.701  loss_sem_seg: 1.46  loss_center: 0.4839  loss_offset: 0.6821  time: 0.3495  data_time: 0.0254  lr: 0.00216  max_mem: 7418M
[12/11 23:12:30 d2.utils.events]:  eta: 0:49:20  iter: 1519  total_loss: 2.919  loss_sem_seg: 1.449  loss_center: 0.5006  loss_offset: 0.8043  time: 0.3495  data_time: 0.0260  lr: 0.0021555  max_mem: 7418M
[12/11 23:12:37 d2.utils.events]:  eta: 0:49:14  iter: 1539  total_loss: 2.782  loss_sem_seg: 1.43  loss_center: 0.5558  loss_offset: 0.7246  time: 0.3495  data_time: 0.0263  lr: 0.0021509  max_mem: 7418M
[12/11 23:12:44 d2.utils.events]:  eta: 0:49:07  iter: 1559  total_loss: 3.25  loss_sem_seg: 1.573  loss_center: 0.6985  loss_offset: 0.7506  time: 0.3495  data_time: 0.0269  lr: 0.0021463  max_mem: 7418M
[12/11 23:12:51 d2.utils.events]:  eta: 0:49:00  iter: 1579  total_loss: 2.907  loss_sem_seg: 1.573  loss_center: 0.5449  loss_offset: 0.6447  time: 0.3496  data_time: 0.0299  lr: 0.0021417  max_mem: 7418M
[12/11 23:12:58 d2.utils.events]:  eta: 0:48:51  iter: 1599  total_loss: 2.689  loss_sem_seg: 1.399  loss_center: 0.6331  loss_offset: 0.6544  time: 0.3496  data_time: 0.0250  lr: 0.0021372  max_mem: 7418M
[12/11 23:13:05 d2.utils.events]:  eta: 0:48:46  iter: 1619  total_loss: 2.74  loss_sem_seg: 1.503  loss_center: 0.6677  loss_offset: 0.6962  time: 0.3496  data_time: 0.0268  lr: 0.0021326  max_mem: 7418M
[12/11 23:13:12 d2.utils.events]:  eta: 0:48:39  iter: 1639  total_loss: 2.773  loss_sem_seg: 1.478  loss_center: 0.5016  loss_offset: 0.7574  time: 0.3496  data_time: 0.0256  lr: 0.002128  max_mem: 7418M
[12/11 23:13:19 d2.utils.events]:  eta: 0:48:33  iter: 1659  total_loss: 2.532  loss_sem_seg: 1.313  loss_center: 0.4861  loss_offset: 0.7084  time: 0.3496  data_time: 0.0263  lr: 0.0021234  max_mem: 7418M
[12/11 23:13:26 d2.utils.events]:  eta: 0:48:26  iter: 1679  total_loss: 2.629  loss_sem_seg: 1.38  loss_center: 0.5911  loss_offset: 0.6598  time: 0.3496  data_time: 0.0255  lr: 0.0021188  max_mem: 7418M
[12/11 23:13:33 d2.utils.events]:  eta: 0:48:18  iter: 1699  total_loss: 2.938  loss_sem_seg: 1.513  loss_center: 0.6184  loss_offset: 0.6762  time: 0.3496  data_time: 0.0273  lr: 0.0021143  max_mem: 7418M
[12/11 23:13:40 d2.utils.events]:  eta: 0:48:11  iter: 1719  total_loss: 3.144  loss_sem_seg: 1.682  loss_center: 0.5979  loss_offset: 0.7668  time: 0.3496  data_time: 0.0265  lr: 0.0021097  max_mem: 7418M
[12/11 23:13:47 d2.utils.events]:  eta: 0:48:04  iter: 1739  total_loss: 3.047  loss_sem_seg: 1.563  loss_center: 0.6921  loss_offset: 0.6515  time: 0.3496  data_time: 0.0250  lr: 0.0021051  max_mem: 7418M
[12/11 23:13:54 d2.utils.events]:  eta: 0:47:55  iter: 1759  total_loss: 2.662  loss_sem_seg: 1.331  loss_center: 0.6185  loss_offset: 0.6763  time: 0.3496  data_time: 0.0265  lr: 0.0021005  max_mem: 7418M
[12/11 23:14:01 d2.utils.events]:  eta: 0:47:50  iter: 1779  total_loss: 2.897  loss_sem_seg: 1.267  loss_center: 0.5295  loss_offset: 0.7615  time: 0.3497  data_time: 0.0257  lr: 0.0020959  max_mem: 7418M
[12/11 23:14:08 d2.utils.events]:  eta: 0:47:42  iter: 1799  total_loss: 2.852  loss_sem_seg: 1.376  loss_center: 0.6288  loss_offset: 0.7813  time: 0.3497  data_time: 0.0262  lr: 0.0020913  max_mem: 7418M
[12/11 23:14:15 d2.utils.events]:  eta: 0:47:35  iter: 1819  total_loss: 2.915  loss_sem_seg: 1.357  loss_center: 0.6666  loss_offset: 0.803  time: 0.3497  data_time: 0.0256  lr: 0.0020867  max_mem: 7418M
[12/11 23:14:22 d2.utils.events]:  eta: 0:47:28  iter: 1839  total_loss: 2.859  loss_sem_seg: 1.402  loss_center: 0.5332  loss_offset: 0.7902  time: 0.3497  data_time: 0.0263  lr: 0.0020821  max_mem: 7418M
[12/11 23:14:29 d2.utils.events]:  eta: 0:47:21  iter: 1859  total_loss: 2.811  loss_sem_seg: 1.417  loss_center: 0.702  loss_offset: 0.7642  time: 0.3497  data_time: 0.0271  lr: 0.0020775  max_mem: 7418M
[12/11 23:14:36 d2.utils.events]:  eta: 0:47:14  iter: 1879  total_loss: 2.534  loss_sem_seg: 1.241  loss_center: 0.4659  loss_offset: 0.7024  time: 0.3497  data_time: 0.0263  lr: 0.0020729  max_mem: 7418M
[12/11 23:14:43 d2.utils.events]:  eta: 0:47:08  iter: 1899  total_loss: 2.917  loss_sem_seg: 1.541  loss_center: 0.6121  loss_offset: 0.6636  time: 0.3498  data_time: 0.0263  lr: 0.0020684  max_mem: 7418M
[12/11 23:14:50 d2.utils.events]:  eta: 0:47:00  iter: 1919  total_loss: 2.841  loss_sem_seg: 1.488  loss_center: 0.6487  loss_offset: 0.594  time: 0.3498  data_time: 0.0262  lr: 0.0020638  max_mem: 7418M
[12/11 23:14:57 d2.utils.events]:  eta: 0:46:52  iter: 1939  total_loss: 2.442  loss_sem_seg: 1.252  loss_center: 0.6384  loss_offset: 0.6345  time: 0.3497  data_time: 0.0250  lr: 0.0020592  max_mem: 7418M
[12/11 23:15:04 d2.utils.events]:  eta: 0:46:46  iter: 1959  total_loss: 2.624  loss_sem_seg: 1.358  loss_center: 0.5028  loss_offset: 0.6252  time: 0.3497  data_time: 0.0278  lr: 0.0020546  max_mem: 7418M
[12/11 23:15:11 d2.utils.events]:  eta: 0:46:38  iter: 1979  total_loss: 2.609  loss_sem_seg: 1.3  loss_center: 0.4793  loss_offset: 0.6761  time: 0.3497  data_time: 0.0264  lr: 0.00205  max_mem: 7418M
[12/11 23:15:18 d2.utils.events]:  eta: 0:46:30  iter: 1999  total_loss: 2.846  loss_sem_seg: 1.604  loss_center: 0.5257  loss_offset: 0.7731  time: 0.3497  data_time: 0.0248  lr: 0.0020454  max_mem: 7418M
[12/11 23:15:25 d2.utils.events]:  eta: 0:46:24  iter: 2019  total_loss: 2.591  loss_sem_seg: 1.285  loss_center: 0.5551  loss_offset: 0.744  time: 0.3497  data_time: 0.0260  lr: 0.0020408  max_mem: 7418M
[12/11 23:15:32 d2.utils.events]:  eta: 0:46:17  iter: 2039  total_loss: 2.705  loss_sem_seg: 1.404  loss_center: 0.5461  loss_offset: 0.7231  time: 0.3497  data_time: 0.0263  lr: 0.0020362  max_mem: 7418M
[12/11 23:15:39 d2.utils.events]:  eta: 0:46:10  iter: 2059  total_loss: 2.424  loss_sem_seg: 1.303  loss_center: 0.5199  loss_offset: 0.6046  time: 0.3497  data_time: 0.0273  lr: 0.0020316  max_mem: 7418M
[12/11 23:15:46 d2.utils.events]:  eta: 0:46:03  iter: 2079  total_loss: 2.605  loss_sem_seg: 1.264  loss_center: 0.5973  loss_offset: 0.5821  time: 0.3497  data_time: 0.0241  lr: 0.0020269  max_mem: 7418M
[12/11 23:15:53 d2.utils.events]:  eta: 0:45:56  iter: 2099  total_loss: 2.923  loss_sem_seg: 1.467  loss_center: 0.6055  loss_offset: 0.6893  time: 0.3497  data_time: 0.0267  lr: 0.0020223  max_mem: 7418M
[12/11 23:16:00 d2.utils.events]:  eta: 0:45:49  iter: 2119  total_loss: 2.773  loss_sem_seg: 1.415  loss_center: 0.586  loss_offset: 0.6224  time: 0.3497  data_time: 0.0283  lr: 0.0020177  max_mem: 7418M
[12/11 23:16:07 d2.utils.events]:  eta: 0:45:42  iter: 2139  total_loss: 2.628  loss_sem_seg: 1.316  loss_center: 0.4322  loss_offset: 0.6858  time: 0.3497  data_time: 0.0260  lr: 0.0020131  max_mem: 7418M
[12/11 23:16:14 d2.utils.events]:  eta: 0:45:37  iter: 2159  total_loss: 2.68  loss_sem_seg: 1.363  loss_center: 0.6126  loss_offset: 0.6169  time: 0.3497  data_time: 0.0274  lr: 0.0020085  max_mem: 7418M
[12/11 23:16:21 d2.utils.events]:  eta: 0:45:28  iter: 2179  total_loss: 2.674  loss_sem_seg: 1.361  loss_center: 0.6284  loss_offset: 0.6702  time: 0.3497  data_time: 0.0249  lr: 0.0020039  max_mem: 7418M
[12/11 23:16:28 d2.utils.events]:  eta: 0:45:21  iter: 2199  total_loss: 2.683  loss_sem_seg: 1.337  loss_center: 0.5501  loss_offset: 0.6283  time: 0.3497  data_time: 0.0259  lr: 0.0019993  max_mem: 7418M
[12/11 23:16:35 d2.utils.events]:  eta: 0:45:14  iter: 2219  total_loss: 2.8  loss_sem_seg: 1.528  loss_center: 0.5554  loss_offset: 0.5558  time: 0.3497  data_time: 0.0256  lr: 0.0019947  max_mem: 7418M
[12/11 23:16:42 d2.utils.events]:  eta: 0:45:08  iter: 2239  total_loss: 3.121  loss_sem_seg: 1.449  loss_center: 0.7333  loss_offset: 0.7983  time: 0.3497  data_time: 0.0285  lr: 0.0019901  max_mem: 7418M
[12/11 23:16:49 d2.utils.events]:  eta: 0:45:01  iter: 2259  total_loss: 2.604  loss_sem_seg: 1.258  loss_center: 0.6489  loss_offset: 0.6916  time: 0.3498  data_time: 0.0267  lr: 0.0019854  max_mem: 7418M
[12/11 23:16:56 d2.utils.events]:  eta: 0:44:57  iter: 2279  total_loss: 2.829  loss_sem_seg: 1.235  loss_center: 0.5471  loss_offset: 0.9091  time: 0.3498  data_time: 0.0267  lr: 0.0019808  max_mem: 7418M
[12/11 23:17:03 d2.utils.events]:  eta: 0:44:50  iter: 2299  total_loss: 2.746  loss_sem_seg: 1.393  loss_center: 0.5684  loss_offset: 0.7073  time: 0.3498  data_time: 0.0261  lr: 0.0019762  max_mem: 7418M
[12/11 23:17:10 d2.utils.events]:  eta: 0:44:43  iter: 2319  total_loss: 2.796  loss_sem_seg: 1.548  loss_center: 0.47  loss_offset: 0.6005  time: 0.3498  data_time: 0.0262  lr: 0.0019716  max_mem: 7418M
[12/11 23:17:17 d2.utils.events]:  eta: 0:44:36  iter: 2339  total_loss: 2.615  loss_sem_seg: 1.548  loss_center: 0.5494  loss_offset: 0.6655  time: 0.3498  data_time: 0.0278  lr: 0.001967  max_mem: 7418M
[12/11 23:17:24 d2.utils.events]:  eta: 0:44:29  iter: 2359  total_loss: 2.712  loss_sem_seg: 1.286  loss_center: 0.7347  loss_offset: 0.6732  time: 0.3498  data_time: 0.0260  lr: 0.0019623  max_mem: 7418M
[12/11 23:17:31 d2.utils.events]:  eta: 0:44:23  iter: 2379  total_loss: 3.118  loss_sem_seg: 1.582  loss_center: 0.5353  loss_offset: 0.7221  time: 0.3498  data_time: 0.0246  lr: 0.0019577  max_mem: 7418M
[12/11 23:17:38 d2.utils.events]:  eta: 0:44:15  iter: 2399  total_loss: 2.871  loss_sem_seg: 1.41  loss_center: 0.6126  loss_offset: 0.7105  time: 0.3498  data_time: 0.0269  lr: 0.0019531  max_mem: 7418M
[12/11 23:17:45 d2.utils.events]:  eta: 0:44:09  iter: 2419  total_loss: 2.619  loss_sem_seg: 1.33  loss_center: 0.503  loss_offset: 0.6717  time: 0.3498  data_time: 0.0301  lr: 0.0019485  max_mem: 7418M
[12/11 23:17:52 d2.utils.events]:  eta: 0:44:02  iter: 2439  total_loss: 2.582  loss_sem_seg: 1.171  loss_center: 0.6184  loss_offset: 0.6523  time: 0.3498  data_time: 0.0255  lr: 0.0019438  max_mem: 7418M
[12/11 23:17:59 d2.utils.events]:  eta: 0:43:55  iter: 2459  total_loss: 2.885  loss_sem_seg: 1.496  loss_center: 0.579  loss_offset: 0.7135  time: 0.3499  data_time: 0.0291  lr: 0.0019392  max_mem: 7418M
[12/11 23:18:06 d2.utils.events]:  eta: 0:43:48  iter: 2479  total_loss: 2.88  loss_sem_seg: 1.386  loss_center: 0.6457  loss_offset: 0.6524  time: 0.3498  data_time: 0.0250  lr: 0.0019346  max_mem: 7418M
[12/11 23:18:13 d2.utils.events]:  eta: 0:43:41  iter: 2499  total_loss: 2.53  loss_sem_seg: 1.412  loss_center: 0.5606  loss_offset: 0.598  time: 0.3498  data_time: 0.0255  lr: 0.00193  max_mem: 7418M
[12/11 23:18:20 d2.utils.events]:  eta: 0:43:34  iter: 2519  total_loss: 2.773  loss_sem_seg: 1.485  loss_center: 0.4625  loss_offset: 0.7376  time: 0.3498  data_time: 0.0258  lr: 0.0019253  max_mem: 7418M
[12/11 23:18:27 d2.utils.events]:  eta: 0:43:27  iter: 2539  total_loss: 2.65  loss_sem_seg: 1.24  loss_center: 0.6199  loss_offset: 0.6737  time: 0.3498  data_time: 0.0260  lr: 0.0019207  max_mem: 7418M
[12/11 23:18:34 d2.utils.events]:  eta: 0:43:20  iter: 2559  total_loss: 2.589  loss_sem_seg: 1.195  loss_center: 0.5859  loss_offset: 0.7377  time: 0.3499  data_time: 0.0277  lr: 0.0019161  max_mem: 7418M
[12/11 23:18:41 d2.utils.events]:  eta: 0:43:13  iter: 2579  total_loss: 2.501  loss_sem_seg: 1.206  loss_center: 0.523  loss_offset: 0.6366  time: 0.3498  data_time: 0.0257  lr: 0.0019114  max_mem: 7418M
[12/11 23:18:48 d2.utils.events]:  eta: 0:43:08  iter: 2599  total_loss: 2.462  loss_sem_seg: 1.283  loss_center: 0.446  loss_offset: 0.7111  time: 0.3499  data_time: 0.0279  lr: 0.0019068  max_mem: 7418M
[12/11 23:18:55 d2.utils.events]:  eta: 0:43:00  iter: 2619  total_loss: 2.711  loss_sem_seg: 1.419  loss_center: 0.6451  loss_offset: 0.5824  time: 0.3499  data_time: 0.0258  lr: 0.0019021  max_mem: 7418M
[12/11 23:19:02 d2.utils.events]:  eta: 0:42:52  iter: 2639  total_loss: 2.555  loss_sem_seg: 1.246  loss_center: 0.4792  loss_offset: 0.7296  time: 0.3498  data_time: 0.0255  lr: 0.0018975  max_mem: 7418M
[12/11 23:19:09 d2.utils.events]:  eta: 0:42:45  iter: 2659  total_loss: 2.654  loss_sem_seg: 1.363  loss_center: 0.6389  loss_offset: 0.672  time: 0.3498  data_time: 0.0258  lr: 0.0018929  max_mem: 7418M
[12/11 23:19:16 d2.utils.events]:  eta: 0:42:36  iter: 2679  total_loss: 2.595  loss_sem_seg: 1.187  loss_center: 0.6184  loss_offset: 0.647  time: 0.3498  data_time: 0.0256  lr: 0.0018882  max_mem: 7418M
[12/11 23:19:23 d2.utils.events]:  eta: 0:42:29  iter: 2699  total_loss: 2.664  loss_sem_seg: 1.381  loss_center: 0.5884  loss_offset: 0.6183  time: 0.3498  data_time: 0.0263  lr: 0.0018836  max_mem: 7418M
[12/11 23:19:30 d2.utils.events]:  eta: 0:42:21  iter: 2719  total_loss: 2.882  loss_sem_seg: 1.381  loss_center: 0.6394  loss_offset: 0.6576  time: 0.3498  data_time: 0.0273  lr: 0.0018789  max_mem: 7418M
[12/11 23:19:37 d2.utils.events]:  eta: 0:42:15  iter: 2739  total_loss: 2.629  loss_sem_seg: 1.296  loss_center: 0.6207  loss_offset: 0.636  time: 0.3498  data_time: 0.0257  lr: 0.0018743  max_mem: 7418M
[12/11 23:19:44 d2.utils.events]:  eta: 0:42:09  iter: 2759  total_loss: 2.445  loss_sem_seg: 1.365  loss_center: 0.5088  loss_offset: 0.6681  time: 0.3499  data_time: 0.0285  lr: 0.0018696  max_mem: 7418M
[12/11 23:19:51 d2.utils.events]:  eta: 0:42:01  iter: 2779  total_loss: 2.596  loss_sem_seg: 1.307  loss_center: 0.5914  loss_offset: 0.668  time: 0.3499  data_time: 0.0265  lr: 0.001865  max_mem: 7418M
[12/11 23:19:58 d2.utils.events]:  eta: 0:41:55  iter: 2799  total_loss: 2.583  loss_sem_seg: 1.381  loss_center: 0.4887  loss_offset: 0.7144  time: 0.3499  data_time: 0.0259  lr: 0.0018603  max_mem: 7418M
[12/11 23:20:05 d2.utils.events]:  eta: 0:41:49  iter: 2819  total_loss: 2.489  loss_sem_seg: 1.147  loss_center: 0.6044  loss_offset: 0.6813  time: 0.3499  data_time: 0.0265  lr: 0.0018557  max_mem: 7418M
[12/11 23:20:12 d2.utils.events]:  eta: 0:41:41  iter: 2839  total_loss: 2.63  loss_sem_seg: 1.317  loss_center: 0.614  loss_offset: 0.6169  time: 0.3498  data_time: 0.0247  lr: 0.001851  max_mem: 7418M
[12/11 23:20:19 d2.utils.events]:  eta: 0:41:35  iter: 2859  total_loss: 2.716  loss_sem_seg: 1.468  loss_center: 0.4173  loss_offset: 0.8292  time: 0.3498  data_time: 0.0271  lr: 0.0018464  max_mem: 7418M
[12/11 23:20:26 d2.utils.events]:  eta: 0:41:27  iter: 2879  total_loss: 2.556  loss_sem_seg: 1.334  loss_center: 0.5469  loss_offset: 0.6279  time: 0.3498  data_time: 0.0251  lr: 0.0018417  max_mem: 7418M
[12/11 23:20:33 d2.utils.events]:  eta: 0:41:20  iter: 2899  total_loss: 2.581  loss_sem_seg: 1.193  loss_center: 0.5845  loss_offset: 0.6633  time: 0.3498  data_time: 0.0252  lr: 0.0018371  max_mem: 7418M
[12/11 23:20:40 d2.utils.events]:  eta: 0:41:13  iter: 2919  total_loss: 2.468  loss_sem_seg: 1.227  loss_center: 0.5278  loss_offset: 0.6469  time: 0.3498  data_time: 0.0255  lr: 0.0018324  max_mem: 7418M
[12/11 23:20:47 d2.utils.events]:  eta: 0:41:06  iter: 2939  total_loss: 2.649  loss_sem_seg: 1.379  loss_center: 0.6887  loss_offset: 0.6606  time: 0.3498  data_time: 0.0271  lr: 0.0018278  max_mem: 7418M
[12/11 23:20:54 d2.utils.events]:  eta: 0:40:59  iter: 2959  total_loss: 2.473  loss_sem_seg: 1.23  loss_center: 0.5768  loss_offset: 0.6178  time: 0.3498  data_time: 0.0257  lr: 0.0018231  max_mem: 7418M
[12/11 23:21:01 d2.utils.events]:  eta: 0:40:53  iter: 2979  total_loss: 2.479  loss_sem_seg: 1.111  loss_center: 0.6022  loss_offset: 0.5971  time: 0.3498  data_time: 0.0266  lr: 0.0018184  max_mem: 7418M
[12/11 23:21:08 d2.utils.events]:  eta: 0:40:46  iter: 2999  total_loss: 2.603  loss_sem_seg: 1.266  loss_center: 0.5224  loss_offset: 0.6587  time: 0.3498  data_time: 0.0263  lr: 0.0018138  max_mem: 7418M
[12/11 23:21:15 d2.utils.events]:  eta: 0:40:39  iter: 3019  total_loss: 2.322  loss_sem_seg: 1.138  loss_center: 0.515  loss_offset: 0.6502  time: 0.3498  data_time: 0.0258  lr: 0.0018091  max_mem: 7418M
[12/11 23:21:22 d2.utils.events]:  eta: 0:40:32  iter: 3039  total_loss: 2.551  loss_sem_seg: 1.376  loss_center: 0.4359  loss_offset: 0.6721  time: 0.3498  data_time: 0.0261  lr: 0.0018044  max_mem: 7418M
[12/11 23:21:30 d2.utils.events]:  eta: 0:40:25  iter: 3059  total_loss: 2.412  loss_sem_seg: 1.171  loss_center: 0.5381  loss_offset: 0.5737  time: 0.3498  data_time: 0.0271  lr: 0.0017998  max_mem: 7418M
[12/11 23:21:37 d2.utils.events]:  eta: 0:40:18  iter: 3079  total_loss: 2.798  loss_sem_seg: 1.256  loss_center: 0.6465  loss_offset: 0.7536  time: 0.3498  data_time: 0.0267  lr: 0.0017951  max_mem: 7418M
[12/11 23:21:43 d2.utils.events]:  eta: 0:40:10  iter: 3099  total_loss: 2.45  loss_sem_seg: 1.122  loss_center: 0.574  loss_offset: 0.6911  time: 0.3498  data_time: 0.0247  lr: 0.0017904  max_mem: 7418M
[12/11 23:21:50 d2.utils.events]:  eta: 0:40:03  iter: 3119  total_loss: 2.566  loss_sem_seg: 1.413  loss_center: 0.5412  loss_offset: 0.6489  time: 0.3498  data_time: 0.0255  lr: 0.0017858  max_mem: 7418M
[12/11 23:21:58 d2.utils.events]:  eta: 0:39:56  iter: 3139  total_loss: 2.833  loss_sem_seg: 1.371  loss_center: 0.469  loss_offset: 0.6749  time: 0.3498  data_time: 0.0264  lr: 0.0017811  max_mem: 7418M
[12/11 23:22:05 d2.utils.events]:  eta: 0:39:48  iter: 3159  total_loss: 2.517  loss_sem_seg: 1.191  loss_center: 0.4857  loss_offset: 0.714  time: 0.3498  data_time: 0.0273  lr: 0.0017764  max_mem: 7418M
[12/11 23:22:12 d2.utils.events]:  eta: 0:39:42  iter: 3179  total_loss: 2.506  loss_sem_seg: 1.153  loss_center: 0.5519  loss_offset: 0.6143  time: 0.3499  data_time: 0.0286  lr: 0.0017718  max_mem: 7418M
[12/11 23:22:19 d2.utils.events]:  eta: 0:39:35  iter: 3199  total_loss: 2.512  loss_sem_seg: 1.163  loss_center: 0.4719  loss_offset: 0.6052  time: 0.3499  data_time: 0.0272  lr: 0.0017671  max_mem: 7418M
[12/11 23:22:26 d2.utils.events]:  eta: 0:39:28  iter: 3219  total_loss: 2.634  loss_sem_seg: 1.255  loss_center: 0.6159  loss_offset: 0.7405  time: 0.3499  data_time: 0.0272  lr: 0.0017624  max_mem: 7418M
[12/11 23:22:33 d2.utils.events]:  eta: 0:39:20  iter: 3239  total_loss: 2.626  loss_sem_seg: 1.404  loss_center: 0.4938  loss_offset: 0.579  time: 0.3499  data_time: 0.0257  lr: 0.0017577  max_mem: 7418M
[12/11 23:22:40 d2.utils.events]:  eta: 0:39:14  iter: 3259  total_loss: 2.49  loss_sem_seg: 1.141  loss_center: 0.5609  loss_offset: 0.6376  time: 0.3498  data_time: 0.0255  lr: 0.001753  max_mem: 7418M
[12/11 23:22:47 d2.utils.events]:  eta: 0:39:06  iter: 3279  total_loss: 2.371  loss_sem_seg: 1.252  loss_center: 0.5051  loss_offset: 0.6387  time: 0.3499  data_time: 0.0269  lr: 0.0017484  max_mem: 7418M
[12/11 23:22:54 d2.utils.events]:  eta: 0:38:59  iter: 3299  total_loss: 2.405  loss_sem_seg: 1.254  loss_center: 0.514  loss_offset: 0.6422  time: 0.3499  data_time: 0.0272  lr: 0.0017437  max_mem: 7418M
[12/11 23:23:01 d2.utils.events]:  eta: 0:38:52  iter: 3319  total_loss: 2.445  loss_sem_seg: 1.25  loss_center: 0.5201  loss_offset: 0.731  time: 0.3499  data_time: 0.0254  lr: 0.001739  max_mem: 7418M
[12/11 23:23:08 d2.utils.events]:  eta: 0:38:45  iter: 3339  total_loss: 2.525  loss_sem_seg: 1.231  loss_center: 0.546  loss_offset: 0.6373  time: 0.3499  data_time: 0.0256  lr: 0.0017343  max_mem: 7418M
[12/11 23:23:15 d2.utils.events]:  eta: 0:38:38  iter: 3359  total_loss: 2.525  loss_sem_seg: 1.114  loss_center: 0.6567  loss_offset: 0.5956  time: 0.3499  data_time: 0.0269  lr: 0.0017296  max_mem: 7418M
[12/11 23:23:22 d2.utils.events]:  eta: 0:38:31  iter: 3379  total_loss: 2.578  loss_sem_seg: 1.259  loss_center: 0.566  loss_offset: 0.6578  time: 0.3499  data_time: 0.0264  lr: 0.0017249  max_mem: 7418M
[12/11 23:23:29 d2.utils.events]:  eta: 0:38:23  iter: 3399  total_loss: 2.432  loss_sem_seg: 1.268  loss_center: 0.6082  loss_offset: 0.6617  time: 0.3499  data_time: 0.0242  lr: 0.0017202  max_mem: 7418M
[12/11 23:23:36 d2.utils.events]:  eta: 0:38:16  iter: 3419  total_loss: 2.477  loss_sem_seg: 1.193  loss_center: 0.6105  loss_offset: 0.6247  time: 0.3499  data_time: 0.0278  lr: 0.0017155  max_mem: 7418M
[12/11 23:23:43 d2.utils.events]:  eta: 0:38:09  iter: 3439  total_loss: 2.287  loss_sem_seg: 1.169  loss_center: 0.3568  loss_offset: 0.7251  time: 0.3499  data_time: 0.0256  lr: 0.0017109  max_mem: 7418M
[12/11 23:23:50 d2.utils.events]:  eta: 0:38:02  iter: 3459  total_loss: 2.345  loss_sem_seg: 1.028  loss_center: 0.5136  loss_offset: 0.6444  time: 0.3499  data_time: 0.0269  lr: 0.0017062  max_mem: 7418M
[12/11 23:23:57 d2.utils.events]:  eta: 0:37:55  iter: 3479  total_loss: 2.332  loss_sem_seg: 1.181  loss_center: 0.6186  loss_offset: 0.756  time: 0.3498  data_time: 0.0243  lr: 0.0017015  max_mem: 7418M
[12/11 23:24:04 d2.utils.events]:  eta: 0:37:48  iter: 3499  total_loss: 2.642  loss_sem_seg: 1.368  loss_center: 0.5117  loss_offset: 0.6064  time: 0.3498  data_time: 0.0258  lr: 0.0016968  max_mem: 7418M
[12/11 23:24:11 d2.utils.events]:  eta: 0:37:41  iter: 3519  total_loss: 2.462  loss_sem_seg: 1.118  loss_center: 0.5331  loss_offset: 0.6769  time: 0.3498  data_time: 0.0261  lr: 0.0016921  max_mem: 7418M
[12/11 23:24:18 d2.utils.events]:  eta: 0:37:35  iter: 3539  total_loss: 2.19  loss_sem_seg: 1.27  loss_center: 0.4407  loss_offset: 0.6195  time: 0.3498  data_time: 0.0270  lr: 0.0016874  max_mem: 7418M
[12/11 23:24:25 d2.utils.events]:  eta: 0:37:28  iter: 3559  total_loss: 2.616  loss_sem_seg: 1.244  loss_center: 0.5313  loss_offset: 0.684  time: 0.3499  data_time: 0.0280  lr: 0.0016827  max_mem: 7418M
[12/11 23:24:32 d2.utils.events]:  eta: 0:37:21  iter: 3579  total_loss: 2.628  loss_sem_seg: 1.425  loss_center: 0.4492  loss_offset: 0.6612  time: 0.3499  data_time: 0.0256  lr: 0.001678  max_mem: 7418M
[12/11 23:24:39 d2.utils.events]:  eta: 0:37:14  iter: 3599  total_loss: 2.5  loss_sem_seg: 1.22  loss_center: 0.5942  loss_offset: 0.5866  time: 0.3499  data_time: 0.0268  lr: 0.0016733  max_mem: 7418M
[12/11 23:24:46 d2.utils.events]:  eta: 0:37:07  iter: 3619  total_loss: 2.399  loss_sem_seg: 1.13  loss_center: 0.6771  loss_offset: 0.5774  time: 0.3499  data_time: 0.0255  lr: 0.0016686  max_mem: 7418M
[12/11 23:24:53 d2.utils.events]:  eta: 0:37:00  iter: 3639  total_loss: 2.383  loss_sem_seg: 1.26  loss_center: 0.5748  loss_offset: 0.6224  time: 0.3499  data_time: 0.0267  lr: 0.0016638  max_mem: 7418M
[12/11 23:25:00 d2.utils.events]:  eta: 0:36:53  iter: 3659  total_loss: 2.268  loss_sem_seg: 1.042  loss_center: 0.616  loss_offset: 0.6408  time: 0.3499  data_time: 0.0256  lr: 0.0016591  max_mem: 7418M
[12/11 23:25:07 d2.utils.events]:  eta: 0:36:47  iter: 3679  total_loss: 2.383  loss_sem_seg: 1.171  loss_center: 0.6062  loss_offset: 0.6419  time: 0.3499  data_time: 0.0250  lr: 0.0016544  max_mem: 7418M
[12/11 23:25:14 d2.utils.events]:  eta: 0:36:41  iter: 3699  total_loss: 2.344  loss_sem_seg: 1.023  loss_center: 0.6284  loss_offset: 0.5651  time: 0.3499  data_time: 0.0271  lr: 0.0016497  max_mem: 7418M
[12/11 23:25:21 d2.utils.events]:  eta: 0:36:34  iter: 3719  total_loss: 2.42  loss_sem_seg: 0.9655  loss_center: 0.581  loss_offset: 0.6586  time: 0.3499  data_time: 0.0261  lr: 0.001645  max_mem: 7418M
[12/11 23:25:28 d2.utils.events]:  eta: 0:36:28  iter: 3739  total_loss: 2.204  loss_sem_seg: 1.191  loss_center: 0.4796  loss_offset: 0.5386  time: 0.3499  data_time: 0.0262  lr: 0.0016403  max_mem: 7418M
[12/11 23:25:35 d2.utils.events]:  eta: 0:36:18  iter: 3759  total_loss: 2.501  loss_sem_seg: 1.144  loss_center: 0.588  loss_offset: 0.6979  time: 0.3499  data_time: 0.0260  lr: 0.0016356  max_mem: 7418M
[12/11 23:25:42 d2.utils.events]:  eta: 0:36:11  iter: 3779  total_loss: 2.508  loss_sem_seg: 1.193  loss_center: 0.6078  loss_offset: 0.6725  time: 0.3499  data_time: 0.0261  lr: 0.0016309  max_mem: 7418M
[12/11 23:25:49 d2.utils.events]:  eta: 0:36:04  iter: 3799  total_loss: 2.44  loss_sem_seg: 1.041  loss_center: 0.5531  loss_offset: 0.7371  time: 0.3499  data_time: 0.0259  lr: 0.0016261  max_mem: 7418M
[12/11 23:25:56 d2.utils.events]:  eta: 0:35:57  iter: 3819  total_loss: 2.59  loss_sem_seg: 1.398  loss_center: 0.4937  loss_offset: 0.559  time: 0.3499  data_time: 0.0254  lr: 0.0016214  max_mem: 7418M
[12/11 23:26:03 d2.utils.events]:  eta: 0:35:50  iter: 3839  total_loss: 2.668  loss_sem_seg: 1.35  loss_center: 0.5109  loss_offset: 0.7455  time: 0.3499  data_time: 0.0251  lr: 0.0016167  max_mem: 7418M
[12/11 23:26:10 d2.utils.events]:  eta: 0:35:42  iter: 3859  total_loss: 2.428  loss_sem_seg: 1.242  loss_center: 0.5372  loss_offset: 0.6284  time: 0.3499  data_time: 0.0270  lr: 0.001612  max_mem: 7418M
[12/11 23:26:17 d2.utils.events]:  eta: 0:35:35  iter: 3879  total_loss: 2.463  loss_sem_seg: 1.074  loss_center: 0.6361  loss_offset: 0.6423  time: 0.3499  data_time: 0.0247  lr: 0.0016072  max_mem: 7418M
[12/11 23:26:24 d2.utils.events]:  eta: 0:35:28  iter: 3899  total_loss: 2.388  loss_sem_seg: 1.1  loss_center: 0.7198  loss_offset: 0.5701  time: 0.3499  data_time: 0.0257  lr: 0.0016025  max_mem: 7418M
[12/11 23:26:31 d2.utils.events]:  eta: 0:35:22  iter: 3919  total_loss: 2.519  loss_sem_seg: 1.149  loss_center: 0.5506  loss_offset: 0.6172  time: 0.3499  data_time: 0.0272  lr: 0.0015978  max_mem: 7418M
[12/11 23:26:38 d2.utils.events]:  eta: 0:35:15  iter: 3939  total_loss: 2.348  loss_sem_seg: 1.114  loss_center: 0.492  loss_offset: 0.5326  time: 0.3499  data_time: 0.0277  lr: 0.0015931  max_mem: 7418M
[12/11 23:26:45 d2.utils.events]:  eta: 0:35:09  iter: 3959  total_loss: 2.534  loss_sem_seg: 1.208  loss_center: 0.5425  loss_offset: 0.6915  time: 0.3499  data_time: 0.0257  lr: 0.0015883  max_mem: 7418M
[12/11 23:26:52 d2.utils.events]:  eta: 0:35:01  iter: 3979  total_loss: 2.602  loss_sem_seg: 1.395  loss_center: 0.589  loss_offset: 0.5487  time: 0.3499  data_time: 0.0255  lr: 0.0015836  max_mem: 7418M
[12/11 23:26:59 d2.utils.events]:  eta: 0:34:54  iter: 3999  total_loss: 2.516  loss_sem_seg: 1.268  loss_center: 0.5388  loss_offset: 0.4967  time: 0.3499  data_time: 0.0246  lr: 0.0015789  max_mem: 7418M
[12/11 23:27:06 d2.utils.events]:  eta: 0:34:47  iter: 4019  total_loss: 2.669  loss_sem_seg: 1.249  loss_center: 0.5716  loss_offset: 0.646  time: 0.3499  data_time: 0.0245  lr: 0.0015741  max_mem: 7418M
[12/11 23:27:13 d2.utils.events]:  eta: 0:34:40  iter: 4039  total_loss: 2.367  loss_sem_seg: 1.267  loss_center: 0.5445  loss_offset: 0.5774  time: 0.3499  data_time: 0.0273  lr: 0.0015694  max_mem: 7418M
[12/11 23:27:20 d2.utils.events]:  eta: 0:34:33  iter: 4059  total_loss: 2.336  loss_sem_seg: 1.16  loss_center: 0.568  loss_offset: 0.7421  time: 0.3499  data_time: 0.0258  lr: 0.0015646  max_mem: 7418M
[12/11 23:27:27 d2.utils.events]:  eta: 0:34:26  iter: 4079  total_loss: 2.31  loss_sem_seg: 1.174  loss_center: 0.4762  loss_offset: 0.6968  time: 0.3499  data_time: 0.0263  lr: 0.0015599  max_mem: 7418M
[12/11 23:27:34 d2.utils.events]:  eta: 0:34:18  iter: 4099  total_loss: 2.555  loss_sem_seg: 1.271  loss_center: 0.5934  loss_offset: 0.6932  time: 0.3499  data_time: 0.0268  lr: 0.0015552  max_mem: 7418M
[12/11 23:27:41 d2.utils.events]:  eta: 0:34:12  iter: 4119  total_loss: 2.417  loss_sem_seg: 1.284  loss_center: 0.4983  loss_offset: 0.6758  time: 0.3499  data_time: 0.0262  lr: 0.0015504  max_mem: 7418M
[12/11 23:27:48 d2.utils.events]:  eta: 0:34:05  iter: 4139  total_loss: 2.155  loss_sem_seg: 1.104  loss_center: 0.5658  loss_offset: 0.5567  time: 0.3499  data_time: 0.0266  lr: 0.0015457  max_mem: 7418M
[12/11 23:27:55 d2.utils.events]:  eta: 0:33:58  iter: 4159  total_loss: 2.499  loss_sem_seg: 1.117  loss_center: 0.5657  loss_offset: 0.7753  time: 0.3499  data_time: 0.0271  lr: 0.0015409  max_mem: 7418M
[12/11 23:28:02 d2.utils.events]:  eta: 0:33:51  iter: 4179  total_loss: 2.467  loss_sem_seg: 1.202  loss_center: 0.5007  loss_offset: 0.654  time: 0.3499  data_time: 0.0292  lr: 0.0015362  max_mem: 7418M
[12/11 23:28:09 d2.utils.events]:  eta: 0:33:44  iter: 4199  total_loss: 2.299  loss_sem_seg: 0.9792  loss_center: 0.5186  loss_offset: 0.5252  time: 0.3499  data_time: 0.0259  lr: 0.0015314  max_mem: 7418M
[12/11 23:28:16 d2.utils.events]:  eta: 0:33:37  iter: 4219  total_loss: 2.126  loss_sem_seg: 1.08  loss_center: 0.4666  loss_offset: 0.5084  time: 0.3499  data_time: 0.0278  lr: 0.0015267  max_mem: 7418M
[12/11 23:28:23 d2.utils.events]:  eta: 0:33:30  iter: 4239  total_loss: 2.374  loss_sem_seg: 1.151  loss_center: 0.5531  loss_offset: 0.6354  time: 0.3499  data_time: 0.0239  lr: 0.0015219  max_mem: 7418M
[12/11 23:28:30 d2.utils.events]:  eta: 0:33:23  iter: 4259  total_loss: 2.421  loss_sem_seg: 1.161  loss_center: 0.5554  loss_offset: 0.6554  time: 0.3499  data_time: 0.0254  lr: 0.0015172  max_mem: 7418M
[12/11 23:28:37 d2.utils.events]:  eta: 0:33:16  iter: 4279  total_loss: 2.174  loss_sem_seg: 1.116  loss_center: 0.4869  loss_offset: 0.5324  time: 0.3499  data_time: 0.0250  lr: 0.0015124  max_mem: 7418M
[12/11 23:28:44 d2.utils.events]:  eta: 0:33:09  iter: 4299  total_loss: 2.502  loss_sem_seg: 1.067  loss_center: 0.7072  loss_offset: 0.7125  time: 0.3499  data_time: 0.0267  lr: 0.0015076  max_mem: 7418M
[12/11 23:28:51 d2.utils.events]:  eta: 0:33:02  iter: 4319  total_loss: 2.002  loss_sem_seg: 0.9904  loss_center: 0.4924  loss_offset: 0.5769  time: 0.3499  data_time: 0.0277  lr: 0.0015029  max_mem: 7418M
[12/11 23:28:58 d2.utils.events]:  eta: 0:32:55  iter: 4339  total_loss: 2.243  loss_sem_seg: 0.9559  loss_center: 0.5537  loss_offset: 0.6864  time: 0.3499  data_time: 0.0246  lr: 0.0014981  max_mem: 7418M
[12/11 23:29:05 d2.utils.events]:  eta: 0:32:47  iter: 4359  total_loss: 2.478  loss_sem_seg: 1.073  loss_center: 0.4712  loss_offset: 0.7008  time: 0.3499  data_time: 0.0270  lr: 0.0014933  max_mem: 7418M
[12/11 23:29:12 d2.utils.events]:  eta: 0:32:40  iter: 4379  total_loss: 2.386  loss_sem_seg: 1.158  loss_center: 0.4484  loss_offset: 0.6537  time: 0.3499  data_time: 0.0268  lr: 0.0014886  max_mem: 7418M
[12/11 23:29:19 d2.utils.events]:  eta: 0:32:34  iter: 4399  total_loss: 2.291  loss_sem_seg: 1.12  loss_center: 0.5898  loss_offset: 0.591  time: 0.3499  data_time: 0.0249  lr: 0.0014838  max_mem: 7418M
[12/11 23:29:26 d2.utils.events]:  eta: 0:32:27  iter: 4419  total_loss: 2.136  loss_sem_seg: 1.032  loss_center: 0.5328  loss_offset: 0.5315  time: 0.3499  data_time: 0.0260  lr: 0.001479  max_mem: 7418M
[12/11 23:29:33 d2.utils.events]:  eta: 0:32:20  iter: 4439  total_loss: 2.401  loss_sem_seg: 1.063  loss_center: 0.6868  loss_offset: 0.5601  time: 0.3499  data_time: 0.0247  lr: 0.0014743  max_mem: 7418M
[12/11 23:29:40 d2.utils.events]:  eta: 0:32:13  iter: 4459  total_loss: 2.148  loss_sem_seg: 0.9981  loss_center: 0.4074  loss_offset: 0.5381  time: 0.3499  data_time: 0.0238  lr: 0.0014695  max_mem: 7418M
[12/11 23:29:47 d2.utils.events]:  eta: 0:32:06  iter: 4479  total_loss: 2.273  loss_sem_seg: 1.077  loss_center: 0.6683  loss_offset: 0.5522  time: 0.3499  data_time: 0.0277  lr: 0.0014647  max_mem: 7418M
[12/11 23:29:54 d2.utils.events]:  eta: 0:31:58  iter: 4499  total_loss: 2.218  loss_sem_seg: 1.077  loss_center: 0.4569  loss_offset: 0.5571  time: 0.3499  data_time: 0.0254  lr: 0.0014599  max_mem: 7418M
[12/11 23:30:01 d2.utils.events]:  eta: 0:31:52  iter: 4519  total_loss: 2.523  loss_sem_seg: 1.285  loss_center: 0.5976  loss_offset: 0.7972  time: 0.3499  data_time: 0.0262  lr: 0.0014552  max_mem: 7418M
[12/11 23:30:08 d2.utils.events]:  eta: 0:31:45  iter: 4539  total_loss: 2.238  loss_sem_seg: 1.204  loss_center: 0.5061  loss_offset: 0.5724  time: 0.3499  data_time: 0.0272  lr: 0.0014504  max_mem: 7418M
[12/11 23:30:15 d2.utils.events]:  eta: 0:31:37  iter: 4559  total_loss: 2.429  loss_sem_seg: 1.225  loss_center: 0.4265  loss_offset: 0.5843  time: 0.3499  data_time: 0.0269  lr: 0.0014456  max_mem: 7418M
[12/11 23:30:22 d2.utils.events]:  eta: 0:31:30  iter: 4579  total_loss: 2.227  loss_sem_seg: 1.104  loss_center: 0.5904  loss_offset: 0.5522  time: 0.3499  data_time: 0.0261  lr: 0.0014408  max_mem: 7418M
[12/11 23:30:29 d2.utils.events]:  eta: 0:31:23  iter: 4599  total_loss: 2.146  loss_sem_seg: 1.215  loss_center: 0.5657  loss_offset: 0.5867  time: 0.3499  data_time: 0.0261  lr: 0.001436  max_mem: 7418M
[12/11 23:30:36 d2.utils.events]:  eta: 0:31:16  iter: 4619  total_loss: 2.353  loss_sem_seg: 1.115  loss_center: 0.5657  loss_offset: 0.4937  time: 0.3499  data_time: 0.0262  lr: 0.0014313  max_mem: 7418M
[12/11 23:30:43 d2.utils.events]:  eta: 0:31:09  iter: 4639  total_loss: 2.432  loss_sem_seg: 1.108  loss_center: 0.5654  loss_offset: 0.5246  time: 0.3499  data_time: 0.0266  lr: 0.0014265  max_mem: 7418M
[12/11 23:30:50 d2.utils.events]:  eta: 0:31:03  iter: 4659  total_loss: 2.264  loss_sem_seg: 1.057  loss_center: 0.5668  loss_offset: 0.576  time: 0.3499  data_time: 0.0261  lr: 0.0014217  max_mem: 7418M
[12/11 23:30:57 d2.utils.events]:  eta: 0:30:56  iter: 4679  total_loss: 2.206  loss_sem_seg: 1.245  loss_center: 0.5072  loss_offset: 0.5774  time: 0.3499  data_time: 0.0263  lr: 0.0014169  max_mem: 7418M
[12/11 23:31:04 d2.utils.events]:  eta: 0:30:48  iter: 4699  total_loss: 2.175  loss_sem_seg: 1.08  loss_center: 0.5366  loss_offset: 0.6148  time: 0.3499  data_time: 0.0255  lr: 0.0014121  max_mem: 7418M
[12/11 23:31:11 d2.utils.events]:  eta: 0:30:41  iter: 4719  total_loss: 2.269  loss_sem_seg: 1.031  loss_center: 0.5677  loss_offset: 0.5288  time: 0.3499  data_time: 0.0257  lr: 0.0014073  max_mem: 7418M
[12/11 23:31:18 d2.utils.events]:  eta: 0:30:33  iter: 4739  total_loss: 2.237  loss_sem_seg: 0.9607  loss_center: 0.629  loss_offset: 0.6373  time: 0.3499  data_time: 0.0254  lr: 0.0014025  max_mem: 7418M
[12/11 23:31:25 d2.utils.events]:  eta: 0:30:27  iter: 4759  total_loss: 2.458  loss_sem_seg: 1.219  loss_center: 0.4838  loss_offset: 0.6481  time: 0.3499  data_time: 0.0273  lr: 0.0013977  max_mem: 7418M
[12/11 23:31:32 d2.utils.events]:  eta: 0:30:20  iter: 4779  total_loss: 2.383  loss_sem_seg: 1.033  loss_center: 0.6548  loss_offset: 0.63  time: 0.3499  data_time: 0.0268  lr: 0.0013929  max_mem: 7418M
[12/11 23:31:39 d2.utils.events]:  eta: 0:30:12  iter: 4799  total_loss: 2.104  loss_sem_seg: 1.163  loss_center: 0.5366  loss_offset: 0.5805  time: 0.3499  data_time: 0.0237  lr: 0.0013881  max_mem: 7418M
[12/11 23:31:46 d2.utils.events]:  eta: 0:30:06  iter: 4819  total_loss: 2.488  loss_sem_seg: 1.173  loss_center: 0.4927  loss_offset: 0.6264  time: 0.3499  data_time: 0.0260  lr: 0.0013833  max_mem: 7418M
[12/11 23:31:53 d2.utils.events]:  eta: 0:30:00  iter: 4839  total_loss: 2.564  loss_sem_seg: 1.249  loss_center: 0.5434  loss_offset: 0.615  time: 0.3499  data_time: 0.0278  lr: 0.0013785  max_mem: 7418M
[12/11 23:32:00 d2.utils.events]:  eta: 0:29:53  iter: 4859  total_loss: 2.38  loss_sem_seg: 1.092  loss_center: 0.5866  loss_offset: 0.5769  time: 0.3499  data_time: 0.0258  lr: 0.0013737  max_mem: 7418M
[12/11 23:32:07 d2.utils.events]:  eta: 0:29:45  iter: 4879  total_loss: 2.273  loss_sem_seg: 1.123  loss_center: 0.4634  loss_offset: 0.5852  time: 0.3499  data_time: 0.0257  lr: 0.0013689  max_mem: 7418M
[12/11 23:32:14 d2.utils.events]:  eta: 0:29:39  iter: 4899  total_loss: 2.138  loss_sem_seg: 0.9362  loss_center: 0.5232  loss_offset: 0.6128  time: 0.3499  data_time: 0.0258  lr: 0.001364  max_mem: 7418M
[12/11 23:32:21 d2.utils.events]:  eta: 0:29:32  iter: 4919  total_loss: 2.073  loss_sem_seg: 0.9119  loss_center: 0.4636  loss_offset: 0.5355  time: 0.3499  data_time: 0.0256  lr: 0.0013592  max_mem: 7418M
[12/11 23:32:28 d2.utils.events]:  eta: 0:29:24  iter: 4939  total_loss: 2.317  loss_sem_seg: 1.134  loss_center: 0.4459  loss_offset: 0.642  time: 0.3499  data_time: 0.0243  lr: 0.0013544  max_mem: 7418M
[12/11 23:32:35 d2.utils.events]:  eta: 0:29:17  iter: 4959  total_loss: 2.309  loss_sem_seg: 1.076  loss_center: 0.6663  loss_offset: 0.6778  time: 0.3499  data_time: 0.0257  lr: 0.0013496  max_mem: 7418M
[12/11 23:32:42 d2.utils.events]:  eta: 0:29:09  iter: 4979  total_loss: 2.004  loss_sem_seg: 0.9983  loss_center: 0.5296  loss_offset: 0.5155  time: 0.3499  data_time: 0.0258  lr: 0.0013448  max_mem: 7418M
[12/11 23:32:49 fvcore.common.checkpoint]: Saving checkpoint to ./output/model_0004999.pth
[12/11 23:32:50 d2.utils.events]:  eta: 0:29:02  iter: 4999  total_loss: 1.843  loss_sem_seg: 0.9356  loss_center: 0.4314  loss_offset: 0.5698  time: 0.3499  data_time: 0.0266  lr: 0.00134  max_mem: 7418M
[12/11 23:32:58 d2.utils.events]:  eta: 0:28:56  iter: 5019  total_loss: 2.172  loss_sem_seg: 1.078  loss_center: 0.4834  loss_offset: 0.549  time: 0.3499  data_time: 0.0279  lr: 0.0013351  max_mem: 7418M
[12/11 23:33:05 d2.utils.events]:  eta: 0:28:49  iter: 5039  total_loss: 2.205  loss_sem_seg: 0.9864  loss_center: 0.5382  loss_offset: 0.6288  time: 0.3499  data_time: 0.0261  lr: 0.0013303  max_mem: 7418M
[12/11 23:33:12 d2.utils.events]:  eta: 0:28:42  iter: 5059  total_loss: 2.38  loss_sem_seg: 1.146  loss_center: 0.5631  loss_offset: 0.6145  time: 0.3499  data_time: 0.0269  lr: 0.0013255  max_mem: 7418M
[12/11 23:33:19 d2.utils.events]:  eta: 0:28:36  iter: 5079  total_loss: 2.232  loss_sem_seg: 1.002  loss_center: 0.5532  loss_offset: 0.6032  time: 0.3499  data_time: 0.0284  lr: 0.0013207  max_mem: 7418M
[12/11 23:33:26 d2.utils.events]:  eta: 0:28:29  iter: 5099  total_loss: 2.435  loss_sem_seg: 1.103  loss_center: 0.6269  loss_offset: 0.591  time: 0.3499  data_time: 0.0241  lr: 0.0013158  max_mem: 7418M
[12/11 23:33:33 d2.utils.events]:  eta: 0:28:22  iter: 5119  total_loss: 2.501  loss_sem_seg: 1.168  loss_center: 0.5688  loss_offset: 0.5541  time: 0.3499  data_time: 0.0253  lr: 0.001311  max_mem: 7418M
[12/11 23:33:40 d2.utils.events]:  eta: 0:28:15  iter: 5139  total_loss: 2.25  loss_sem_seg: 1.135  loss_center: 0.4534  loss_offset: 0.5149  time: 0.3499  data_time: 0.0269  lr: 0.0013062  max_mem: 7418M
[12/11 23:33:47 d2.utils.events]:  eta: 0:28:08  iter: 5159  total_loss: 2.32  loss_sem_seg: 1.189  loss_center: 0.5304  loss_offset: 0.6648  time: 0.3499  data_time: 0.0261  lr: 0.0013013  max_mem: 7418M
[12/11 23:33:54 d2.utils.events]:  eta: 0:28:01  iter: 5179  total_loss: 2.326  loss_sem_seg: 1.08  loss_center: 0.5373  loss_offset: 0.6081  time: 0.3499  data_time: 0.0245  lr: 0.0012965  max_mem: 7418M
[12/11 23:34:01 d2.utils.events]:  eta: 0:27:53  iter: 5199  total_loss: 2.013  loss_sem_seg: 0.9982  loss_center: 0.5411  loss_offset: 0.5144  time: 0.3499  data_time: 0.0259  lr: 0.0012916  max_mem: 7418M
[12/11 23:34:08 d2.utils.events]:  eta: 0:27:46  iter: 5219  total_loss: 2.453  loss_sem_seg: 1.103  loss_center: 0.7644  loss_offset: 0.6137  time: 0.3499  data_time: 0.0269  lr: 0.0012868  max_mem: 7418M
[12/11 23:34:15 d2.utils.events]:  eta: 0:27:39  iter: 5239  total_loss: 2.34  loss_sem_seg: 1.033  loss_center: 0.6047  loss_offset: 0.5878  time: 0.3499  data_time: 0.0265  lr: 0.0012819  max_mem: 7418M
[12/11 23:34:22 d2.utils.events]:  eta: 0:27:32  iter: 5259  total_loss: 2.247  loss_sem_seg: 0.9943  loss_center: 0.557  loss_offset: 0.5758  time: 0.3499  data_time: 0.0266  lr: 0.0012771  max_mem: 7418M
[12/11 23:34:29 d2.utils.events]:  eta: 0:27:25  iter: 5279  total_loss: 2.104  loss_sem_seg: 1.031  loss_center: 0.5288  loss_offset: 0.5046  time: 0.3499  data_time: 0.0252  lr: 0.0012722  max_mem: 7418M
[12/11 23:34:36 d2.utils.events]:  eta: 0:27:18  iter: 5299  total_loss: 2.268  loss_sem_seg: 1.053  loss_center: 0.5623  loss_offset: 0.5979  time: 0.3499  data_time: 0.0279  lr: 0.0012674  max_mem: 7418M
[12/11 23:34:43 d2.utils.events]:  eta: 0:27:10  iter: 5319  total_loss: 2.221  loss_sem_seg: 1.073  loss_center: 0.5984  loss_offset: 0.6222  time: 0.3499  data_time: 0.0242  lr: 0.0012625  max_mem: 7418M
[12/11 23:34:50 d2.utils.events]:  eta: 0:27:04  iter: 5339  total_loss: 2.242  loss_sem_seg: 1.045  loss_center: 0.4482  loss_offset: 0.6221  time: 0.3499  data_time: 0.0274  lr: 0.0012577  max_mem: 7418M
[12/11 23:34:57 d2.utils.events]:  eta: 0:26:58  iter: 5359  total_loss: 2.207  loss_sem_seg: 0.9498  loss_center: 0.4798  loss_offset: 0.5851  time: 0.3499  data_time: 0.0252  lr: 0.0012528  max_mem: 7418M
[12/11 23:35:04 d2.utils.events]:  eta: 0:26:51  iter: 5379  total_loss: 2.444  loss_sem_seg: 1.162  loss_center: 0.4633  loss_offset: 0.6828  time: 0.3499  data_time: 0.0260  lr: 0.001248  max_mem: 7418M
[12/11 23:35:11 d2.utils.events]:  eta: 0:26:44  iter: 5399  total_loss: 2.187  loss_sem_seg: 0.961  loss_center: 0.6901  loss_offset: 0.5966  time: 0.3499  data_time: 0.0255  lr: 0.0012431  max_mem: 7418M
[12/11 23:35:18 d2.utils.events]:  eta: 0:26:37  iter: 5419  total_loss: 2.174  loss_sem_seg: 0.9606  loss_center: 0.5436  loss_offset: 0.5667  time: 0.3499  data_time: 0.0266  lr: 0.0012382  max_mem: 7418M
[12/11 23:35:25 d2.utils.events]:  eta: 0:26:30  iter: 5439  total_loss: 2.302  loss_sem_seg: 1.051  loss_center: 0.5662  loss_offset: 0.7269  time: 0.3499  data_time: 0.0245  lr: 0.0012334  max_mem: 7418M
[12/11 23:35:32 d2.utils.events]:  eta: 0:26:23  iter: 5459  total_loss: 2.181  loss_sem_seg: 0.9484  loss_center: 0.5552  loss_offset: 0.573  time: 0.3499  data_time: 0.0261  lr: 0.0012285  max_mem: 7418M
[12/11 23:35:39 d2.utils.events]:  eta: 0:26:15  iter: 5479  total_loss: 2.02  loss_sem_seg: 1.032  loss_center: 0.447  loss_offset: 0.5625  time: 0.3499  data_time: 0.0257  lr: 0.0012236  max_mem: 7418M
[12/11 23:35:46 d2.utils.events]:  eta: 0:26:08  iter: 5499  total_loss: 2.169  loss_sem_seg: 1.091  loss_center: 0.5566  loss_offset: 0.6071  time: 0.3499  data_time: 0.0260  lr: 0.0012188  max_mem: 7418M
[12/11 23:35:53 d2.utils.events]:  eta: 0:26:01  iter: 5519  total_loss: 2.188  loss_sem_seg: 1.025  loss_center: 0.5663  loss_offset: 0.5779  time: 0.3499  data_time: 0.0257  lr: 0.0012139  max_mem: 7418M
[12/11 23:36:00 d2.utils.events]:  eta: 0:25:54  iter: 5539  total_loss: 2.317  loss_sem_seg: 1.219  loss_center: 0.5184  loss_offset: 0.566  time: 0.3499  data_time: 0.0253  lr: 0.001209  max_mem: 7418M
[12/11 23:36:07 d2.utils.events]:  eta: 0:25:47  iter: 5559  total_loss: 2.123  loss_sem_seg: 1.095  loss_center: 0.5141  loss_offset: 0.485  time: 0.3499  data_time: 0.0263  lr: 0.0012041  max_mem: 7418M
[12/11 23:36:14 d2.utils.events]:  eta: 0:25:40  iter: 5579  total_loss: 2.27  loss_sem_seg: 1.118  loss_center: 0.7414  loss_offset: 0.5162  time: 0.3499  data_time: 0.0260  lr: 0.0011992  max_mem: 7418M
[12/11 23:36:21 d2.utils.events]:  eta: 0:25:32  iter: 5599  total_loss: 2.059  loss_sem_seg: 1.049  loss_center: 0.3826  loss_offset: 0.5225  time: 0.3499  data_time: 0.0272  lr: 0.0011944  max_mem: 7418M
[12/11 23:36:28 d2.utils.events]:  eta: 0:25:25  iter: 5619  total_loss: 2.225  loss_sem_seg: 1.094  loss_center: 0.3906  loss_offset: 0.57  time: 0.3499  data_time: 0.0267  lr: 0.0011895  max_mem: 7418M
[12/11 23:36:35 d2.utils.events]:  eta: 0:25:19  iter: 5639  total_loss: 2.337  loss_sem_seg: 1.088  loss_center: 0.3931  loss_offset: 0.539  time: 0.3499  data_time: 0.0261  lr: 0.0011846  max_mem: 7418M
[12/11 23:36:42 d2.utils.events]:  eta: 0:25:11  iter: 5659  total_loss: 2.149  loss_sem_seg: 1.106  loss_center: 0.5548  loss_offset: 0.6172  time: 0.3499  data_time: 0.0242  lr: 0.0011797  max_mem: 7418M
[12/11 23:36:49 d2.utils.events]:  eta: 0:25:04  iter: 5679  total_loss: 2.131  loss_sem_seg: 1.073  loss_center: 0.5038  loss_offset: 0.5123  time: 0.3499  data_time: 0.0250  lr: 0.0011748  max_mem: 7418M
[12/11 23:36:56 d2.utils.events]:  eta: 0:24:57  iter: 5699  total_loss: 2.297  loss_sem_seg: 1.101  loss_center: 0.4878  loss_offset: 0.6121  time: 0.3499  data_time: 0.0243  lr: 0.0011699  max_mem: 7418M
[12/11 23:37:03 d2.utils.events]:  eta: 0:24:50  iter: 5719  total_loss: 2.413  loss_sem_seg: 1.042  loss_center: 0.5742  loss_offset: 0.6244  time: 0.3499  data_time: 0.0246  lr: 0.001165  max_mem: 7418M
[12/11 23:37:10 d2.utils.events]:  eta: 0:24:43  iter: 5739  total_loss: 2.402  loss_sem_seg: 0.994  loss_center: 0.5221  loss_offset: 0.6661  time: 0.3499  data_time: 0.0271  lr: 0.0011601  max_mem: 7418M
[12/11 23:37:17 d2.utils.events]:  eta: 0:24:36  iter: 5759  total_loss: 2.307  loss_sem_seg: 1.147  loss_center: 0.4895  loss_offset: 0.5065  time: 0.3498  data_time: 0.0254  lr: 0.0011552  max_mem: 7418M
[12/11 23:37:24 d2.utils.events]:  eta: 0:24:29  iter: 5779  total_loss: 2.022  loss_sem_seg: 0.9857  loss_center: 0.4796  loss_offset: 0.5088  time: 0.3498  data_time: 0.0249  lr: 0.0011503  max_mem: 7418M
[12/11 23:37:31 d2.utils.events]:  eta: 0:24:22  iter: 5799  total_loss: 2.186  loss_sem_seg: 1.051  loss_center: 0.5592  loss_offset: 0.5531  time: 0.3498  data_time: 0.0268  lr: 0.0011454  max_mem: 7418M
[12/11 23:37:38 d2.utils.events]:  eta: 0:24:15  iter: 5819  total_loss: 2.23  loss_sem_seg: 1.076  loss_center: 0.464  loss_offset: 0.5559  time: 0.3498  data_time: 0.0253  lr: 0.0011405  max_mem: 7418M
[12/11 23:37:45 d2.utils.events]:  eta: 0:24:08  iter: 5839  total_loss: 2.594  loss_sem_seg: 1.14  loss_center: 0.572  loss_offset: 0.6416  time: 0.3498  data_time: 0.0258  lr: 0.0011356  max_mem: 7418M
[12/11 23:37:52 d2.utils.events]:  eta: 0:24:01  iter: 5859  total_loss: 2.051  loss_sem_seg: 0.978  loss_center: 0.4565  loss_offset: 0.5918  time: 0.3498  data_time: 0.0263  lr: 0.0011307  max_mem: 7418M
[12/11 23:37:59 d2.utils.events]:  eta: 0:23:54  iter: 5879  total_loss: 2.43  loss_sem_seg: 1.116  loss_center: 0.5304  loss_offset: 0.6106  time: 0.3498  data_time: 0.0261  lr: 0.0011258  max_mem: 7418M
[12/11 23:38:06 d2.utils.events]:  eta: 0:23:47  iter: 5899  total_loss: 2.201  loss_sem_seg: 1.035  loss_center: 0.4697  loss_offset: 0.5857  time: 0.3498  data_time: 0.0266  lr: 0.0011208  max_mem: 7418M
[12/11 23:38:13 d2.utils.events]:  eta: 0:23:40  iter: 5919  total_loss: 1.899  loss_sem_seg: 0.9313  loss_center: 0.5378  loss_offset: 0.5155  time: 0.3498  data_time: 0.0247  lr: 0.0011159  max_mem: 7418M
[12/11 23:38:20 d2.utils.events]:  eta: 0:23:33  iter: 5939  total_loss: 2.197  loss_sem_seg: 0.985  loss_center: 0.4936  loss_offset: 0.6351  time: 0.3498  data_time: 0.0263  lr: 0.001111  max_mem: 7418M
[12/11 23:38:27 d2.utils.events]:  eta: 0:23:26  iter: 5959  total_loss: 2.134  loss_sem_seg: 0.918  loss_center: 0.5039  loss_offset: 0.6305  time: 0.3498  data_time: 0.0258  lr: 0.0011061  max_mem: 7418M
[12/11 23:38:34 d2.utils.events]:  eta: 0:23:19  iter: 5979  total_loss: 2.237  loss_sem_seg: 1.039  loss_center: 0.393  loss_offset: 0.5341  time: 0.3498  data_time: 0.0253  lr: 0.0011011  max_mem: 7418M
[12/11 23:38:41 d2.utils.events]:  eta: 0:23:13  iter: 5999  total_loss: 2.248  loss_sem_seg: 1.098  loss_center: 0.4983  loss_offset: 0.5936  time: 0.3498  data_time: 0.0266  lr: 0.0010962  max_mem: 7418M
[12/11 23:38:47 d2.utils.events]:  eta: 0:23:05  iter: 6019  total_loss: 2.091  loss_sem_seg: 0.9258  loss_center: 0.4367  loss_offset: 0.6161  time: 0.3498  data_time: 0.0228  lr: 0.0010913  max_mem: 7418M
[12/11 23:38:54 d2.utils.events]:  eta: 0:22:58  iter: 6039  total_loss: 2.184  loss_sem_seg: 0.8313  loss_center: 0.6331  loss_offset: 0.5963  time: 0.3498  data_time: 0.0253  lr: 0.0010863  max_mem: 7418M
[12/11 23:39:02 d2.utils.events]:  eta: 0:22:51  iter: 6059  total_loss: 2.173  loss_sem_seg: 0.9972  loss_center: 0.4327  loss_offset: 0.6639  time: 0.3498  data_time: 0.0269  lr: 0.0010814  max_mem: 7418M
[12/11 23:39:09 d2.utils.events]:  eta: 0:22:43  iter: 6079  total_loss: 2.448  loss_sem_seg: 1.135  loss_center: 0.5921  loss_offset: 0.5871  time: 0.3498  data_time: 0.0267  lr: 0.0010765  max_mem: 7418M
[12/11 23:39:16 d2.utils.events]:  eta: 0:22:37  iter: 6099  total_loss: 2.356  loss_sem_seg: 1.088  loss_center: 0.5092  loss_offset: 0.6457  time: 0.3498  data_time: 0.0253  lr: 0.0010715  max_mem: 7418M
[12/11 23:39:22 d2.utils.events]:  eta: 0:22:29  iter: 6119  total_loss: 2.197  loss_sem_seg: 1.004  loss_center: 0.4915  loss_offset: 0.6309  time: 0.3498  data_time: 0.0250  lr: 0.0010666  max_mem: 7418M
[12/11 23:39:30 d2.utils.events]:  eta: 0:22:22  iter: 6139  total_loss: 2.061  loss_sem_seg: 0.9195  loss_center: 0.4193  loss_offset: 0.5788  time: 0.3498  data_time: 0.0254  lr: 0.0010616  max_mem: 7418M
[12/11 23:39:36 d2.utils.events]:  eta: 0:22:15  iter: 6159  total_loss: 2.246  loss_sem_seg: 1.013  loss_center: 0.6337  loss_offset: 0.5614  time: 0.3498  data_time: 0.0256  lr: 0.0010567  max_mem: 7418M
[12/11 23:39:44 d2.utils.events]:  eta: 0:22:09  iter: 6179  total_loss: 2.317  loss_sem_seg: 0.9829  loss_center: 0.5336  loss_offset: 0.5175  time: 0.3498  data_time: 0.0269  lr: 0.0010517  max_mem: 7418M
[12/11 23:39:51 d2.utils.events]:  eta: 0:22:02  iter: 6199  total_loss: 2.272  loss_sem_seg: 1.038  loss_center: 0.5937  loss_offset: 0.6112  time: 0.3498  data_time: 0.0246  lr: 0.0010468  max_mem: 7418M
[12/11 23:39:58 d2.utils.events]:  eta: 0:21:54  iter: 6219  total_loss: 2.466  loss_sem_seg: 1.278  loss_center: 0.574  loss_offset: 0.6113  time: 0.3498  data_time: 0.0264  lr: 0.0010418  max_mem: 7418M
[12/11 23:40:05 d2.utils.events]:  eta: 0:21:48  iter: 6239  total_loss: 2.114  loss_sem_seg: 0.9612  loss_center: 0.5139  loss_offset: 0.4936  time: 0.3498  data_time: 0.0250  lr: 0.0010368  max_mem: 7418M
[12/11 23:40:12 d2.utils.events]:  eta: 0:21:41  iter: 6259  total_loss: 2.23  loss_sem_seg: 0.9963  loss_center: 0.5494  loss_offset: 0.6094  time: 0.3498  data_time: 0.0279  lr: 0.0010319  max_mem: 7418M
[12/11 23:40:18 d2.utils.events]:  eta: 0:21:34  iter: 6279  total_loss: 2.059  loss_sem_seg: 0.9137  loss_center: 0.4679  loss_offset: 0.561  time: 0.3498  data_time: 0.0232  lr: 0.0010269  max_mem: 7418M
[12/11 23:40:25 d2.utils.events]:  eta: 0:21:26  iter: 6299  total_loss: 2.031  loss_sem_seg: 0.9457  loss_center: 0.434  loss_offset: 0.5798  time: 0.3498  data_time: 0.0267  lr: 0.0010219  max_mem: 7418M
[12/11 23:40:32 d2.utils.events]:  eta: 0:21:19  iter: 6319  total_loss: 2.123  loss_sem_seg: 1.001  loss_center: 0.5452  loss_offset: 0.6426  time: 0.3498  data_time: 0.0248  lr: 0.001017  max_mem: 7418M
[12/11 23:40:39 d2.utils.events]:  eta: 0:21:12  iter: 6339  total_loss: 2.415  loss_sem_seg: 1.009  loss_center: 0.5665  loss_offset: 0.5947  time: 0.3498  data_time: 0.0246  lr: 0.001012  max_mem: 7418M
[12/11 23:40:46 d2.utils.events]:  eta: 0:21:05  iter: 6359  total_loss: 2.601  loss_sem_seg: 1.155  loss_center: 0.5711  loss_offset: 0.6606  time: 0.3498  data_time: 0.0247  lr: 0.001007  max_mem: 7418M
[12/11 23:40:53 d2.utils.events]:  eta: 0:20:58  iter: 6379  total_loss: 2.263  loss_sem_seg: 1.02  loss_center: 0.6777  loss_offset: 0.5029  time: 0.3498  data_time: 0.0257  lr: 0.001002  max_mem: 7418M
[12/11 23:41:00 d2.utils.events]:  eta: 0:20:52  iter: 6399  total_loss: 2.101  loss_sem_seg: 0.9921  loss_center: 0.4696  loss_offset: 0.4449  time: 0.3498  data_time: 0.0284  lr: 0.00099706  max_mem: 7418M
[12/11 23:41:07 d2.utils.events]:  eta: 0:20:44  iter: 6419  total_loss: 2.147  loss_sem_seg: 0.9875  loss_center: 0.4892  loss_offset: 0.5479  time: 0.3498  data_time: 0.0282  lr: 0.00099207  max_mem: 7418M
[12/11 23:41:15 d2.utils.events]:  eta: 0:20:38  iter: 6439  total_loss: 2.421  loss_sem_seg: 1.211  loss_center: 0.4942  loss_offset: 0.6796  time: 0.3498  data_time: 0.0269  lr: 0.00098709  max_mem: 7418M
[12/11 23:41:22 d2.utils.events]:  eta: 0:20:31  iter: 6459  total_loss: 2.262  loss_sem_seg: 0.9429  loss_center: 0.5263  loss_offset: 0.5419  time: 0.3498  data_time: 0.0267  lr: 0.00098209  max_mem: 7418M
[12/11 23:41:29 d2.utils.events]:  eta: 0:20:25  iter: 6479  total_loss: 2.013  loss_sem_seg: 0.9487  loss_center: 0.4772  loss_offset: 0.5256  time: 0.3498  data_time: 0.0274  lr: 0.0009771  max_mem: 7418M
[12/11 23:41:36 d2.utils.events]:  eta: 0:20:19  iter: 6499  total_loss: 2.303  loss_sem_seg: 1.021  loss_center: 0.5342  loss_offset: 0.4964  time: 0.3498  data_time: 0.0247  lr: 0.0009721  max_mem: 7418M
[12/11 23:41:42 d2.utils.events]:  eta: 0:20:11  iter: 6519  total_loss: 2.225  loss_sem_seg: 1.168  loss_center: 0.4618  loss_offset: 0.605  time: 0.3498  data_time: 0.0250  lr: 0.00096711  max_mem: 7418M
[12/11 23:41:49 d2.utils.events]:  eta: 0:20:05  iter: 6539  total_loss: 2.17  loss_sem_seg: 0.9139  loss_center: 0.5531  loss_offset: 0.571  time: 0.3498  data_time: 0.0264  lr: 0.0009621  max_mem: 7418M
[12/11 23:41:56 d2.utils.events]:  eta: 0:19:57  iter: 6559  total_loss: 2.295  loss_sem_seg: 1.043  loss_center: 0.5709  loss_offset: 0.5582  time: 0.3498  data_time: 0.0246  lr: 0.0009571  max_mem: 7418M
[12/11 23:42:03 d2.utils.events]:  eta: 0:19:50  iter: 6579  total_loss: 1.856  loss_sem_seg: 0.9041  loss_center: 0.4493  loss_offset: 0.5861  time: 0.3498  data_time: 0.0259  lr: 0.00095209  max_mem: 7418M
[12/11 23:42:10 d2.utils.events]:  eta: 0:19:43  iter: 6599  total_loss: 2.299  loss_sem_seg: 1.145  loss_center: 0.5686  loss_offset: 0.5635  time: 0.3498  data_time: 0.0243  lr: 0.00094708  max_mem: 7418M
[12/11 23:42:17 d2.utils.events]:  eta: 0:19:35  iter: 6619  total_loss: 1.825  loss_sem_seg: 0.8653  loss_center: 0.4571  loss_offset: 0.4662  time: 0.3498  data_time: 0.0248  lr: 0.00094206  max_mem: 7418M
[12/11 23:42:24 d2.utils.events]:  eta: 0:19:28  iter: 6639  total_loss: 2.037  loss_sem_seg: 0.9447  loss_center: 0.4896  loss_offset: 0.5106  time: 0.3498  data_time: 0.0252  lr: 0.00093705  max_mem: 7418M
[12/11 23:42:31 d2.utils.events]:  eta: 0:19:22  iter: 6659  total_loss: 2.015  loss_sem_seg: 1.072  loss_center: 0.4256  loss_offset: 0.5862  time: 0.3498  data_time: 0.0248  lr: 0.00093203  max_mem: 7418M
[12/11 23:42:38 d2.utils.events]:  eta: 0:19:15  iter: 6679  total_loss: 2.198  loss_sem_seg: 1.069  loss_center: 0.5006  loss_offset: 0.6115  time: 0.3497  data_time: 0.0242  lr: 0.000927  max_mem: 7418M
[12/11 23:42:45 d2.utils.events]:  eta: 0:19:08  iter: 6699  total_loss: 1.991  loss_sem_seg: 0.9238  loss_center: 0.5945  loss_offset: 0.5289  time: 0.3497  data_time: 0.0258  lr: 0.00092198  max_mem: 7418M
[12/11 23:42:52 d2.utils.events]:  eta: 0:19:01  iter: 6719  total_loss: 2.222  loss_sem_seg: 1.094  loss_center: 0.4262  loss_offset: 0.5192  time: 0.3497  data_time: 0.0262  lr: 0.00091695  max_mem: 7418M
[12/11 23:42:59 d2.utils.events]:  eta: 0:18:55  iter: 6739  total_loss: 2.306  loss_sem_seg: 1.054  loss_center: 0.5392  loss_offset: 0.6182  time: 0.3497  data_time: 0.0263  lr: 0.00091192  max_mem: 7418M
[12/11 23:43:06 d2.utils.events]:  eta: 0:18:48  iter: 6759  total_loss: 2.16  loss_sem_seg: 0.9784  loss_center: 0.5206  loss_offset: 0.553  time: 0.3497  data_time: 0.0241  lr: 0.00090688  max_mem: 7418M
[12/11 23:43:13 d2.utils.events]:  eta: 0:18:40  iter: 6779  total_loss: 2.333  loss_sem_seg: 1.249  loss_center: 0.4158  loss_offset: 0.6098  time: 0.3497  data_time: 0.0258  lr: 0.00090184  max_mem: 7418M
[12/11 23:43:20 d2.utils.events]:  eta: 0:18:34  iter: 6799  total_loss: 1.951  loss_sem_seg: 0.8443  loss_center: 0.5049  loss_offset: 0.6861  time: 0.3497  data_time: 0.0267  lr: 0.0008968  max_mem: 7418M
[12/11 23:43:27 d2.utils.events]:  eta: 0:18:28  iter: 6819  total_loss: 2.079  loss_sem_seg: 1.067  loss_center: 0.4508  loss_offset: 0.498  time: 0.3497  data_time: 0.0264  lr: 0.00089176  max_mem: 7418M
[12/11 23:43:34 d2.utils.events]:  eta: 0:18:21  iter: 6839  total_loss: 2.028  loss_sem_seg: 1  loss_center: 0.5563  loss_offset: 0.5294  time: 0.3497  data_time: 0.0278  lr: 0.00088671  max_mem: 7418M
[12/11 23:43:41 d2.utils.events]:  eta: 0:18:14  iter: 6859  total_loss: 2.266  loss_sem_seg: 1.067  loss_center: 0.5552  loss_offset: 0.5528  time: 0.3497  data_time: 0.0254  lr: 0.00088166  max_mem: 7418M
[12/11 23:43:48 d2.utils.events]:  eta: 0:18:07  iter: 6879  total_loss: 2.24  loss_sem_seg: 1.029  loss_center: 0.5562  loss_offset: 0.5951  time: 0.3497  data_time: 0.0245  lr: 0.00087661  max_mem: 7418M
[12/11 23:43:55 d2.utils.events]:  eta: 0:18:00  iter: 6899  total_loss: 2.098  loss_sem_seg: 0.9152  loss_center: 0.6342  loss_offset: 0.523  time: 0.3497  data_time: 0.0269  lr: 0.00087155  max_mem: 7418M
[12/11 23:44:02 d2.utils.events]:  eta: 0:17:54  iter: 6919  total_loss: 2.108  loss_sem_seg: 0.9864  loss_center: 0.5198  loss_offset: 0.5767  time: 0.3497  data_time: 0.0258  lr: 0.00086649  max_mem: 7418M
[12/11 23:44:09 d2.utils.events]:  eta: 0:17:46  iter: 6939  total_loss: 2.073  loss_sem_seg: 0.9999  loss_center: 0.5529  loss_offset: 0.5099  time: 0.3497  data_time: 0.0254  lr: 0.00086142  max_mem: 7418M
[12/11 23:44:16 d2.utils.events]:  eta: 0:17:39  iter: 6959  total_loss: 2.277  loss_sem_seg: 0.998  loss_center: 0.4775  loss_offset: 0.5588  time: 0.3497  data_time: 0.0239  lr: 0.00085636  max_mem: 7418M
[12/11 23:44:23 d2.utils.events]:  eta: 0:17:33  iter: 6979  total_loss: 2.131  loss_sem_seg: 0.8832  loss_center: 0.5022  loss_offset: 0.4567  time: 0.3497  data_time: 0.0261  lr: 0.00085129  max_mem: 7418M
[12/11 23:44:30 d2.utils.events]:  eta: 0:17:25  iter: 6999  total_loss: 2.372  loss_sem_seg: 1.063  loss_center: 0.4987  loss_offset: 0.6015  time: 0.3497  data_time: 0.0257  lr: 0.00084621  max_mem: 7418M
[12/11 23:44:37 d2.utils.events]:  eta: 0:17:18  iter: 7019  total_loss: 1.988  loss_sem_seg: 0.9753  loss_center: 0.473  loss_offset: 0.5033  time: 0.3497  data_time: 0.0235  lr: 0.00084114  max_mem: 7418M
[12/11 23:44:44 d2.utils.events]:  eta: 0:17:11  iter: 7039  total_loss: 2.075  loss_sem_seg: 0.8786  loss_center: 0.4713  loss_offset: 0.5704  time: 0.3497  data_time: 0.0262  lr: 0.00083605  max_mem: 7418M
[12/11 23:44:51 d2.utils.events]:  eta: 0:17:04  iter: 7059  total_loss: 2.224  loss_sem_seg: 0.9927  loss_center: 0.6269  loss_offset: 0.5922  time: 0.3497  data_time: 0.0258  lr: 0.00083097  max_mem: 7418M
[12/11 23:44:58 d2.utils.events]:  eta: 0:16:56  iter: 7079  total_loss: 2.256  loss_sem_seg: 1.067  loss_center: 0.5682  loss_offset: 0.6025  time: 0.3497  data_time: 0.0257  lr: 0.00082588  max_mem: 7418M
[12/11 23:45:05 d2.utils.events]:  eta: 0:16:49  iter: 7099  total_loss: 2.037  loss_sem_seg: 0.8981  loss_center: 0.4469  loss_offset: 0.6046  time: 0.3497  data_time: 0.0237  lr: 0.00082079  max_mem: 7418M
[12/11 23:45:12 d2.utils.events]:  eta: 0:16:42  iter: 7119  total_loss: 2.205  loss_sem_seg: 0.9012  loss_center: 0.4659  loss_offset: 0.5135  time: 0.3497  data_time: 0.0255  lr: 0.0008157  max_mem: 7418M
[12/11 23:45:19 d2.utils.events]:  eta: 0:16:35  iter: 7139  total_loss: 2.059  loss_sem_seg: 1.073  loss_center: 0.5258  loss_offset: 0.4941  time: 0.3497  data_time: 0.0261  lr: 0.0008106  max_mem: 7418M
[12/11 23:45:26 d2.utils.events]:  eta: 0:16:28  iter: 7159  total_loss: 1.945  loss_sem_seg: 0.7326  loss_center: 0.5771  loss_offset: 0.5876  time: 0.3497  data_time: 0.0249  lr: 0.0008055  max_mem: 7418M
[12/11 23:45:33 d2.utils.events]:  eta: 0:16:21  iter: 7179  total_loss: 2.186  loss_sem_seg: 1.053  loss_center: 0.5665  loss_offset: 0.4445  time: 0.3497  data_time: 0.0262  lr: 0.00080039  max_mem: 7418M
[12/11 23:45:40 d2.utils.events]:  eta: 0:16:13  iter: 7199  total_loss: 2.338  loss_sem_seg: 1.002  loss_center: 0.5957  loss_offset: 0.6756  time: 0.3497  data_time: 0.0248  lr: 0.00079528  max_mem: 7418M
[12/11 23:45:47 d2.utils.events]:  eta: 0:16:06  iter: 7219  total_loss: 2.293  loss_sem_seg: 1.06  loss_center: 0.5259  loss_offset: 0.4467  time: 0.3497  data_time: 0.0245  lr: 0.00079017  max_mem: 7418M
[12/11 23:45:54 d2.utils.events]:  eta: 0:16:00  iter: 7239  total_loss: 2.055  loss_sem_seg: 0.9445  loss_center: 0.4897  loss_offset: 0.4663  time: 0.3497  data_time: 0.0279  lr: 0.00078505  max_mem: 7418M
[12/11 23:46:01 d2.utils.events]:  eta: 0:15:53  iter: 7259  total_loss: 1.919  loss_sem_seg: 0.9655  loss_center: 0.4876  loss_offset: 0.4427  time: 0.3497  data_time: 0.0253  lr: 0.00077993  max_mem: 7418M
[12/11 23:46:08 d2.utils.events]:  eta: 0:15:46  iter: 7279  total_loss: 1.978  loss_sem_seg: 0.8832  loss_center: 0.4403  loss_offset: 0.6088  time: 0.3497  data_time: 0.0255  lr: 0.00077481  max_mem: 7418M
[12/11 23:46:15 d2.utils.events]:  eta: 0:15:39  iter: 7299  total_loss: 2.275  loss_sem_seg: 0.8366  loss_center: 0.6729  loss_offset: 0.574  time: 0.3497  data_time: 0.0240  lr: 0.00076968  max_mem: 7418M
[12/11 23:46:22 d2.utils.events]:  eta: 0:15:32  iter: 7319  total_loss: 2.359  loss_sem_seg: 1.192  loss_center: 0.5469  loss_offset: 0.4988  time: 0.3497  data_time: 0.0276  lr: 0.00076455  max_mem: 7418M
[12/11 23:46:29 d2.utils.events]:  eta: 0:15:25  iter: 7339  total_loss: 2.174  loss_sem_seg: 0.8573  loss_center: 0.6086  loss_offset: 0.5839  time: 0.3497  data_time: 0.0238  lr: 0.00075942  max_mem: 7418M
[12/11 23:46:36 d2.utils.events]:  eta: 0:15:18  iter: 7359  total_loss: 2.178  loss_sem_seg: 0.9189  loss_center: 0.5991  loss_offset: 0.5825  time: 0.3497  data_time: 0.0257  lr: 0.00075428  max_mem: 7418M
[12/11 23:46:43 d2.utils.events]:  eta: 0:15:11  iter: 7379  total_loss: 2.075  loss_sem_seg: 0.9462  loss_center: 0.5012  loss_offset: 0.5483  time: 0.3497  data_time: 0.0243  lr: 0.00074914  max_mem: 7418M
[12/11 23:46:50 d2.utils.events]:  eta: 0:15:04  iter: 7399  total_loss: 1.851  loss_sem_seg: 0.9048  loss_center: 0.5115  loss_offset: 0.4375  time: 0.3497  data_time: 0.0252  lr: 0.00074399  max_mem: 7418M
[12/11 23:46:57 d2.utils.events]:  eta: 0:14:57  iter: 7419  total_loss: 2.361  loss_sem_seg: 0.9784  loss_center: 0.5378  loss_offset: 0.597  time: 0.3497  data_time: 0.0255  lr: 0.00073884  max_mem: 7418M
[12/11 23:47:04 d2.utils.events]:  eta: 0:14:50  iter: 7439  total_loss: 1.903  loss_sem_seg: 0.8541  loss_center: 0.4725  loss_offset: 0.4847  time: 0.3496  data_time: 0.0251  lr: 0.00073368  max_mem: 7418M
[12/11 23:47:11 d2.utils.events]:  eta: 0:14:42  iter: 7459  total_loss: 2.172  loss_sem_seg: 0.9385  loss_center: 0.5191  loss_offset: 0.636  time: 0.3496  data_time: 0.0245  lr: 0.00072852  max_mem: 7418M
[12/11 23:47:18 d2.utils.events]:  eta: 0:14:35  iter: 7479  total_loss: 1.879  loss_sem_seg: 0.891  loss_center: 0.4883  loss_offset: 0.5413  time: 0.3496  data_time: 0.0254  lr: 0.00072336  max_mem: 7418M
[12/11 23:47:25 d2.utils.events]:  eta: 0:14:28  iter: 7499  total_loss: 2.313  loss_sem_seg: 1.026  loss_center: 0.5399  loss_offset: 0.5327  time: 0.3496  data_time: 0.0253  lr: 0.00071819  max_mem: 7418M
[12/11 23:47:32 d2.utils.events]:  eta: 0:14:21  iter: 7519  total_loss: 2.4  loss_sem_seg: 1.007  loss_center: 0.4628  loss_offset: 0.7804  time: 0.3496  data_time: 0.0258  lr: 0.00071302  max_mem: 7418M
[12/11 23:47:39 d2.utils.events]:  eta: 0:14:14  iter: 7539  total_loss: 2.097  loss_sem_seg: 0.9828  loss_center: 0.4702  loss_offset: 0.5406  time: 0.3496  data_time: 0.0264  lr: 0.00070785  max_mem: 7418M
[12/11 23:47:46 d2.utils.events]:  eta: 0:14:07  iter: 7559  total_loss: 2.234  loss_sem_seg: 1.096  loss_center: 0.4679  loss_offset: 0.5098  time: 0.3496  data_time: 0.0249  lr: 0.00070267  max_mem: 7418M
[12/11 23:47:53 d2.utils.events]:  eta: 0:14:00  iter: 7579  total_loss: 2.169  loss_sem_seg: 0.894  loss_center: 0.6133  loss_offset: 0.5022  time: 0.3496  data_time: 0.0291  lr: 0.00069749  max_mem: 7418M
[12/11 23:48:00 d2.utils.events]:  eta: 0:13:54  iter: 7599  total_loss: 2.066  loss_sem_seg: 0.9006  loss_center: 0.5152  loss_offset: 0.5908  time: 0.3496  data_time: 0.0241  lr: 0.0006923  max_mem: 7418M
[12/11 23:48:07 d2.utils.events]:  eta: 0:13:47  iter: 7619  total_loss: 2.277  loss_sem_seg: 1.126  loss_center: 0.4584  loss_offset: 0.5254  time: 0.3496  data_time: 0.0253  lr: 0.00068711  max_mem: 7418M
[12/11 23:48:14 d2.utils.events]:  eta: 0:13:40  iter: 7639  total_loss: 2.263  loss_sem_seg: 0.9189  loss_center: 0.5306  loss_offset: 0.5497  time: 0.3496  data_time: 0.0267  lr: 0.00068191  max_mem: 7418M
[12/11 23:48:21 d2.utils.events]:  eta: 0:13:33  iter: 7659  total_loss: 2.552  loss_sem_seg: 1.042  loss_center: 0.5454  loss_offset: 0.5198  time: 0.3496  data_time: 0.0266  lr: 0.00067671  max_mem: 7418M
[12/11 23:48:27 d2.utils.events]:  eta: 0:13:26  iter: 7679  total_loss: 1.739  loss_sem_seg: 0.7573  loss_center: 0.5067  loss_offset: 0.4402  time: 0.3496  data_time: 0.0241  lr: 0.0006715  max_mem: 7418M
[12/11 23:48:34 d2.utils.events]:  eta: 0:13:19  iter: 7699  total_loss: 2.261  loss_sem_seg: 1.129  loss_center: 0.4846  loss_offset: 0.6844  time: 0.3496  data_time: 0.0258  lr: 0.00066629  max_mem: 7418M
[12/11 23:48:41 d2.utils.events]:  eta: 0:13:12  iter: 7719  total_loss: 1.925  loss_sem_seg: 0.8828  loss_center: 0.5302  loss_offset: 0.5667  time: 0.3496  data_time: 0.0264  lr: 0.00066108  max_mem: 7418M
[12/11 23:48:48 d2.utils.events]:  eta: 0:13:05  iter: 7739  total_loss: 2.03  loss_sem_seg: 0.8439  loss_center: 0.4041  loss_offset: 0.5253  time: 0.3496  data_time: 0.0250  lr: 0.00065586  max_mem: 7418M
[12/11 23:48:55 d2.utils.events]:  eta: 0:12:58  iter: 7759  total_loss: 2.142  loss_sem_seg: 0.848  loss_center: 0.4988  loss_offset: 0.5828  time: 0.3496  data_time: 0.0259  lr: 0.00065064  max_mem: 7418M
[12/11 23:49:02 d2.utils.events]:  eta: 0:12:51  iter: 7779  total_loss: 2.319  loss_sem_seg: 1.288  loss_center: 0.4759  loss_offset: 0.601  time: 0.3496  data_time: 0.0243  lr: 0.00064541  max_mem: 7418M
[12/11 23:49:09 d2.utils.events]:  eta: 0:12:44  iter: 7799  total_loss: 1.891  loss_sem_seg: 0.8443  loss_center: 0.5316  loss_offset: 0.4901  time: 0.3496  data_time: 0.0255  lr: 0.00064017  max_mem: 7418M
[12/11 23:49:16 d2.utils.events]:  eta: 0:12:37  iter: 7819  total_loss: 2.052  loss_sem_seg: 0.9441  loss_center: 0.6895  loss_offset: 0.4635  time: 0.3496  data_time: 0.0255  lr: 0.00063494  max_mem: 7418M
[12/11 23:49:23 d2.utils.events]:  eta: 0:12:30  iter: 7839  total_loss: 1.958  loss_sem_seg: 0.9834  loss_center: 0.5322  loss_offset: 0.4877  time: 0.3496  data_time: 0.0250  lr: 0.00062969  max_mem: 7418M
[12/11 23:49:30 d2.utils.events]:  eta: 0:12:23  iter: 7859  total_loss: 1.993  loss_sem_seg: 0.952  loss_center: 0.5351  loss_offset: 0.4676  time: 0.3496  data_time: 0.0250  lr: 0.00062445  max_mem: 7418M
[12/11 23:49:37 d2.utils.events]:  eta: 0:12:16  iter: 7879  total_loss: 2.057  loss_sem_seg: 0.9419  loss_center: 0.6083  loss_offset: 0.6168  time: 0.3496  data_time: 0.0251  lr: 0.00061919  max_mem: 7418M
[12/11 23:49:44 d2.utils.events]:  eta: 0:12:09  iter: 7899  total_loss: 2.215  loss_sem_seg: 1.052  loss_center: 0.4609  loss_offset: 0.6653  time: 0.3496  data_time: 0.0251  lr: 0.00061394  max_mem: 7418M
[12/11 23:49:51 d2.utils.events]:  eta: 0:12:02  iter: 7919  total_loss: 2.088  loss_sem_seg: 0.8829  loss_center: 0.4159  loss_offset: 0.5417  time: 0.3496  data_time: 0.0272  lr: 0.00060867  max_mem: 7418M
[12/11 23:49:58 d2.utils.events]:  eta: 0:11:55  iter: 7939  total_loss: 2.096  loss_sem_seg: 0.882  loss_center: 0.524  loss_offset: 0.5774  time: 0.3496  data_time: 0.0252  lr: 0.00060341  max_mem: 7418M
[12/11 23:50:05 d2.utils.events]:  eta: 0:11:48  iter: 7959  total_loss: 2.189  loss_sem_seg: 0.9714  loss_center: 0.4596  loss_offset: 0.5813  time: 0.3496  data_time: 0.0258  lr: 0.00059813  max_mem: 7418M
[12/11 23:50:12 d2.utils.events]:  eta: 0:11:41  iter: 7979  total_loss: 2.201  loss_sem_seg: 0.9887  loss_center: 0.5884  loss_offset: 0.5744  time: 0.3496  data_time: 0.0256  lr: 0.00059286  max_mem: 7418M
[12/11 23:50:19 d2.utils.events]:  eta: 0:11:35  iter: 7999  total_loss: 1.935  loss_sem_seg: 0.9701  loss_center: 0.4626  loss_offset: 0.56  time: 0.3496  data_time: 0.0248  lr: 0.00058757  max_mem: 7418M
[12/11 23:50:26 d2.utils.events]:  eta: 0:11:28  iter: 8019  total_loss: 2.072  loss_sem_seg: 0.8584  loss_center: 0.59  loss_offset: 0.5587  time: 0.3496  data_time: 0.0253  lr: 0.00058229  max_mem: 7418M
[12/11 23:50:33 d2.utils.events]:  eta: 0:11:21  iter: 8039  total_loss: 2.02  loss_sem_seg: 0.9331  loss_center: 0.6913  loss_offset: 0.5137  time: 0.3496  data_time: 0.0272  lr: 0.00057699  max_mem: 7418M
[12/11 23:50:40 d2.utils.events]:  eta: 0:11:14  iter: 8059  total_loss: 2.006  loss_sem_seg: 0.9197  loss_center: 0.5803  loss_offset: 0.4675  time: 0.3496  data_time: 0.0249  lr: 0.00057169  max_mem: 7418M
[12/11 23:50:47 d2.utils.events]:  eta: 0:11:08  iter: 8079  total_loss: 2.013  loss_sem_seg: 0.8579  loss_center: 0.5142  loss_offset: 0.5583  time: 0.3496  data_time: 0.0275  lr: 0.00056639  max_mem: 7418M
[12/11 23:50:54 d2.utils.events]:  eta: 0:11:01  iter: 8099  total_loss: 2.206  loss_sem_seg: 1.003  loss_center: 0.4895  loss_offset: 0.6319  time: 0.3496  data_time: 0.0259  lr: 0.00056108  max_mem: 7418M
[12/11 23:51:01 d2.utils.events]:  eta: 0:10:54  iter: 8119  total_loss: 2.105  loss_sem_seg: 0.8658  loss_center: 0.5663  loss_offset: 0.5569  time: 0.3496  data_time: 0.0237  lr: 0.00055576  max_mem: 7418M
[12/11 23:51:08 d2.utils.events]:  eta: 0:10:47  iter: 8139  total_loss: 2.139  loss_sem_seg: 1.056  loss_center: 0.4699  loss_offset: 0.5559  time: 0.3496  data_time: 0.0252  lr: 0.00055044  max_mem: 7418M
[12/11 23:51:15 d2.utils.events]:  eta: 0:10:41  iter: 8159  total_loss: 2.275  loss_sem_seg: 0.9361  loss_center: 0.6965  loss_offset: 0.6435  time: 0.3496  data_time: 0.0258  lr: 0.00054512  max_mem: 7418M
[12/11 23:51:22 d2.utils.events]:  eta: 0:10:34  iter: 8179  total_loss: 2.015  loss_sem_seg: 1.037  loss_center: 0.395  loss_offset: 0.5394  time: 0.3496  data_time: 0.0280  lr: 0.00053978  max_mem: 7418M
[12/11 23:51:29 d2.utils.events]:  eta: 0:10:27  iter: 8199  total_loss: 2.318  loss_sem_seg: 0.9398  loss_center: 0.5298  loss_offset: 0.5518  time: 0.3496  data_time: 0.0256  lr: 0.00053444  max_mem: 7418M
[12/11 23:51:36 d2.utils.events]:  eta: 0:10:20  iter: 8219  total_loss: 1.912  loss_sem_seg: 1.01  loss_center: 0.4778  loss_offset: 0.4354  time: 0.3496  data_time: 0.0254  lr: 0.0005291  max_mem: 7418M
[12/11 23:51:43 d2.utils.events]:  eta: 0:10:13  iter: 8239  total_loss: 2.419  loss_sem_seg: 1.039  loss_center: 0.5483  loss_offset: 0.65  time: 0.3496  data_time: 0.0251  lr: 0.00052375  max_mem: 7418M
[12/11 23:51:50 d2.utils.events]:  eta: 0:10:06  iter: 8259  total_loss: 2.137  loss_sem_seg: 0.9721  loss_center: 0.446  loss_offset: 0.5768  time: 0.3496  data_time: 0.0256  lr: 0.00051839  max_mem: 7418M
[12/11 23:51:57 d2.utils.events]:  eta: 0:09:59  iter: 8279  total_loss: 2.139  loss_sem_seg: 1.053  loss_center: 0.4918  loss_offset: 0.5805  time: 0.3496  data_time: 0.0253  lr: 0.00051303  max_mem: 7418M
[12/11 23:52:04 d2.utils.events]:  eta: 0:09:52  iter: 8299  total_loss: 2.09  loss_sem_seg: 0.9533  loss_center: 0.447  loss_offset: 0.545  time: 0.3496  data_time: 0.0248  lr: 0.00050766  max_mem: 7418M
[12/11 23:52:11 d2.utils.events]:  eta: 0:09:45  iter: 8319  total_loss: 2.047  loss_sem_seg: 0.9077  loss_center: 0.5613  loss_offset: 0.5784  time: 0.3496  data_time: 0.0271  lr: 0.00050229  max_mem: 7418M
[12/11 23:52:18 d2.utils.events]:  eta: 0:09:38  iter: 8339  total_loss: 1.799  loss_sem_seg: 0.905  loss_center: 0.4509  loss_offset: 0.4586  time: 0.3496  data_time: 0.0278  lr: 0.0004969  max_mem: 7418M
[12/11 23:52:25 d2.utils.events]:  eta: 0:09:31  iter: 8359  total_loss: 1.997  loss_sem_seg: 0.9435  loss_center: 0.4876  loss_offset: 0.5411  time: 0.3496  data_time: 0.0248  lr: 0.00049152  max_mem: 7418M
[12/11 23:52:32 d2.utils.events]:  eta: 0:09:24  iter: 8379  total_loss: 2.07  loss_sem_seg: 1.024  loss_center: 0.5164  loss_offset: 0.5635  time: 0.3496  data_time: 0.0259  lr: 0.00048612  max_mem: 7418M
[12/11 23:52:39 d2.utils.events]:  eta: 0:09:18  iter: 8399  total_loss: 1.959  loss_sem_seg: 0.9274  loss_center: 0.4396  loss_offset: 0.4893  time: 0.3496  data_time: 0.0272  lr: 0.00048072  max_mem: 7418M
[12/11 23:52:46 d2.utils.events]:  eta: 0:09:11  iter: 8419  total_loss: 1.864  loss_sem_seg: 0.9045  loss_center: 0.476  loss_offset: 0.4835  time: 0.3496  data_time: 0.0239  lr: 0.00047531  max_mem: 7418M
[12/11 23:52:53 d2.utils.events]:  eta: 0:09:03  iter: 8439  total_loss: 2.31  loss_sem_seg: 0.9865  loss_center: 0.5618  loss_offset: 0.4631  time: 0.3496  data_time: 0.0262  lr: 0.0004699  max_mem: 7418M
[12/11 23:53:00 d2.utils.events]:  eta: 0:08:57  iter: 8459  total_loss: 1.79  loss_sem_seg: 0.7712  loss_center: 0.4633  loss_offset: 0.387  time: 0.3496  data_time: 0.0262  lr: 0.00046448  max_mem: 7418M
[12/11 23:53:07 d2.utils.events]:  eta: 0:08:50  iter: 8479  total_loss: 2.023  loss_sem_seg: 0.9766  loss_center: 0.5187  loss_offset: 0.4787  time: 0.3496  data_time: 0.0264  lr: 0.00045905  max_mem: 7418M
[12/11 23:53:14 d2.utils.events]:  eta: 0:08:43  iter: 8499  total_loss: 2.164  loss_sem_seg: 0.955  loss_center: 0.5406  loss_offset: 0.5787  time: 0.3496  data_time: 0.0256  lr: 0.00045361  max_mem: 7418M
[12/11 23:53:21 d2.utils.events]:  eta: 0:08:36  iter: 8519  total_loss: 1.923  loss_sem_seg: 0.9805  loss_center: 0.4752  loss_offset: 0.3853  time: 0.3496  data_time: 0.0262  lr: 0.00044817  max_mem: 7418M
[12/11 23:53:28 d2.utils.events]:  eta: 0:08:29  iter: 8539  total_loss: 2.073  loss_sem_seg: 0.9865  loss_center: 0.5592  loss_offset: 0.4934  time: 0.3496  data_time: 0.0253  lr: 0.00044272  max_mem: 7418M
[12/11 23:53:35 d2.utils.events]:  eta: 0:08:22  iter: 8559  total_loss: 2.356  loss_sem_seg: 0.9753  loss_center: 0.6067  loss_offset: 0.5535  time: 0.3496  data_time: 0.0246  lr: 0.00043726  max_mem: 7418M
[12/11 23:53:42 d2.utils.events]:  eta: 0:08:15  iter: 8579  total_loss: 2.061  loss_sem_seg: 0.8306  loss_center: 0.4353  loss_offset: 0.4517  time: 0.3496  data_time: 0.0274  lr: 0.00043179  max_mem: 7418M
[12/11 23:53:49 d2.utils.events]:  eta: 0:08:08  iter: 8599  total_loss: 1.997  loss_sem_seg: 0.8838  loss_center: 0.5352  loss_offset: 0.4878  time: 0.3496  data_time: 0.0270  lr: 0.00042632  max_mem: 7418M
[12/11 23:53:56 d2.utils.events]:  eta: 0:08:01  iter: 8619  total_loss: 1.82  loss_sem_seg: 0.8182  loss_center: 0.4489  loss_offset: 0.5419  time: 0.3496  data_time: 0.0256  lr: 0.00042084  max_mem: 7418M
[12/11 23:54:03 d2.utils.events]:  eta: 0:07:54  iter: 8639  total_loss: 2.244  loss_sem_seg: 0.8401  loss_center: 0.4749  loss_offset: 0.6079  time: 0.3496  data_time: 0.0261  lr: 0.00041535  max_mem: 7418M
[12/11 23:54:10 d2.utils.events]:  eta: 0:07:47  iter: 8659  total_loss: 1.985  loss_sem_seg: 0.8601  loss_center: 0.4997  loss_offset: 0.4172  time: 0.3496  data_time: 0.0258  lr: 0.00040985  max_mem: 7418M
[12/11 23:54:17 d2.utils.events]:  eta: 0:07:40  iter: 8679  total_loss: 1.971  loss_sem_seg: 0.9175  loss_center: 0.5265  loss_offset: 0.4795  time: 0.3496  data_time: 0.0257  lr: 0.00040435  max_mem: 7418M
[12/11 23:54:24 d2.utils.events]:  eta: 0:07:33  iter: 8699  total_loss: 1.93  loss_sem_seg: 0.9302  loss_center: 0.4172  loss_offset: 0.5393  time: 0.3496  data_time: 0.0243  lr: 0.00039883  max_mem: 7418M
[12/11 23:54:31 d2.utils.events]:  eta: 0:07:26  iter: 8719  total_loss: 1.942  loss_sem_seg: 0.863  loss_center: 0.4585  loss_offset: 0.524  time: 0.3496  data_time: 0.0262  lr: 0.00039331  max_mem: 7418M
[12/11 23:54:38 d2.utils.events]:  eta: 0:07:19  iter: 8739  total_loss: 2.096  loss_sem_seg: 0.9315  loss_center: 0.5921  loss_offset: 0.5127  time: 0.3496  data_time: 0.0247  lr: 0.00038778  max_mem: 7418M
[12/11 23:54:45 d2.utils.events]:  eta: 0:07:12  iter: 8759  total_loss: 2.091  loss_sem_seg: 0.9153  loss_center: 0.5753  loss_offset: 0.5084  time: 0.3496  data_time: 0.0251  lr: 0.00038224  max_mem: 7418M
[12/11 23:54:52 d2.utils.events]:  eta: 0:07:05  iter: 8779  total_loss: 1.996  loss_sem_seg: 0.7861  loss_center: 0.5237  loss_offset: 0.5346  time: 0.3496  data_time: 0.0247  lr: 0.00037669  max_mem: 7418M
[12/11 23:54:59 d2.utils.events]:  eta: 0:06:58  iter: 8799  total_loss: 1.91  loss_sem_seg: 0.8033  loss_center: 0.5013  loss_offset: 0.4468  time: 0.3496  data_time: 0.0255  lr: 0.00037113  max_mem: 7418M
[12/11 23:55:06 d2.utils.events]:  eta: 0:06:51  iter: 8819  total_loss: 1.995  loss_sem_seg: 0.9344  loss_center: 0.46  loss_offset: 0.6156  time: 0.3496  data_time: 0.0258  lr: 0.00036557  max_mem: 7418M
[12/11 23:55:13 d2.utils.events]:  eta: 0:06:44  iter: 8839  total_loss: 2.063  loss_sem_seg: 0.8711  loss_center: 0.5602  loss_offset: 0.4938  time: 0.3496  data_time: 0.0259  lr: 0.00035999  max_mem: 7418M
[12/11 23:55:20 d2.utils.events]:  eta: 0:06:37  iter: 8859  total_loss: 2.197  loss_sem_seg: 0.81  loss_center: 0.443  loss_offset: 0.626  time: 0.3496  data_time: 0.0253  lr: 0.0003544  max_mem: 7418M
[12/11 23:55:27 d2.utils.events]:  eta: 0:06:30  iter: 8879  total_loss: 2.035  loss_sem_seg: 0.8118  loss_center: 0.5022  loss_offset: 0.5435  time: 0.3496  data_time: 0.0255  lr: 0.00034881  max_mem: 7418M
[12/11 23:55:34 d2.utils.events]:  eta: 0:06:23  iter: 8899  total_loss: 1.945  loss_sem_seg: 0.8183  loss_center: 0.4615  loss_offset: 0.5846  time: 0.3496  data_time: 0.0251  lr: 0.0003432  max_mem: 7418M
[12/11 23:55:41 d2.utils.events]:  eta: 0:06:16  iter: 8919  total_loss: 2.047  loss_sem_seg: 0.85  loss_center: 0.4592  loss_offset: 0.59  time: 0.3496  data_time: 0.0258  lr: 0.00033758  max_mem: 7418M
[12/11 23:55:48 d2.utils.events]:  eta: 0:06:10  iter: 8939  total_loss: 2.127  loss_sem_seg: 0.936  loss_center: 0.5168  loss_offset: 0.5262  time: 0.3496  data_time: 0.0250  lr: 0.00033196  max_mem: 7418M
[12/11 23:55:55 d2.utils.events]:  eta: 0:06:03  iter: 8959  total_loss: 1.966  loss_sem_seg: 0.8032  loss_center: 0.5329  loss_offset: 0.499  time: 0.3496  data_time: 0.0257  lr: 0.00032632  max_mem: 7418M
[12/11 23:56:02 d2.utils.events]:  eta: 0:05:55  iter: 8979  total_loss: 1.835  loss_sem_seg: 0.831  loss_center: 0.5005  loss_offset: 0.5385  time: 0.3496  data_time: 0.0260  lr: 0.00032067  max_mem: 7418M
[12/11 23:56:09 d2.utils.events]:  eta: 0:05:48  iter: 8999  total_loss: 2.171  loss_sem_seg: 1.049  loss_center: 0.5221  loss_offset: 0.5223  time: 0.3496  data_time: 0.0249  lr: 0.00031501  max_mem: 7418M
[12/11 23:56:16 d2.utils.events]:  eta: 0:05:41  iter: 9019  total_loss: 2.113  loss_sem_seg: 0.9583  loss_center: 0.5042  loss_offset: 0.5996  time: 0.3496  data_time: 0.0256  lr: 0.00030934  max_mem: 7418M
[12/11 23:56:23 d2.utils.events]:  eta: 0:05:34  iter: 9039  total_loss: 1.903  loss_sem_seg: 0.9355  loss_center: 0.4769  loss_offset: 0.5182  time: 0.3496  data_time: 0.0252  lr: 0.00030366  max_mem: 7418M
[12/11 23:56:30 d2.utils.events]:  eta: 0:05:27  iter: 9059  total_loss: 1.87  loss_sem_seg: 0.881  loss_center: 0.4342  loss_offset: 0.4273  time: 0.3496  data_time: 0.0255  lr: 0.00029797  max_mem: 7418M
[12/11 23:56:37 d2.utils.events]:  eta: 0:05:20  iter: 9079  total_loss: 2.084  loss_sem_seg: 0.8278  loss_center: 0.4661  loss_offset: 0.4781  time: 0.3496  data_time: 0.0233  lr: 0.00029226  max_mem: 7418M
[12/11 23:56:44 d2.utils.events]:  eta: 0:05:13  iter: 9099  total_loss: 2.092  loss_sem_seg: 0.9446  loss_center: 0.4552  loss_offset: 0.4935  time: 0.3496  data_time: 0.0263  lr: 0.00028654  max_mem: 7418M
[12/11 23:56:51 d2.utils.events]:  eta: 0:05:06  iter: 9119  total_loss: 2.04  loss_sem_seg: 1.021  loss_center: 0.456  loss_offset: 0.4952  time: 0.3496  data_time: 0.0253  lr: 0.00028081  max_mem: 7418M
[12/11 23:56:58 d2.utils.events]:  eta: 0:04:59  iter: 9139  total_loss: 2.383  loss_sem_seg: 1.075  loss_center: 0.5587  loss_offset: 0.5449  time: 0.3496  data_time: 0.0273  lr: 0.00027507  max_mem: 7418M
[12/11 23:57:05 d2.utils.events]:  eta: 0:04:52  iter: 9159  total_loss: 1.76  loss_sem_seg: 0.7715  loss_center: 0.4968  loss_offset: 0.4791  time: 0.3496  data_time: 0.0253  lr: 0.00026931  max_mem: 7418M
[12/11 23:57:12 d2.utils.events]:  eta: 0:04:45  iter: 9179  total_loss: 1.93  loss_sem_seg: 0.8307  loss_center: 0.5087  loss_offset: 0.5625  time: 0.3496  data_time: 0.0266  lr: 0.00026354  max_mem: 7418M
[12/11 23:57:19 d2.utils.events]:  eta: 0:04:38  iter: 9199  total_loss: 2.014  loss_sem_seg: 0.9817  loss_center: 0.499  loss_offset: 0.5946  time: 0.3496  data_time: 0.0256  lr: 0.00025776  max_mem: 7418M
[12/11 23:57:26 d2.utils.events]:  eta: 0:04:31  iter: 9219  total_loss: 2.001  loss_sem_seg: 0.8721  loss_center: 0.5513  loss_offset: 0.4765  time: 0.3496  data_time: 0.0252  lr: 0.00025196  max_mem: 7418M
[12/11 23:57:33 d2.utils.events]:  eta: 0:04:25  iter: 9239  total_loss: 2.005  loss_sem_seg: 0.9311  loss_center: 0.5977  loss_offset: 0.3767  time: 0.3496  data_time: 0.0266  lr: 0.00024614  max_mem: 7418M
[12/11 23:57:40 d2.utils.events]:  eta: 0:04:17  iter: 9259  total_loss: 1.992  loss_sem_seg: 0.8541  loss_center: 0.5766  loss_offset: 0.5246  time: 0.3496  data_time: 0.0241  lr: 0.00024031  max_mem: 7418M
[12/11 23:57:47 d2.utils.events]:  eta: 0:04:10  iter: 9279  total_loss: 1.998  loss_sem_seg: 0.8173  loss_center: 0.3927  loss_offset: 0.5413  time: 0.3495  data_time: 0.0244  lr: 0.00023447  max_mem: 7418M
[12/11 23:57:54 d2.utils.events]:  eta: 0:04:03  iter: 9299  total_loss: 1.755  loss_sem_seg: 0.818  loss_center: 0.3944  loss_offset: 0.5632  time: 0.3496  data_time: 0.0266  lr: 0.00022861  max_mem: 7418M
[12/11 23:58:01 d2.utils.events]:  eta: 0:03:56  iter: 9319  total_loss: 2.016  loss_sem_seg: 0.9666  loss_center: 0.6352  loss_offset: 0.4775  time: 0.3495  data_time: 0.0261  lr: 0.00022273  max_mem: 7418M
[12/11 23:58:08 d2.utils.events]:  eta: 0:03:49  iter: 9339  total_loss: 2.003  loss_sem_seg: 0.8577  loss_center: 0.4661  loss_offset: 0.5424  time: 0.3496  data_time: 0.0266  lr: 0.00021683  max_mem: 7418M
[12/11 23:58:15 d2.utils.events]:  eta: 0:03:43  iter: 9359  total_loss: 1.821  loss_sem_seg: 0.7346  loss_center: 0.5326  loss_offset: 0.4715  time: 0.3496  data_time: 0.0270  lr: 0.00021092  max_mem: 7418M
[12/11 23:58:22 d2.utils.events]:  eta: 0:03:36  iter: 9379  total_loss: 2.005  loss_sem_seg: 0.8998  loss_center: 0.5559  loss_offset: 0.4812  time: 0.3496  data_time: 0.0246  lr: 0.00020499  max_mem: 7418M
[12/11 23:58:29 d2.utils.events]:  eta: 0:03:28  iter: 9399  total_loss: 1.862  loss_sem_seg: 0.9135  loss_center: 0.4886  loss_offset: 0.4465  time: 0.3496  data_time: 0.0251  lr: 0.00019903  max_mem: 7418M
[12/11 23:58:36 d2.utils.events]:  eta: 0:03:22  iter: 9419  total_loss: 1.937  loss_sem_seg: 0.8821  loss_center: 0.4659  loss_offset: 0.4805  time: 0.3495  data_time: 0.0265  lr: 0.00019306  max_mem: 7418M
[12/11 23:58:43 d2.utils.events]:  eta: 0:03:15  iter: 9439  total_loss: 1.785  loss_sem_seg: 0.7951  loss_center: 0.4867  loss_offset: 0.5197  time: 0.3495  data_time: 0.0253  lr: 0.00018707  max_mem: 7418M
[12/11 23:58:50 d2.utils.events]:  eta: 0:03:08  iter: 9459  total_loss: 2.153  loss_sem_seg: 0.9413  loss_center: 0.5057  loss_offset: 0.5617  time: 0.3496  data_time: 0.0259  lr: 0.00018106  max_mem: 7418M
[12/11 23:58:57 d2.utils.events]:  eta: 0:03:01  iter: 9479  total_loss: 1.892  loss_sem_seg: 0.8892  loss_center: 0.537  loss_offset: 0.4783  time: 0.3496  data_time: 0.0251  lr: 0.00017502  max_mem: 7418M
[12/11 23:59:04 d2.utils.events]:  eta: 0:02:54  iter: 9499  total_loss: 2.132  loss_sem_seg: 0.9187  loss_center: 0.6567  loss_offset: 0.5166  time: 0.3496  data_time: 0.0265  lr: 0.00016896  max_mem: 7418M
[12/11 23:59:11 d2.utils.events]:  eta: 0:02:47  iter: 9519  total_loss: 1.969  loss_sem_seg: 0.9536  loss_center: 0.4466  loss_offset: 0.484  time: 0.3496  data_time: 0.0276  lr: 0.00016288  max_mem: 7418M
[12/11 23:59:18 d2.utils.events]:  eta: 0:02:40  iter: 9539  total_loss: 1.98  loss_sem_seg: 1.021  loss_center: 0.5316  loss_offset: 0.4776  time: 0.3496  data_time: 0.0257  lr: 0.00015677  max_mem: 7418M
[12/11 23:59:25 d2.utils.events]:  eta: 0:02:33  iter: 9559  total_loss: 1.941  loss_sem_seg: 0.9515  loss_center: 0.4661  loss_offset: 0.4547  time: 0.3496  data_time: 0.0259  lr: 0.00015064  max_mem: 7418M
[12/11 23:59:32 d2.utils.events]:  eta: 0:02:26  iter: 9579  total_loss: 1.707  loss_sem_seg: 0.7478  loss_center: 0.4497  loss_offset: 0.4432  time: 0.3496  data_time: 0.0276  lr: 0.00014448  max_mem: 7418M
[12/11 23:59:39 d2.utils.events]:  eta: 0:02:19  iter: 9599  total_loss: 2.165  loss_sem_seg: 1.003  loss_center: 0.4939  loss_offset: 0.5827  time: 0.3496  data_time: 0.0262  lr: 0.00013828  max_mem: 7418M
[12/11 23:59:46 d2.utils.events]:  eta: 0:02:12  iter: 9619  total_loss: 2.1  loss_sem_seg: 1.071  loss_center: 0.4674  loss_offset: 0.5486  time: 0.3496  data_time: 0.0266  lr: 0.00013206  max_mem: 7418M
[12/11 23:59:53 d2.utils.events]:  eta: 0:02:05  iter: 9639  total_loss: 2.285  loss_sem_seg: 1.017  loss_center: 0.5347  loss_offset: 0.5597  time: 0.3496  data_time: 0.0256  lr: 0.0001258  max_mem: 7418M
[12/12 00:00:00 d2.utils.events]:  eta: 0:01:58  iter: 9659  total_loss: 1.765  loss_sem_seg: 0.8281  loss_center: 0.4457  loss_offset: 0.4873  time: 0.3496  data_time: 0.0254  lr: 0.00011951  max_mem: 7418M
[12/12 00:00:07 d2.utils.events]:  eta: 0:01:51  iter: 9679  total_loss: 1.928  loss_sem_seg: 0.8725  loss_center: 0.5355  loss_offset: 0.5105  time: 0.3496  data_time: 0.0275  lr: 0.00011319  max_mem: 7418M
[12/12 00:00:14 d2.utils.events]:  eta: 0:01:44  iter: 9699  total_loss: 2.09  loss_sem_seg: 0.7968  loss_center: 0.5194  loss_offset: 0.5186  time: 0.3496  data_time: 0.0275  lr: 0.00010682  max_mem: 7418M
[12/12 00:00:21 d2.utils.events]:  eta: 0:01:37  iter: 9719  total_loss: 2.264  loss_sem_seg: 0.8769  loss_center: 0.4649  loss_offset: 0.5424  time: 0.3496  data_time: 0.0253  lr: 0.00010041  max_mem: 7418M
[12/12 00:00:28 d2.utils.events]:  eta: 0:01:30  iter: 9739  total_loss: 1.806  loss_sem_seg: 0.8548  loss_center: 0.3974  loss_offset: 0.5422  time: 0.3496  data_time: 0.0264  lr: 9.3954e-05  max_mem: 7418M
[12/12 00:00:35 d2.utils.events]:  eta: 0:01:23  iter: 9759  total_loss: 1.847  loss_sem_seg: 0.8212  loss_center: 0.4564  loss_offset: 0.5018  time: 0.3496  data_time: 0.0259  lr: 8.7449e-05  max_mem: 7418M
[12/12 00:00:42 d2.utils.events]:  eta: 0:01:16  iter: 9779  total_loss: 2.028  loss_sem_seg: 0.8104  loss_center: 0.5023  loss_offset: 0.5216  time: 0.3496  data_time: 0.0265  lr: 8.089e-05  max_mem: 7418M
[12/12 00:00:49 d2.utils.events]:  eta: 0:01:09  iter: 9799  total_loss: 2.274  loss_sem_seg: 1.019  loss_center: 0.627  loss_offset: 0.5884  time: 0.3496  data_time: 0.0257  lr: 7.4271e-05  max_mem: 7418M
[12/12 00:00:56 d2.utils.events]:  eta: 0:01:02  iter: 9819  total_loss: 2.128  loss_sem_seg: 0.924  loss_center: 0.5981  loss_offset: 0.5835  time: 0.3496  data_time: 0.0259  lr: 6.7585e-05  max_mem: 7418M
[12/12 00:01:03 d2.utils.events]:  eta: 0:00:55  iter: 9839  total_loss: 1.738  loss_sem_seg: 0.7398  loss_center: 0.4697  loss_offset: 0.4971  time: 0.3496  data_time: 0.0255  lr: 6.0825e-05  max_mem: 7418M
[12/12 00:01:10 d2.utils.events]:  eta: 0:00:48  iter: 9859  total_loss: 1.852  loss_sem_seg: 0.7718  loss_center: 0.4259  loss_offset: 0.5277  time: 0.3496  data_time: 0.0280  lr: 5.3981e-05  max_mem: 7418M
[12/12 00:01:17 d2.utils.events]:  eta: 0:00:41  iter: 9879  total_loss: 1.805  loss_sem_seg: 0.7877  loss_center: 0.5249  loss_offset: 0.4816  time: 0.3496  data_time: 0.0277  lr: 4.7038e-05  max_mem: 7418M
[12/12 00:01:24 d2.utils.events]:  eta: 0:00:34  iter: 9899  total_loss: 1.962  loss_sem_seg: 0.8822  loss_center: 0.5169  loss_offset: 0.4886  time: 0.3496  data_time: 0.0247  lr: 3.9979e-05  max_mem: 7418M
[12/12 00:01:31 d2.utils.events]:  eta: 0:00:27  iter: 9919  total_loss: 1.935  loss_sem_seg: 0.8509  loss_center: 0.5071  loss_offset: 0.5591  time: 0.3496  data_time: 0.0272  lr: 3.2778e-05  max_mem: 7418M
[12/12 00:01:38 d2.utils.events]:  eta: 0:00:20  iter: 9939  total_loss: 2.149  loss_sem_seg: 0.9185  loss_center: 0.4415  loss_offset: 0.6466  time: 0.3496  data_time: 0.0263  lr: 2.5394e-05  max_mem: 7418M
[12/12 00:01:45 d2.utils.events]:  eta: 0:00:13  iter: 9959  total_loss: 1.841  loss_sem_seg: 0.8514  loss_center: 0.4124  loss_offset: 0.4868  time: 0.3496  data_time: 0.0244  lr: 1.776e-05  max_mem: 7418M
[12/12 00:01:52 d2.utils.events]:  eta: 0:00:06  iter: 9979  total_loss: 1.896  loss_sem_seg: 0.8633  loss_center: 0.4477  loss_offset: 0.4284  time: 0.3496  data_time: 0.0267  lr: 9.7261e-06  max_mem: 7418M
[12/12 00:01:59 fvcore.common.checkpoint]: Saving checkpoint to ./output/model_0009999.pth
[12/12 00:02:00 fvcore.common.checkpoint]: Saving checkpoint to ./output/model_final.pth
[12/12 00:02:00 d2.utils.events]:  eta: 0:00:00  iter: 9999  total_loss: 1.801  loss_sem_seg: 0.8332  loss_center: 0.4581  loss_offset: 0.3921  time: 0.3496  data_time: 0.0239  lr: 6.2797e-07  max_mem: 7418M
[12/12 00:02:01 d2.engine.hooks]: Overall training speed: 9998 iterations in 0:58:15 (0.3496 s / it)
[12/12 00:02:01 d2.engine.hooks]: Total training time: 0:58:22 (0:00:07 on hooks)
[12/12 00:02:01 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=640, sample_style='choice')]
[12/12 00:02:01 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[12/12 00:02:01 d2.data.common]: Serializing 5000 elements to byte tensors and concatenating them all ...
[12/12 00:02:01 d2.data.common]: Serialized dataset takes 3.40 MiB
/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
[12/12 00:02:02 d2.evaluation.evaluator]: Start inference on 5000 batches
/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
[12/12 00:02:04 d2.evaluation.evaluator]: Inference done 11/5000. Dataloading: 0.0014 s/iter. Inference: 0.0651 s/iter. Eval: 0.0376 s/iter. Total: 0.1041 s/iter. ETA=0:08:39
[12/12 00:02:09 d2.evaluation.evaluator]: Inference done 66/5000. Dataloading: 0.0010 s/iter. Inference: 0.0612 s/iter. Eval: 0.0306 s/iter. Total: 0.0929 s/iter. ETA=0:07:38
[12/12 00:02:14 d2.evaluation.evaluator]: Inference done 119/5000. Dataloading: 0.0011 s/iter. Inference: 0.0613 s/iter. Eval: 0.0323 s/iter. Total: 0.0946 s/iter. ETA=0:07:41
[12/12 00:02:19 d2.evaluation.evaluator]: Inference done 170/5000. Dataloading: 0.0011 s/iter. Inference: 0.0619 s/iter. Eval: 0.0328 s/iter. Total: 0.0958 s/iter. ETA=0:07:42
[12/12 00:02:24 d2.evaluation.evaluator]: Inference done 224/5000. Dataloading: 0.0011 s/iter. Inference: 0.0614 s/iter. Eval: 0.0326 s/iter. Total: 0.0951 s/iter. ETA=0:07:34
[12/12 00:02:29 d2.evaluation.evaluator]: Inference done 273/5000. Dataloading: 0.0011 s/iter. Inference: 0.0627 s/iter. Eval: 0.0326 s/iter. Total: 0.0964 s/iter. ETA=0:07:35
[12/12 00:02:34 d2.evaluation.evaluator]: Inference done 324/5000. Dataloading: 0.0011 s/iter. Inference: 0.0630 s/iter. Eval: 0.0327 s/iter. Total: 0.0968 s/iter. ETA=0:07:32
[12/12 00:02:39 d2.evaluation.evaluator]: Inference done 376/5000. Dataloading: 0.0011 s/iter. Inference: 0.0630 s/iter. Eval: 0.0327 s/iter. Total: 0.0969 s/iter. ETA=0:07:28
[12/12 00:02:44 d2.evaluation.evaluator]: Inference done 430/5000. Dataloading: 0.0011 s/iter. Inference: 0.0628 s/iter. Eval: 0.0325 s/iter. Total: 0.0964 s/iter. ETA=0:07:20
[12/12 00:02:49 d2.evaluation.evaluator]: Inference done 484/5000. Dataloading: 0.0011 s/iter. Inference: 0.0627 s/iter. Eval: 0.0323 s/iter. Total: 0.0962 s/iter. ETA=0:07:14
[12/12 00:02:54 d2.evaluation.evaluator]: Inference done 534/5000. Dataloading: 0.0011 s/iter. Inference: 0.0629 s/iter. Eval: 0.0325 s/iter. Total: 0.0966 s/iter. ETA=0:07:11
[12/12 00:03:00 d2.evaluation.evaluator]: Inference done 584/5000. Dataloading: 0.0011 s/iter. Inference: 0.0632 s/iter. Eval: 0.0327 s/iter. Total: 0.0970 s/iter. ETA=0:07:08
[12/12 00:03:05 d2.evaluation.evaluator]: Inference done 635/5000. Dataloading: 0.0011 s/iter. Inference: 0.0633 s/iter. Eval: 0.0327 s/iter. Total: 0.0972 s/iter. ETA=0:07:04
[12/12 00:03:10 d2.evaluation.evaluator]: Inference done 686/5000. Dataloading: 0.0011 s/iter. Inference: 0.0633 s/iter. Eval: 0.0329 s/iter. Total: 0.0973 s/iter. ETA=0:06:59
[12/12 00:03:15 d2.evaluation.evaluator]: Inference done 737/5000. Dataloading: 0.0011 s/iter. Inference: 0.0634 s/iter. Eval: 0.0329 s/iter. Total: 0.0974 s/iter. ETA=0:06:55
[12/12 00:03:20 d2.evaluation.evaluator]: Inference done 790/5000. Dataloading: 0.0011 s/iter. Inference: 0.0632 s/iter. Eval: 0.0329 s/iter. Total: 0.0972 s/iter. ETA=0:06:49
[12/12 00:03:25 d2.evaluation.evaluator]: Inference done 844/5000. Dataloading: 0.0011 s/iter. Inference: 0.0631 s/iter. Eval: 0.0327 s/iter. Total: 0.0970 s/iter. ETA=0:06:43
[12/12 00:03:30 d2.evaluation.evaluator]: Inference done 898/5000. Dataloading: 0.0011 s/iter. Inference: 0.0630 s/iter. Eval: 0.0326 s/iter. Total: 0.0967 s/iter. ETA=0:06:36
[12/12 00:03:35 d2.evaluation.evaluator]: Inference done 951/5000. Dataloading: 0.0011 s/iter. Inference: 0.0629 s/iter. Eval: 0.0325 s/iter. Total: 0.0966 s/iter. ETA=0:06:31
[12/12 00:03:40 d2.evaluation.evaluator]: Inference done 1004/5000. Dataloading: 0.0011 s/iter. Inference: 0.0629 s/iter. Eval: 0.0325 s/iter. Total: 0.0965 s/iter. ETA=0:06:25
[12/12 00:03:45 d2.evaluation.evaluator]: Inference done 1057/5000. Dataloading: 0.0011 s/iter. Inference: 0.0628 s/iter. Eval: 0.0325 s/iter. Total: 0.0964 s/iter. ETA=0:06:20
[12/12 00:03:50 d2.evaluation.evaluator]: Inference done 1111/5000. Dataloading: 0.0011 s/iter. Inference: 0.0627 s/iter. Eval: 0.0324 s/iter. Total: 0.0963 s/iter. ETA=0:06:14
[12/12 00:03:55 d2.evaluation.evaluator]: Inference done 1165/5000. Dataloading: 0.0011 s/iter. Inference: 0.0626 s/iter. Eval: 0.0324 s/iter. Total: 0.0961 s/iter. ETA=0:06:08
[12/12 00:04:00 d2.evaluation.evaluator]: Inference done 1216/5000. Dataloading: 0.0011 s/iter. Inference: 0.0627 s/iter. Eval: 0.0325 s/iter. Total: 0.0963 s/iter. ETA=0:06:04
[12/12 00:04:05 d2.evaluation.evaluator]: Inference done 1267/5000. Dataloading: 0.0011 s/iter. Inference: 0.0627 s/iter. Eval: 0.0325 s/iter. Total: 0.0964 s/iter. ETA=0:05:59
[12/12 00:04:10 d2.evaluation.evaluator]: Inference done 1320/5000. Dataloading: 0.0011 s/iter. Inference: 0.0627 s/iter. Eval: 0.0325 s/iter. Total: 0.0964 s/iter. ETA=0:05:54
[12/12 00:04:15 d2.evaluation.evaluator]: Inference done 1371/5000. Dataloading: 0.0011 s/iter. Inference: 0.0627 s/iter. Eval: 0.0326 s/iter. Total: 0.0964 s/iter. ETA=0:05:50
[12/12 00:04:20 d2.evaluation.evaluator]: Inference done 1424/5000. Dataloading: 0.0011 s/iter. Inference: 0.0627 s/iter. Eval: 0.0326 s/iter. Total: 0.0964 s/iter. ETA=0:05:44
[12/12 00:04:25 d2.evaluation.evaluator]: Inference done 1477/5000. Dataloading: 0.0011 s/iter. Inference: 0.0626 s/iter. Eval: 0.0326 s/iter. Total: 0.0964 s/iter. ETA=0:05:39
[12/12 00:04:30 d2.evaluation.evaluator]: Inference done 1530/5000. Dataloading: 0.0011 s/iter. Inference: 0.0625 s/iter. Eval: 0.0326 s/iter. Total: 0.0963 s/iter. ETA=0:05:34
[12/12 00:04:35 d2.evaluation.evaluator]: Inference done 1581/5000. Dataloading: 0.0011 s/iter. Inference: 0.0626 s/iter. Eval: 0.0327 s/iter. Total: 0.0964 s/iter. ETA=0:05:29
[12/12 00:04:40 d2.evaluation.evaluator]: Inference done 1633/5000. Dataloading: 0.0011 s/iter. Inference: 0.0625 s/iter. Eval: 0.0327 s/iter. Total: 0.0964 s/iter. ETA=0:05:24
[12/12 00:04:45 d2.evaluation.evaluator]: Inference done 1686/5000. Dataloading: 0.0011 s/iter. Inference: 0.0625 s/iter. Eval: 0.0327 s/iter. Total: 0.0964 s/iter. ETA=0:05:19
[12/12 00:04:50 d2.evaluation.evaluator]: Inference done 1740/5000. Dataloading: 0.0011 s/iter. Inference: 0.0625 s/iter. Eval: 0.0327 s/iter. Total: 0.0963 s/iter. ETA=0:05:13
[12/12 00:04:56 d2.evaluation.evaluator]: Inference done 1794/5000. Dataloading: 0.0011 s/iter. Inference: 0.0624 s/iter. Eval: 0.0327 s/iter. Total: 0.0962 s/iter. ETA=0:05:08
[12/12 00:05:01 d2.evaluation.evaluator]: Inference done 1847/5000. Dataloading: 0.0011 s/iter. Inference: 0.0624 s/iter. Eval: 0.0327 s/iter. Total: 0.0962 s/iter. ETA=0:05:03
[12/12 00:05:06 d2.evaluation.evaluator]: Inference done 1900/5000. Dataloading: 0.0011 s/iter. Inference: 0.0624 s/iter. Eval: 0.0327 s/iter. Total: 0.0962 s/iter. ETA=0:04:58
[12/12 00:05:11 d2.evaluation.evaluator]: Inference done 1955/5000. Dataloading: 0.0011 s/iter. Inference: 0.0623 s/iter. Eval: 0.0326 s/iter. Total: 0.0960 s/iter. ETA=0:04:52
[12/12 00:05:16 d2.evaluation.evaluator]: Inference done 2007/5000. Dataloading: 0.0011 s/iter. Inference: 0.0623 s/iter. Eval: 0.0327 s/iter. Total: 0.0961 s/iter. ETA=0:04:47
[12/12 00:05:21 d2.evaluation.evaluator]: Inference done 2059/5000. Dataloading: 0.0011 s/iter. Inference: 0.0623 s/iter. Eval: 0.0327 s/iter. Total: 0.0961 s/iter. ETA=0:04:42
[12/12 00:05:26 d2.evaluation.evaluator]: Inference done 2112/5000. Dataloading: 0.0011 s/iter. Inference: 0.0623 s/iter. Eval: 0.0327 s/iter. Total: 0.0961 s/iter. ETA=0:04:37
[12/12 00:05:31 d2.evaluation.evaluator]: Inference done 2166/5000. Dataloading: 0.0011 s/iter. Inference: 0.0623 s/iter. Eval: 0.0326 s/iter. Total: 0.0960 s/iter. ETA=0:04:32
[12/12 00:05:36 d2.evaluation.evaluator]: Inference done 2217/5000. Dataloading: 0.0011 s/iter. Inference: 0.0623 s/iter. Eval: 0.0326 s/iter. Total: 0.0961 s/iter. ETA=0:04:27
[12/12 00:05:41 d2.evaluation.evaluator]: Inference done 2270/5000. Dataloading: 0.0011 s/iter. Inference: 0.0623 s/iter. Eval: 0.0326 s/iter. Total: 0.0961 s/iter. ETA=0:04:22
[12/12 00:05:46 d2.evaluation.evaluator]: Inference done 2325/5000. Dataloading: 0.0011 s/iter. Inference: 0.0623 s/iter. Eval: 0.0326 s/iter. Total: 0.0960 s/iter. ETA=0:04:16
[12/12 00:05:51 d2.evaluation.evaluator]: Inference done 2380/5000. Dataloading: 0.0011 s/iter. Inference: 0.0622 s/iter. Eval: 0.0325 s/iter. Total: 0.0959 s/iter. ETA=0:04:11
[12/12 00:05:56 d2.evaluation.evaluator]: Inference done 2430/5000. Dataloading: 0.0011 s/iter. Inference: 0.0623 s/iter. Eval: 0.0326 s/iter. Total: 0.0960 s/iter. ETA=0:04:06
[12/12 00:06:01 d2.evaluation.evaluator]: Inference done 2484/5000. Dataloading: 0.0011 s/iter. Inference: 0.0622 s/iter. Eval: 0.0326 s/iter. Total: 0.0959 s/iter. ETA=0:04:01
[12/12 00:06:06 d2.evaluation.evaluator]: Inference done 2537/5000. Dataloading: 0.0011 s/iter. Inference: 0.0622 s/iter. Eval: 0.0326 s/iter. Total: 0.0959 s/iter. ETA=0:03:56
[12/12 00:06:11 d2.evaluation.evaluator]: Inference done 2590/5000. Dataloading: 0.0011 s/iter. Inference: 0.0622 s/iter. Eval: 0.0326 s/iter. Total: 0.0959 s/iter. ETA=0:03:51
[12/12 00:06:16 d2.evaluation.evaluator]: Inference done 2641/5000. Dataloading: 0.0011 s/iter. Inference: 0.0622 s/iter. Eval: 0.0326 s/iter. Total: 0.0960 s/iter. ETA=0:03:46
[12/12 00:06:21 d2.evaluation.evaluator]: Inference done 2697/5000. Dataloading: 0.0011 s/iter. Inference: 0.0622 s/iter. Eval: 0.0325 s/iter. Total: 0.0958 s/iter. ETA=0:03:40
[12/12 00:06:26 d2.evaluation.evaluator]: Inference done 2753/5000. Dataloading: 0.0011 s/iter. Inference: 0.0621 s/iter. Eval: 0.0325 s/iter. Total: 0.0957 s/iter. ETA=0:03:35
[12/12 00:06:31 d2.evaluation.evaluator]: Inference done 2807/5000. Dataloading: 0.0011 s/iter. Inference: 0.0620 s/iter. Eval: 0.0325 s/iter. Total: 0.0957 s/iter. ETA=0:03:29
[12/12 00:06:37 d2.evaluation.evaluator]: Inference done 2861/5000. Dataloading: 0.0011 s/iter. Inference: 0.0620 s/iter. Eval: 0.0325 s/iter. Total: 0.0956 s/iter. ETA=0:03:24
[12/12 00:06:42 d2.evaluation.evaluator]: Inference done 2914/5000. Dataloading: 0.0011 s/iter. Inference: 0.0620 s/iter. Eval: 0.0325 s/iter. Total: 0.0956 s/iter. ETA=0:03:19
[12/12 00:06:47 d2.evaluation.evaluator]: Inference done 2969/5000. Dataloading: 0.0011 s/iter. Inference: 0.0619 s/iter. Eval: 0.0324 s/iter. Total: 0.0955 s/iter. ETA=0:03:14
[12/12 00:06:52 d2.evaluation.evaluator]: Inference done 3024/5000. Dataloading: 0.0011 s/iter. Inference: 0.0619 s/iter. Eval: 0.0324 s/iter. Total: 0.0955 s/iter. ETA=0:03:08
[12/12 00:06:57 d2.evaluation.evaluator]: Inference done 3079/5000. Dataloading: 0.0011 s/iter. Inference: 0.0619 s/iter. Eval: 0.0324 s/iter. Total: 0.0954 s/iter. ETA=0:03:03
[12/12 00:07:02 d2.evaluation.evaluator]: Inference done 3132/5000. Dataloading: 0.0011 s/iter. Inference: 0.0619 s/iter. Eval: 0.0324 s/iter. Total: 0.0954 s/iter. ETA=0:02:58
[12/12 00:07:07 d2.evaluation.evaluator]: Inference done 3186/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0324 s/iter. Total: 0.0954 s/iter. ETA=0:02:53
[12/12 00:07:12 d2.evaluation.evaluator]: Inference done 3237/5000. Dataloading: 0.0011 s/iter. Inference: 0.0619 s/iter. Eval: 0.0325 s/iter. Total: 0.0955 s/iter. ETA=0:02:48
[12/12 00:07:17 d2.evaluation.evaluator]: Inference done 3291/5000. Dataloading: 0.0011 s/iter. Inference: 0.0619 s/iter. Eval: 0.0324 s/iter. Total: 0.0955 s/iter. ETA=0:02:43
[12/12 00:07:22 d2.evaluation.evaluator]: Inference done 3342/5000. Dataloading: 0.0011 s/iter. Inference: 0.0619 s/iter. Eval: 0.0325 s/iter. Total: 0.0955 s/iter. ETA=0:02:38
[12/12 00:07:27 d2.evaluation.evaluator]: Inference done 3394/5000. Dataloading: 0.0011 s/iter. Inference: 0.0619 s/iter. Eval: 0.0325 s/iter. Total: 0.0956 s/iter. ETA=0:02:33
[12/12 00:07:32 d2.evaluation.evaluator]: Inference done 3444/5000. Dataloading: 0.0011 s/iter. Inference: 0.0620 s/iter. Eval: 0.0325 s/iter. Total: 0.0956 s/iter. ETA=0:02:28
[12/12 00:07:37 d2.evaluation.evaluator]: Inference done 3500/5000. Dataloading: 0.0011 s/iter. Inference: 0.0619 s/iter. Eval: 0.0325 s/iter. Total: 0.0955 s/iter. ETA=0:02:23
[12/12 00:07:42 d2.evaluation.evaluator]: Inference done 3552/5000. Dataloading: 0.0011 s/iter. Inference: 0.0619 s/iter. Eval: 0.0325 s/iter. Total: 0.0955 s/iter. ETA=0:02:18
[12/12 00:07:47 d2.evaluation.evaluator]: Inference done 3605/5000. Dataloading: 0.0011 s/iter. Inference: 0.0619 s/iter. Eval: 0.0325 s/iter. Total: 0.0955 s/iter. ETA=0:02:13
[12/12 00:07:52 d2.evaluation.evaluator]: Inference done 3660/5000. Dataloading: 0.0011 s/iter. Inference: 0.0619 s/iter. Eval: 0.0325 s/iter. Total: 0.0955 s/iter. ETA=0:02:07
[12/12 00:07:57 d2.evaluation.evaluator]: Inference done 3713/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0325 s/iter. Total: 0.0955 s/iter. ETA=0:02:02
[12/12 00:08:02 d2.evaluation.evaluator]: Inference done 3767/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0325 s/iter. Total: 0.0954 s/iter. ETA=0:01:57
[12/12 00:08:08 d2.evaluation.evaluator]: Inference done 3820/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0325 s/iter. Total: 0.0955 s/iter. ETA=0:01:52
[12/12 00:08:13 d2.evaluation.evaluator]: Inference done 3875/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0324 s/iter. Total: 0.0954 s/iter. ETA=0:01:47
[12/12 00:08:18 d2.evaluation.evaluator]: Inference done 3929/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0324 s/iter. Total: 0.0954 s/iter. ETA=0:01:42
[12/12 00:08:23 d2.evaluation.evaluator]: Inference done 3983/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0324 s/iter. Total: 0.0953 s/iter. ETA=0:01:36
[12/12 00:08:28 d2.evaluation.evaluator]: Inference done 4035/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0324 s/iter. Total: 0.0953 s/iter. ETA=0:01:32
[12/12 00:08:33 d2.evaluation.evaluator]: Inference done 4086/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0324 s/iter. Total: 0.0954 s/iter. ETA=0:01:27
[12/12 00:08:38 d2.evaluation.evaluator]: Inference done 4140/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0324 s/iter. Total: 0.0954 s/iter. ETA=0:01:22
[12/12 00:08:43 d2.evaluation.evaluator]: Inference done 4191/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0325 s/iter. Total: 0.0954 s/iter. ETA=0:01:17
[12/12 00:08:48 d2.evaluation.evaluator]: Inference done 4244/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0324 s/iter. Total: 0.0954 s/iter. ETA=0:01:12
[12/12 00:08:53 d2.evaluation.evaluator]: Inference done 4296/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0325 s/iter. Total: 0.0954 s/iter. ETA=0:01:07
[12/12 00:08:58 d2.evaluation.evaluator]: Inference done 4351/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0324 s/iter. Total: 0.0954 s/iter. ETA=0:01:01
[12/12 00:09:03 d2.evaluation.evaluator]: Inference done 4405/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0324 s/iter. Total: 0.0953 s/iter. ETA=0:00:56
[12/12 00:09:08 d2.evaluation.evaluator]: Inference done 4458/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0324 s/iter. Total: 0.0953 s/iter. ETA=0:00:51
[12/12 00:09:13 d2.evaluation.evaluator]: Inference done 4511/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0324 s/iter. Total: 0.0953 s/iter. ETA=0:00:46
[12/12 00:09:18 d2.evaluation.evaluator]: Inference done 4562/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0324 s/iter. Total: 0.0953 s/iter. ETA=0:00:41
[12/12 00:09:23 d2.evaluation.evaluator]: Inference done 4615/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0324 s/iter. Total: 0.0953 s/iter. ETA=0:00:36
[12/12 00:09:28 d2.evaluation.evaluator]: Inference done 4667/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0324 s/iter. Total: 0.0954 s/iter. ETA=0:00:31
[12/12 00:09:33 d2.evaluation.evaluator]: Inference done 4720/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0324 s/iter. Total: 0.0953 s/iter. ETA=0:00:26
[12/12 00:09:38 d2.evaluation.evaluator]: Inference done 4775/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0324 s/iter. Total: 0.0953 s/iter. ETA=0:00:21
[12/12 00:09:43 d2.evaluation.evaluator]: Inference done 4826/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0324 s/iter. Total: 0.0954 s/iter. ETA=0:00:16
[12/12 00:09:48 d2.evaluation.evaluator]: Inference done 4882/5000. Dataloading: 0.0011 s/iter. Inference: 0.0617 s/iter. Eval: 0.0324 s/iter. Total: 0.0953 s/iter. ETA=0:00:11
[12/12 00:09:53 d2.evaluation.evaluator]: Inference done 4933/5000. Dataloading: 0.0011 s/iter. Inference: 0.0618 s/iter. Eval: 0.0324 s/iter. Total: 0.0953 s/iter. ETA=0:00:06
[12/12 00:09:58 d2.evaluation.evaluator]: Inference done 4988/5000. Dataloading: 0.0011 s/iter. Inference: 0.0617 s/iter. Eval: 0.0324 s/iter. Total: 0.0953 s/iter. ETA=0:00:01
[12/12 00:10:00 d2.evaluation.evaluator]: Total inference time: 0:07:56.126141 (0.095321 s / iter per device, on 1 devices)
[12/12 00:10:00 d2.evaluation.evaluator]: Total inference pure compute time: 0:05:08 (0.061741 s / iter per device, on 1 devices)
[12/12 00:10:00 d2.evaluation.panoptic_evaluation]: Writing all panoptic predictions to /tmp/panoptic_evalvc5qkkqq ...
[12/12 00:10:22 d2.evaluation.panoptic_evaluation]: Panoptic Evaluation Results:
|        |   PQ   |   SQ   |   RQ   |  #categories  |
|:------:|:------:|:------:|:------:|:-------------:|
|  All   | 19.356 | 65.765 | 25.206 |      133      |
| Things | 19.513 | 68.198 | 25.472 |      80       |
| Stuff  | 19.120 | 62.093 | 24.805 |      53       |
[12/12 00:10:22 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...
[12/12 00:10:22 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json
[12/12 00:10:23 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
[12/12 00:10:23 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*
[12/12 00:10:30 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 7.51 seconds.
[12/12 00:10:30 d2.evaluation.fast_eval_api]: Accumulating evaluation results...
[12/12 00:10:31 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.61 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.076
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.148
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.070
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.011
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.067
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.132
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.095
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.130
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.130
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.012
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.106
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.228
[12/12 00:10:31 d2.evaluation.coco_evaluation]: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:-----:|:------:|:------:|:-----:|:-----:|:------:|
| 7.643 | 14.795 | 7.034  | 1.061 | 6.689 | 13.186 |
[12/12 00:10:31 d2.evaluation.coco_evaluation]: Per-category bbox AP: 
| category      | AP     | category     | AP     | category       | AP     |
|:--------------|:-------|:-------------|:-------|:---------------|:-------|
| person        | 15.176 | bicycle      | 4.768  | car            | 7.220  |
| motorcycle    | 7.802  | airplane     | 23.983 | bus            | 32.289 |
| train         | 31.727 | truck        | 2.967  | boat           | 2.424  |
| traffic light | 2.530  | fire hydrant | 25.969 | stop sign      | 30.119 |
| parking meter | 5.094  | bench        | 3.201  | bird           | 6.513  |
| cat           | 12.406 | dog          | 15.703 | horse          | 18.477 |
| sheep         | 11.118 | cow          | 13.856 | elephant       | 25.569 |
| bear          | 32.681 | zebra        | 33.239 | giraffe        | 28.584 |
| backpack      | 0.231  | umbrella     | 8.771  | handbag        | 0.000  |
| tie           | 0.000  | suitcase     | 3.952  | frisbee        | 12.967 |
| skis          | 1.002  | snowboard    | 1.262  | sports ball    | 3.422  |
| kite          | 8.511  | baseball bat | 0.490  | baseball glove | 2.243  |
| skateboard    | 5.251  | surfboard    | 4.203  | tennis racket  | 9.829  |
| bottle        | 0.935  | wine glass   | 0.207  | cup            | 1.884  |
| fork          | 0.008  | knife        | 0.000  | spoon          | 0.000  |
| bowl          | 1.835  | banana       | 1.941  | apple          | 0.733  |
| sandwich      | 0.336  | orange       | 2.987  | broccoli       | 1.532  |
| carrot        | 0.525  | hot dog      | 0.121  | pizza          | 4.847  |
| donut         | 2.481  | cake         | 0.643  | chair          | 3.430  |
| couch         | 13.227 | potted plant | 1.755  | bed            | 14.828 |
| dining table  | 2.714  | toilet       | 19.059 | tv             | 18.433 |
| laptop        | 11.851 | mouse        | 3.765  | remote         | 0.652  |
| keyboard      | 3.464  | cell phone   | 1.447  | microwave      | 7.229  |
| oven          | 2.856  | toaster      | 0.000  | sink           | 4.751  |
| refrigerator  | 6.500  | book         | 0.104  | clock          | 12.546 |
| vase          | 1.602  | scissors     | 0.000  | teddy bear     | 4.503  |
| hair drier    | 0.000  | toothbrush   | 0.198  |                |        |
Loading and preparing results...
DONE (t=0.46s)
creating index...
index created!
[12/12 00:10:32 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*
[12/12 00:10:41 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 8.74 seconds.
[12/12 00:10:41 d2.evaluation.fast_eval_api]: Accumulating evaluation results...
[12/12 00:10:42 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.67 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.081
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.159
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.074
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.006
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.071
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.172
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.095
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.124
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.125
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.108
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.228
[12/12 00:10:43 d2.evaluation.coco_evaluation]: Evaluation results for segm: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:-----:|:------:|:------:|:-----:|:-----:|:------:|
| 8.066 | 15.878 | 7.438  | 0.632 | 7.057 | 17.240 |
[12/12 00:10:43 d2.evaluation.coco_evaluation]: Per-category segm AP: 
| category      | AP     | category     | AP     | category       | AP     |
|:--------------|:-------|:-------------|:-------|:---------------|:-------|
| person        | 11.212 | bicycle      | 2.824  | car            | 6.789  |
| motorcycle    | 5.122  | airplane     | 19.944 | bus            | 33.424 |
| train         | 35.790 | truck        | 3.094  | boat           | 2.446  |
| traffic light | 3.292  | fire hydrant | 32.595 | stop sign      | 34.441 |
| parking meter | 6.400  | bench        | 2.307  | bird           | 5.472  |
| cat           | 19.945 | dog          | 16.913 | horse          | 12.880 |
| sheep         | 10.031 | cow          | 11.913 | elephant       | 25.107 |
| bear          | 36.352 | zebra        | 22.870 | giraffe        | 21.388 |
| backpack      | 0.198  | umbrella     | 13.396 | handbag        | 0.000  |
| tie           | 0.000  | suitcase     | 6.247  | frisbee        | 10.001 |
| skis          | 0.006  | snowboard    | 1.032  | sports ball    | 4.124  |
| kite          | 3.654  | baseball bat | 0.489  | baseball glove | 3.413  |
| skateboard    | 2.670  | surfboard    | 3.627  | tennis racket  | 17.529 |
| bottle        | 2.074  | wine glass   | 0.305  | cup            | 3.830  |
| fork          | 0.038  | knife        | 0.000  | spoon          | 0.000  |
| bowl          | 4.284  | banana       | 1.019  | apple          | 0.831  |
| sandwich      | 1.044  | orange       | 3.746  | broccoli       | 2.537  |
| carrot        | 0.331  | hot dog      | 1.294  | pizza          | 5.038  |
| donut         | 4.636  | cake         | 1.006  | chair          | 2.308  |
| couch         | 12.024 | potted plant | 1.436  | bed            | 13.309 |
| dining table  | 0.168  | toilet       | 27.210 | tv             | 24.975 |
| laptop        | 17.837 | mouse        | 7.673  | remote         | 0.587  |
| keyboard      | 8.581  | cell phone   | 2.694  | microwave      | 9.748  |
| oven          | 2.031  | toaster      | 0.000  | sink           | 5.596  |
| refrigerator  | 4.432  | book         | 0.098  | clock          | 14.725 |
| vase          | 2.402  | scissors     | 0.000  | teddy bear     | 6.299  |
| hair drier    | 0.000  | toothbrush   | 0.231  |                |        |
[12/12 00:10:47 d2.engine.defaults]: Evaluation results for coco_2017_val_panoptic in csv format:
[12/12 00:10:47 d2.evaluation.testing]: copypaste: Task: panoptic_seg
[12/12 00:10:47 d2.evaluation.testing]: copypaste: PQ,SQ,RQ,PQ_th,SQ_th,RQ_th,PQ_st,SQ_st,RQ_st
[12/12 00:10:47 d2.evaluation.testing]: copypaste: 19.3563,65.7653,25.2061,19.5130,68.1982,25.4718,19.1198,62.0931,24.8050
[12/12 00:10:47 d2.evaluation.testing]: copypaste: Task: bbox
[12/12 00:10:47 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl
[12/12 00:10:47 d2.evaluation.testing]: copypaste: 7.6435,14.7954,7.0336,1.0614,6.6887,13.1857
[12/12 00:10:47 d2.evaluation.testing]: copypaste: Task: segm
[12/12 00:10:47 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl
[12/12 00:10:47 d2.evaluation.testing]: copypaste: 8.0665,15.8778,7.4383,0.6323,7.0566,17.2401