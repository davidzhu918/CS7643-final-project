env: DETECTRON2_DATASETS=/content/datasets
Command Line Args: Namespace(config_file='./detectron2/projects/Panoptic-DeepLab/configs/COCO-PanopticSegmentation/panoptic_deeplab_R_52_os16_mg124_poly_200k_bs64_crop_640_640_coco_dsconv.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.RESNETS.NORM', 'BN', 'MODEL.INS_EMBED_HEAD.NORM', 'BN', 'MODEL.SEM_SEG_HEAD.NORM', 'BN', 'SOLVER.MAX_ITER', '10000', 'SOLVER.IMS_PER_BATCH', '5', 'SOLVER.BASE_LR', '0.0025', 'MODEL.BACKBONE.FREEZE_AT', '4', 'MODEL.RESNETS.DEPTH', '101', 'MODEL.WEIGHTS', 'detectron2://DeepLab/R-103.pkl'], resume=False)
Loading config ./detectron2/projects/Panoptic-DeepLab/configs/COCO-PanopticSegmentation/panoptic_deeplab_R_52_os16_mg124_poly_200k_bs64_crop_640_640_coco_dsconv.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
[12/11 19:51:35 detectron2]: Rank of current process: 0. World size: 1
[12/11 19:51:36 detectron2]: Environment info:
----------------------  ----------------------------------------------------------------
sys.platform            linux
Python                  3.7.12 (default, Jan 15 2022, 18:48:18) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/content/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.1
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.10.0+cu111 @/usr/local/lib/python3.7/dist-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   Tesla V100-SXM2-16GB (arch=7.0)
Driver version          460.32.03
CUDA_HOME               /usr/local/cuda
Pillow                  7.1.2
torchvision             0.11.1+cu111 @/usr/local/lib/python3.7/dist-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221122
iopath                  0.1.9
cv2                     4.1.2
----------------------  ----------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[12/11 19:51:36 detectron2]: Command line arguments: Namespace(config_file='./detectron2/projects/Panoptic-DeepLab/configs/COCO-PanopticSegmentation/panoptic_deeplab_R_52_os16_mg124_poly_200k_bs64_crop_640_640_coco_dsconv.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.RESNETS.NORM', 'BN', 'MODEL.INS_EMBED_HEAD.NORM', 'BN', 'MODEL.SEM_SEG_HEAD.NORM', 'BN', 'SOLVER.MAX_ITER', '10000', 'SOLVER.IMS_PER_BATCH', '5', 'SOLVER.BASE_LR', '0.0025', 'MODEL.BACKBONE.FREEZE_AT', '4', 'MODEL.RESNETS.DEPTH', '101', 'MODEL.WEIGHTS', 'detectron2://DeepLab/R-103.pkl'], resume=False)
[12/11 19:51:36 detectron2]: Contents of args.config_file=./detectron2/projects/Panoptic-DeepLab/configs/COCO-PanopticSegmentation/panoptic_deeplab_R_52_os16_mg124_poly_200k_bs64_crop_640_640_coco_dsconv.yaml:
_BASE_: ../Cityscapes-PanopticSegmentation/Base-PanopticDeepLab-OS16.yaml
MODEL:
  WEIGHTS: "detectron2://DeepLab/R-52.pkl"
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  PIXEL_STD: [58.395, 57.120, 57.375]
  BACKBONE:
    NAME: "build_resnet_deeplab_backbone"
  RESNETS:
    DEPTH: 50
    NORM: "SyncBN"
    RES5_MULTI_GRID: [1, 2, 4]
    STEM_TYPE: "deeplab"
    STEM_OUT_CHANNELS: 128
    STRIDE_IN_1X1: False
  SEM_SEG_HEAD:
    NUM_CLASSES: 133
    LOSS_TOP_K: 1.0
    USE_DEPTHWISE_SEPARABLE_CONV: True
  PANOPTIC_DEEPLAB:
    STUFF_AREA: 4096
    NMS_KERNEL: 41
    SIZE_DIVISIBILITY: 640
    USE_DEPTHWISE_SEPARABLE_CONV: True
DATASETS:
  TRAIN: ("coco_2017_train_panoptic",)
  TEST: ("coco_2017_val_panoptic",)
SOLVER:
  BASE_LR: 0.0005
  MAX_ITER: 200000
  IMS_PER_BATCH: 64
INPUT:
  FORMAT: "RGB"
  GAUSSIAN_SIGMA: 8
  MIN_SIZE_TRAIN: !!python/object/apply:eval ["[int(x * 0.1 * 640) for x in range(5, 16)]"]
  MIN_SIZE_TRAIN_SAMPLING: "choice"
  MIN_SIZE_TEST: 640
  MAX_SIZE_TRAIN: 960
  MAX_SIZE_TEST: 640
  CROP:
    ENABLED: True
    TYPE: "absolute"
    SIZE: (640, 640)

[12/11 19:51:36 detectron2]: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 10
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_val_panoptic
  TRAIN:
  - coco_2017_train_panoptic
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: true
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 640
    - 640
    TYPE: absolute
  FORMAT: RGB
  GAUSSIAN_SIGMA: 8
  IGNORE_CROWD_IN_SEMANTIC: false
  IGNORE_STUFF_IN_OFFSET: true
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 640
  MAX_SIZE_TRAIN: 960
  MIN_SIZE_TEST: 640
  MIN_SIZE_TRAIN:
  - 320
  - 384
  - 448
  - 512
  - 576
  - 640
  - 704
  - 768
  - 832
  - 896
  - 960
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SMALL_INSTANCE_AREA: 4096
  SMALL_INSTANCE_WEIGHT: 3
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 4
    NAME: build_resnet_deeplab_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  INS_EMBED_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    CENTER_LOSS_WEIGHT: 200.0
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    HEAD_CHANNELS: 32
    IN_FEATURES:
    - res2
    - res3
    - res5
    NAME: PanopticDeepLabInsEmbedHead
    NORM: BN
    OFFSET_LOSS_WEIGHT: 0.01
    PROJECT_CHANNELS:
    - 32
    - 64
    PROJECT_FEATURES:
    - res2
    - res3
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: PanopticDeepLab
  PANOPTIC_DEEPLAB:
    BENCHMARK_NETWORK_SPEED: false
    CENTER_THRESHOLD: 0.1
    NMS_KERNEL: 41
    PREDICT_INSTANCES: true
    SIZE_DIVISIBILITY: 640
    STUFF_AREA: 4096
    TOP_K_INSTANCE: 200
    USE_DEPTHWISE_SEPARABLE_CONV: true
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 101
    NORM: BN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 2
    RES5_MULTI_GRID:
    - 1
    - 2
    - 4
    STEM_OUT_CHANNELS: 128
    STEM_TYPE: deeplab
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id001
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    HEAD_CHANNELS: 256
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res5
    LOSS_TOP_K: 1.0
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    NAME: PanopticDeepLabSemSegHead
    NORM: BN
    NUM_CLASSES: 133
    PROJECT_CHANNELS:
    - 32
    - 64
    PROJECT_FEATURES:
    - res2
    - res3
    USE_DEPTHWISE_SEPARABLE_CONV: true
  WEIGHTS: detectron2://DeepLab/R-103.pkl
OUTPUT_DIR: ./output
SEED: -1
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 0.0025
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: value
    CLIP_VALUE: 1.0
    ENABLED: false
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 5
  LR_SCHEDULER_NAME: WarmupPolyLR
  MAX_ITER: 10000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAM
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0
  WEIGHT_DECAY_BIAS: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[12/11 19:51:36 detectron2]: Full config saved to ./output/config.yaml
[12/11 19:51:36 d2.utils.env]: Using a generated random seed 36560001
[12/11 19:51:48 d2.engine.defaults]: Model:
PanopticDeepLab(
  (backbone): ResNet(
    (stem): DeepLabStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
      (conv3): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False
          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False
          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8), bias=False
          (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): PanopticDeepLabSemSegHead(
    (decoder): ModuleDict(
      (res2): ModuleDict(
        (project_conv): Conv2d(
          256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (fuse_conv): DepthwiseSeparableConv2d(
          (depthwise): Conv2d(
            288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False
            (norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pointwise): Conv2d(
            288, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (res3): ModuleDict(
        (project_conv): Conv2d(
          512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (fuse_conv): DepthwiseSeparableConv2d(
          (depthwise): Conv2d(
            320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320, bias=False
            (norm): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pointwise): Conv2d(
            320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (res5): ModuleDict(
        (project_conv): ASPP(
          (convs): ModuleList(
            (0): Conv2d(
              2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): DepthwiseSeparableConv2d(
              (depthwise): Conv2d(
                2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), groups=2048, bias=False
                (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (pointwise): Conv2d(
                2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): DepthwiseSeparableConv2d(
              (depthwise): Conv2d(
                2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=2048, bias=False
                (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (pointwise): Conv2d(
                2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (3): DepthwiseSeparableConv2d(
              (depthwise): Conv2d(
                2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), groups=2048, bias=False
                (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (pointwise): Conv2d(
                2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (4): Sequential(
              (0): AvgPool2d(kernel_size=(40, 40), stride=1, padding=0)
              (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (project): Conv2d(
            1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (fuse_conv): None
      )
    )
    (head): DepthwiseSeparableConv2d(
      (depthwise): Conv2d(
        256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256, bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (pointwise): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (predictor): Conv2d(256, 133, kernel_size=(1, 1), stride=(1, 1))
    (loss): DeepLabCE(
      (criterion): CrossEntropyLoss()
    )
  )
  (ins_embed_head): PanopticDeepLabInsEmbedHead(
    (decoder): ModuleDict(
      (res2): ModuleDict(
        (project_conv): Conv2d(
          256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (fuse_conv): DepthwiseSeparableConv2d(
          (depthwise): Conv2d(
            160, 160, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=160, bias=False
            (norm): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pointwise): Conv2d(
            160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (res3): ModuleDict(
        (project_conv): Conv2d(
          512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (fuse_conv): DepthwiseSeparableConv2d(
          (depthwise): Conv2d(
            320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320, bias=False
            (norm): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pointwise): Conv2d(
            320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (res5): ModuleDict(
        (project_conv): ASPP(
          (convs): ModuleList(
            (0): Conv2d(
              2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (1): DepthwiseSeparableConv2d(
              (depthwise): Conv2d(
                2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), groups=2048, bias=False
                (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (pointwise): Conv2d(
                2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): DepthwiseSeparableConv2d(
              (depthwise): Conv2d(
                2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=2048, bias=False
                (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (pointwise): Conv2d(
                2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (3): DepthwiseSeparableConv2d(
              (depthwise): Conv2d(
                2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), groups=2048, bias=False
                (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (pointwise): Conv2d(
                2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (4): Sequential(
              (0): AvgPool2d(kernel_size=(40, 40), stride=1, padding=0)
              (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (project): Conv2d(
            1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (fuse_conv): None
      )
    )
    (center_head): Sequential(
      (0): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): Conv2d(
        128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (center_predictor): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))
    (offset_head): DepthwiseSeparableConv2d(
      (depthwise): Conv2d(
        128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=128, bias=False
        (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (pointwise): Conv2d(
        128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (offset_predictor): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))
    (center_loss): MSELoss()
    (offset_loss): L1Loss()
  )
)
[12/11 19:51:48 d2.projects.panoptic_deeplab.dataset_mapper]: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(320, 384, 448, 512, 576, 640, 704, 768, 832, 896, 960), max_size=960, sample_style='choice'), RandomCrop(crop_type='absolute', crop_size=[640, 640]), RandomFlip()]
[12/11 19:51:56 d2.data.build]: Using training sampler TrainingSampler
[12/11 19:51:56 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[12/11 19:51:56 d2.data.common]: Serializing 118287 elements to byte tensors and concatenating them all ...
[12/11 19:51:57 d2.data.common]: Serialized dataset takes 80.32 MiB
/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
[12/11 19:52:03 fvcore.common.checkpoint]: [Checkpointer] Loading from detectron2://DeepLab/R-103.pkl ...
R-103.pkl: 179MB [00:03, 48.8MB/s]               
[12/11 19:52:07 fvcore.common.checkpoint]: Reading a file from 'torchvision'
[12/11 19:52:07 d2.checkpoint.c2_model_loading]: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint                                                                                        | Shapes                                             |
|:------------------|:-----------------------------------------------------------------------------------------------------------|:---------------------------------------------------|
| res2.0.conv1.*    | res2.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (64,) (64,) (64,) (64,) (64,128,1,1)               |
| res2.0.conv2.*    | res2.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (64,) (64,) (64,) (64,) (64,64,3,3)                |
| res2.0.conv3.*    | res2.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,64,1,1)           |
| res2.0.shortcut.* | res2.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                          | (256,) (256,) (256,) (256,) (256,128,1,1)          |
| res2.1.conv1.*    | res2.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (64,) (64,) (64,) (64,) (64,256,1,1)               |
| res2.1.conv2.*    | res2.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (64,) (64,) (64,) (64,) (64,64,3,3)                |
| res2.1.conv3.*    | res2.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,64,1,1)           |
| res2.2.conv1.*    | res2.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (64,) (64,) (64,) (64,) (64,256,1,1)               |
| res2.2.conv2.*    | res2.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (64,) (64,) (64,) (64,) (64,64,3,3)                |
| res2.2.conv3.*    | res2.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,64,1,1)           |
| res3.0.conv1.*    | res3.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (128,) (128,) (128,) (128,) (128,256,1,1)          |
| res3.0.conv2.*    | res3.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (128,) (128,) (128,) (128,) (128,128,3,3)          |
| res3.0.conv3.*    | res3.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (512,) (512,) (512,) (512,) (512,128,1,1)          |
| res3.0.shortcut.* | res3.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                          | (512,) (512,) (512,) (512,) (512,256,1,1)          |
| res3.1.conv1.*    | res3.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (128,) (128,) (128,) (128,) (128,512,1,1)          |
| res3.1.conv2.*    | res3.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (128,) (128,) (128,) (128,) (128,128,3,3)          |
| res3.1.conv3.*    | res3.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (512,) (512,) (512,) (512,) (512,128,1,1)          |
| res3.2.conv1.*    | res3.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (128,) (128,) (128,) (128,) (128,512,1,1)          |
| res3.2.conv2.*    | res3.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (128,) (128,) (128,) (128,) (128,128,3,3)          |
| res3.2.conv3.*    | res3.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (512,) (512,) (512,) (512,) (512,128,1,1)          |
| res3.3.conv1.*    | res3.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (128,) (128,) (128,) (128,) (128,512,1,1)          |
| res3.3.conv2.*    | res3.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (128,) (128,) (128,) (128,) (128,128,3,3)          |
| res3.3.conv3.*    | res3.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (512,) (512,) (512,) (512,) (512,128,1,1)          |
| res4.0.conv1.*    | res4.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,512,1,1)          |
| res4.0.conv2.*    | res4.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.0.conv3.*    | res4.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.0.shortcut.* | res4.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                          | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)     |
| res4.1.conv1.*    | res4.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.1.conv2.*    | res4.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.1.conv3.*    | res4.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.10.conv1.*   | res4.10.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.10.conv2.*   | res4.10.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.10.conv3.*   | res4.10.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.11.conv1.*   | res4.11.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.11.conv2.*   | res4.11.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.11.conv3.*   | res4.11.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.12.conv1.*   | res4.12.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.12.conv2.*   | res4.12.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.12.conv3.*   | res4.12.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.13.conv1.*   | res4.13.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.13.conv2.*   | res4.13.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.13.conv3.*   | res4.13.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.14.conv1.*   | res4.14.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.14.conv2.*   | res4.14.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.14.conv3.*   | res4.14.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.15.conv1.*   | res4.15.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.15.conv2.*   | res4.15.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.15.conv3.*   | res4.15.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.16.conv1.*   | res4.16.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.16.conv2.*   | res4.16.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.16.conv3.*   | res4.16.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.17.conv1.*   | res4.17.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.17.conv2.*   | res4.17.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.17.conv3.*   | res4.17.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.18.conv1.*   | res4.18.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.18.conv2.*   | res4.18.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.18.conv3.*   | res4.18.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.19.conv1.*   | res4.19.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.19.conv2.*   | res4.19.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.19.conv3.*   | res4.19.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.2.conv1.*    | res4.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.2.conv2.*    | res4.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.2.conv3.*    | res4.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.20.conv1.*   | res4.20.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.20.conv2.*   | res4.20.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.20.conv3.*   | res4.20.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.21.conv1.*   | res4.21.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.21.conv2.*   | res4.21.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.21.conv3.*   | res4.21.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.22.conv1.*   | res4.22.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.22.conv2.*   | res4.22.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.22.conv3.*   | res4.22.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                            | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.3.conv1.*    | res4.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.3.conv2.*    | res4.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.3.conv3.*    | res4.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.4.conv1.*    | res4.4.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.4.conv2.*    | res4.4.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.4.conv3.*    | res4.4.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.5.conv1.*    | res4.5.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.5.conv2.*    | res4.5.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.5.conv3.*    | res4.5.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.6.conv1.*    | res4.6.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.6.conv2.*    | res4.6.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.6.conv3.*    | res4.6.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.7.conv1.*    | res4.7.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.7.conv2.*    | res4.7.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.7.conv3.*    | res4.7.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.8.conv1.*    | res4.8.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.8.conv2.*    | res4.8.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.8.conv3.*    | res4.8.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res4.9.conv1.*    | res4.9.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,1024,1,1)         |
| res4.9.conv2.*    | res4.9.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (256,) (256,) (256,) (256,) (256,256,3,3)          |
| res4.9.conv3.*    | res4.9.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                             | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)     |
| res5.0.conv1.*    | res5.0.conv1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) () (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5.0.conv2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) () (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5.0.conv3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) () (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5.0.shortcut.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight} | (2048,) () (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5.1.conv1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) () (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5.1.conv2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) () (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5.1.conv3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) () (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5.2.conv1.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) () (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5.2.conv2.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) () (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5.2.conv3.{norm.bias,norm.num_batches_tracked,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) () (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.*      | stem.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                               | (64,) (64,) (64,) (64,) (64,3,3,3)                 |
| stem.conv2.*      | stem.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                               | (64,) (64,) (64,) (64,) (64,64,3,3)                |
| stem.conv3.*      | stem.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}                               | (128,) (128,) (128,) (128,) (128,64,3,3)           |
WARNING [12/11 19:52:10 fvcore.common.checkpoint]: Some model parameters or buffers are not found in the checkpoint:
ins_embed_head.center_head.0.norm.{bias, running_mean, running_var, weight}
ins_embed_head.center_head.0.weight
ins_embed_head.center_head.1.norm.{bias, running_mean, running_var, weight}
ins_embed_head.center_head.1.weight
ins_embed_head.center_predictor.{bias, weight}
ins_embed_head.decoder.res2.fuse_conv.depthwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res2.fuse_conv.depthwise.weight
ins_embed_head.decoder.res2.fuse_conv.pointwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res2.fuse_conv.pointwise.weight
ins_embed_head.decoder.res2.project_conv.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res2.project_conv.weight
ins_embed_head.decoder.res3.fuse_conv.depthwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res3.fuse_conv.depthwise.weight
ins_embed_head.decoder.res3.fuse_conv.pointwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res3.fuse_conv.pointwise.weight
ins_embed_head.decoder.res3.project_conv.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res3.project_conv.weight
ins_embed_head.decoder.res5.project_conv.convs.0.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res5.project_conv.convs.0.weight
ins_embed_head.decoder.res5.project_conv.convs.1.depthwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res5.project_conv.convs.1.depthwise.weight
ins_embed_head.decoder.res5.project_conv.convs.1.pointwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res5.project_conv.convs.1.pointwise.weight
ins_embed_head.decoder.res5.project_conv.convs.2.depthwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res5.project_conv.convs.2.depthwise.weight
ins_embed_head.decoder.res5.project_conv.convs.2.pointwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res5.project_conv.convs.2.pointwise.weight
ins_embed_head.decoder.res5.project_conv.convs.3.depthwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res5.project_conv.convs.3.depthwise.weight
ins_embed_head.decoder.res5.project_conv.convs.3.pointwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res5.project_conv.convs.3.pointwise.weight
ins_embed_head.decoder.res5.project_conv.convs.4.1.{bias, weight}
ins_embed_head.decoder.res5.project_conv.project.norm.{bias, running_mean, running_var, weight}
ins_embed_head.decoder.res5.project_conv.project.weight
ins_embed_head.offset_head.depthwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.offset_head.depthwise.weight
ins_embed_head.offset_head.pointwise.norm.{bias, running_mean, running_var, weight}
ins_embed_head.offset_head.pointwise.weight
ins_embed_head.offset_predictor.{bias, weight}
sem_seg_head.decoder.res2.fuse_conv.depthwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res2.fuse_conv.depthwise.weight
sem_seg_head.decoder.res2.fuse_conv.pointwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res2.fuse_conv.pointwise.weight
sem_seg_head.decoder.res2.project_conv.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res2.project_conv.weight
sem_seg_head.decoder.res3.fuse_conv.depthwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res3.fuse_conv.depthwise.weight
sem_seg_head.decoder.res3.fuse_conv.pointwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res3.fuse_conv.pointwise.weight
sem_seg_head.decoder.res3.project_conv.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res3.project_conv.weight
sem_seg_head.decoder.res5.project_conv.convs.0.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res5.project_conv.convs.0.weight
sem_seg_head.decoder.res5.project_conv.convs.1.depthwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res5.project_conv.convs.1.depthwise.weight
sem_seg_head.decoder.res5.project_conv.convs.1.pointwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res5.project_conv.convs.1.pointwise.weight
sem_seg_head.decoder.res5.project_conv.convs.2.depthwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res5.project_conv.convs.2.depthwise.weight
sem_seg_head.decoder.res5.project_conv.convs.2.pointwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res5.project_conv.convs.2.pointwise.weight
sem_seg_head.decoder.res5.project_conv.convs.3.depthwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res5.project_conv.convs.3.depthwise.weight
sem_seg_head.decoder.res5.project_conv.convs.3.pointwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res5.project_conv.convs.3.pointwise.weight
sem_seg_head.decoder.res5.project_conv.convs.4.1.{bias, weight}
sem_seg_head.decoder.res5.project_conv.project.norm.{bias, running_mean, running_var, weight}
sem_seg_head.decoder.res5.project_conv.project.weight
sem_seg_head.head.depthwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.head.depthwise.weight
sem_seg_head.head.pointwise.norm.{bias, running_mean, running_var, weight}
sem_seg_head.head.pointwise.weight
sem_seg_head.predictor.{bias, weight}
WARNING [12/11 19:52:10 fvcore.common.checkpoint]: The checkpoint state_dict contains keys that are not used by the model:
  res2.0.conv1.norm.num_batches_tracked
  res2.0.conv2.norm.num_batches_tracked
  res2.0.conv3.norm.num_batches_tracked
  res2.0.shortcut.norm.num_batches_tracked
  res2.1.conv1.norm.num_batches_tracked
  res2.1.conv2.norm.num_batches_tracked
  res2.1.conv3.norm.num_batches_tracked
  res2.2.conv1.norm.num_batches_tracked
  res2.2.conv2.norm.num_batches_tracked
  res2.2.conv3.norm.num_batches_tracked
  res3.0.conv1.norm.num_batches_tracked
  res3.0.conv2.norm.num_batches_tracked
  res3.0.conv3.norm.num_batches_tracked
  res3.0.shortcut.norm.num_batches_tracked
  res3.1.conv1.norm.num_batches_tracked
  res3.1.conv2.norm.num_batches_tracked
  res3.1.conv3.norm.num_batches_tracked
  res3.2.conv1.norm.num_batches_tracked
  res3.2.conv2.norm.num_batches_tracked
  res3.2.conv3.norm.num_batches_tracked
  res3.3.conv1.norm.num_batches_tracked
  res3.3.conv2.norm.num_batches_tracked
  res3.3.conv3.norm.num_batches_tracked
  res4.0.conv1.norm.num_batches_tracked
  res4.0.conv2.norm.num_batches_tracked
  res4.0.conv3.norm.num_batches_tracked
  res4.0.shortcut.norm.num_batches_tracked
  res4.1.conv1.norm.num_batches_tracked
  res4.1.conv2.norm.num_batches_tracked
  res4.1.conv3.norm.num_batches_tracked
  res4.10.conv1.norm.num_batches_tracked
  res4.10.conv2.norm.num_batches_tracked
  res4.10.conv3.norm.num_batches_tracked
  res4.11.conv1.norm.num_batches_tracked
  res4.11.conv2.norm.num_batches_tracked
  res4.11.conv3.norm.num_batches_tracked
  res4.12.conv1.norm.num_batches_tracked
  res4.12.conv2.norm.num_batches_tracked
  res4.12.conv3.norm.num_batches_tracked
  res4.13.conv1.norm.num_batches_tracked
  res4.13.conv2.norm.num_batches_tracked
  res4.13.conv3.norm.num_batches_tracked
  res4.14.conv1.norm.num_batches_tracked
  res4.14.conv2.norm.num_batches_tracked
  res4.14.conv3.norm.num_batches_tracked
  res4.15.conv1.norm.num_batches_tracked
  res4.15.conv2.norm.num_batches_tracked
  res4.15.conv3.norm.num_batches_tracked
  res4.16.conv1.norm.num_batches_tracked
  res4.16.conv2.norm.num_batches_tracked
  res4.16.conv3.norm.num_batches_tracked
  res4.17.conv1.norm.num_batches_tracked
  res4.17.conv2.norm.num_batches_tracked
  res4.17.conv3.norm.num_batches_tracked
  res4.18.conv1.norm.num_batches_tracked
  res4.18.conv2.norm.num_batches_tracked
  res4.18.conv3.norm.num_batches_tracked
  res4.19.conv1.norm.num_batches_tracked
  res4.19.conv2.norm.num_batches_tracked
  res4.19.conv3.norm.num_batches_tracked
  res4.2.conv1.norm.num_batches_tracked
  res4.2.conv2.norm.num_batches_tracked
  res4.2.conv3.norm.num_batches_tracked
  res4.20.conv1.norm.num_batches_tracked
  res4.20.conv2.norm.num_batches_tracked
  res4.20.conv3.norm.num_batches_tracked
  res4.21.conv1.norm.num_batches_tracked
  res4.21.conv2.norm.num_batches_tracked
  res4.21.conv3.norm.num_batches_tracked
  res4.22.conv1.norm.num_batches_tracked
  res4.22.conv2.norm.num_batches_tracked
  res4.22.conv3.norm.num_batches_tracked
  res4.3.conv1.norm.num_batches_tracked
  res4.3.conv2.norm.num_batches_tracked
  res4.3.conv3.norm.num_batches_tracked
  res4.4.conv1.norm.num_batches_tracked
  res4.4.conv2.norm.num_batches_tracked
  res4.4.conv3.norm.num_batches_tracked
  res4.5.conv1.norm.num_batches_tracked
  res4.5.conv2.norm.num_batches_tracked
  res4.5.conv3.norm.num_batches_tracked
  res4.6.conv1.norm.num_batches_tracked
  res4.6.conv2.norm.num_batches_tracked
  res4.6.conv3.norm.num_batches_tracked
  res4.7.conv1.norm.num_batches_tracked
  res4.7.conv2.norm.num_batches_tracked
  res4.7.conv3.norm.num_batches_tracked
  res4.8.conv1.norm.num_batches_tracked
  res4.8.conv2.norm.num_batches_tracked
  res4.8.conv3.norm.num_batches_tracked
  res4.9.conv1.norm.num_batches_tracked
  res4.9.conv2.norm.num_batches_tracked
  res4.9.conv3.norm.num_batches_tracked
  stem.conv1.norm.num_batches_tracked
  stem.conv2.norm.num_batches_tracked
  stem.conv3.norm.num_batches_tracked
  stem.fc.{bias, weight}
[12/11 19:52:10 d2.engine.train_loop]: Starting training from iteration 0
/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
[12/11 19:52:18 d2.utils.events]:  eta: 0:58:50  iter: 19  total_loss: 6.102  loss_sem_seg: 3.738  loss_center: 0.6604  loss_offset: 1.53  time: 0.3553  data_time: 0.0806  lr: 4.9867e-05  max_mem: 7418M
[12/11 19:52:25 d2.utils.events]:  eta: 0:58:50  iter: 39  total_loss: 6.018  loss_sem_seg: 3.603  loss_center: 0.7683  loss_offset: 1.477  time: 0.3550  data_time: 0.0286  lr: 9.9552e-05  max_mem: 7418M
[12/11 19:52:33 d2.utils.events]:  eta: 0:58:43  iter: 59  total_loss: 5.658  loss_sem_seg: 3.444  loss_center: 0.5245  loss_offset: 1.588  time: 0.3545  data_time: 0.0290  lr: 0.00014906  max_mem: 7418M
[12/11 19:52:40 d2.utils.events]:  eta: 0:58:32  iter: 79  total_loss: 5.539  loss_sem_seg: 3.372  loss_center: 0.7364  loss_offset: 1.395  time: 0.3545  data_time: 0.0289  lr: 0.00019838  max_mem: 7418M
[12/11 19:52:47 d2.utils.events]:  eta: 0:58:22  iter: 99  total_loss: 5.047  loss_sem_seg: 2.773  loss_center: 0.7953  loss_offset: 1.641  time: 0.3545  data_time: 0.0291  lr: 0.00024753  max_mem: 7418M
[12/11 19:52:54 d2.utils.events]:  eta: 0:58:20  iter: 119  total_loss: 5.376  loss_sem_seg: 3.12  loss_center: 0.6325  loss_offset: 1.57  time: 0.3547  data_time: 0.0288  lr: 0.00029649  max_mem: 7418M
[12/11 19:53:01 d2.utils.events]:  eta: 0:58:08  iter: 139  total_loss: 5.01  loss_sem_seg: 2.889  loss_center: 0.7028  loss_offset: 1.467  time: 0.3543  data_time: 0.0264  lr: 0.00034528  max_mem: 7418M
[12/11 19:53:08 d2.utils.events]:  eta: 0:58:03  iter: 159  total_loss: 5.463  loss_sem_seg: 2.996  loss_center: 0.77  loss_offset: 1.555  time: 0.3549  data_time: 0.0306  lr: 0.00039388  max_mem: 7418M
[12/11 19:53:15 d2.utils.events]:  eta: 0:57:41  iter: 179  total_loss: 4.746  loss_sem_seg: 2.44  loss_center: 0.6194  loss_offset: 1.519  time: 0.3546  data_time: 0.0272  lr: 0.0004423  max_mem: 7418M
[12/11 19:53:22 d2.utils.events]:  eta: 0:57:34  iter: 199  total_loss: 4.865  loss_sem_seg: 2.765  loss_center: 0.584  loss_offset: 1.644  time: 0.3547  data_time: 0.0296  lr: 0.00049055  max_mem: 7418M
[12/11 19:53:29 d2.utils.events]:  eta: 0:57:26  iter: 219  total_loss: 4.258  loss_sem_seg: 2.273  loss_center: 0.6129  loss_offset: 1.435  time: 0.3545  data_time: 0.0270  lr: 0.00053861  max_mem: 7418M
[12/11 19:53:36 d2.utils.events]:  eta: 0:57:14  iter: 239  total_loss: 4.409  loss_sem_seg: 2.283  loss_center: 0.8049  loss_offset: 1.356  time: 0.3541  data_time: 0.0271  lr: 0.00058649  max_mem: 7418M
[12/11 19:53:43 d2.utils.events]:  eta: 0:57:12  iter: 259  total_loss: 4.831  loss_sem_seg: 2.461  loss_center: 0.6274  loss_offset: 1.609  time: 0.3543  data_time: 0.0293  lr: 0.0006342  max_mem: 7418M
[12/11 19:53:51 d2.utils.events]:  eta: 0:57:01  iter: 279  total_loss: 4.608  loss_sem_seg: 2.509  loss_center: 0.7283  loss_offset: 1.268  time: 0.3543  data_time: 0.0286  lr: 0.00068172  max_mem: 7418M
[12/11 19:53:58 d2.utils.events]:  eta: 0:56:51  iter: 299  total_loss: 4.067  loss_sem_seg: 2.026  loss_center: 0.5803  loss_offset: 1.421  time: 0.3540  data_time: 0.0270  lr: 0.00072906  max_mem: 7418M
[12/11 19:54:05 d2.utils.events]:  eta: 0:56:42  iter: 319  total_loss: 4.215  loss_sem_seg: 2.193  loss_center: 0.6523  loss_offset: 1.288  time: 0.3540  data_time: 0.0291  lr: 0.00077622  max_mem: 7418M
[12/11 19:54:12 d2.utils.events]:  eta: 0:56:37  iter: 339  total_loss: 4.126  loss_sem_seg: 2.231  loss_center: 0.4643  loss_offset: 1.433  time: 0.3540  data_time: 0.0286  lr: 0.0008232  max_mem: 7418M
[12/11 19:54:19 d2.utils.events]:  eta: 0:56:30  iter: 359  total_loss: 3.873  loss_sem_seg: 2.153  loss_center: 0.5965  loss_offset: 1.074  time: 0.3538  data_time: 0.0269  lr: 0.00087  max_mem: 7418M
[12/11 19:54:26 d2.utils.events]:  eta: 0:56:25  iter: 379  total_loss: 4.173  loss_sem_seg: 2.159  loss_center: 0.5779  loss_offset: 1.401  time: 0.3539  data_time: 0.0295  lr: 0.00091662  max_mem: 7418M
[12/11 19:54:33 d2.utils.events]:  eta: 0:56:18  iter: 399  total_loss: 3.729  loss_sem_seg: 2.029  loss_center: 0.5097  loss_offset: 1.041  time: 0.3539  data_time: 0.0279  lr: 0.00096306  max_mem: 7418M
[12/11 19:54:40 d2.utils.events]:  eta: 0:56:14  iter: 419  total_loss: 4.005  loss_sem_seg: 2.192  loss_center: 0.6065  loss_offset: 1.13  time: 0.3541  data_time: 0.0307  lr: 0.0010093  max_mem: 7418M
[12/11 19:54:47 d2.utils.events]:  eta: 0:56:07  iter: 439  total_loss: 3.754  loss_sem_seg: 2.038  loss_center: 0.5859  loss_offset: 0.9354  time: 0.3540  data_time: 0.0283  lr: 0.0010554  max_mem: 7418M
[12/11 19:54:54 d2.utils.events]:  eta: 0:55:58  iter: 459  total_loss: 3.793  loss_sem_seg: 1.915  loss_center: 0.4465  loss_offset: 1.203  time: 0.3539  data_time: 0.0272  lr: 0.0011013  max_mem: 7418M
[12/11 19:55:01 d2.utils.events]:  eta: 0:55:52  iter: 479  total_loss: 3.548  loss_sem_seg: 1.886  loss_center: 0.5825  loss_offset: 1.033  time: 0.3539  data_time: 0.0274  lr: 0.001147  max_mem: 7418M
[12/11 19:55:08 d2.utils.events]:  eta: 0:55:47  iter: 499  total_loss: 3.842  loss_sem_seg: 2.207  loss_center: 0.5439  loss_offset: 1.052  time: 0.3539  data_time: 0.0313  lr: 0.0011925  max_mem: 7418M
[12/11 19:55:16 d2.utils.events]:  eta: 0:55:40  iter: 519  total_loss: 3.404  loss_sem_seg: 1.911  loss_center: 0.7136  loss_offset: 0.9307  time: 0.3539  data_time: 0.0291  lr: 0.0012379  max_mem: 7418M
[12/11 19:55:23 d2.utils.events]:  eta: 0:55:33  iter: 539  total_loss: 3.598  loss_sem_seg: 1.881  loss_center: 0.528  loss_offset: 0.9659  time: 0.3539  data_time: 0.0283  lr: 0.001283  max_mem: 7418M
[12/11 19:55:30 d2.utils.events]:  eta: 0:55:26  iter: 559  total_loss: 3.794  loss_sem_seg: 1.964  loss_center: 0.5902  loss_offset: 0.9397  time: 0.3539  data_time: 0.0276  lr: 0.001328  max_mem: 7418M
[12/11 19:55:37 d2.utils.events]:  eta: 0:55:22  iter: 579  total_loss: 3.822  loss_sem_seg: 2.012  loss_center: 0.5719  loss_offset: 0.9514  time: 0.3539  data_time: 0.0291  lr: 0.0013728  max_mem: 7418M
[12/11 19:55:44 d2.utils.events]:  eta: 0:55:15  iter: 599  total_loss: 3.274  loss_sem_seg: 1.652  loss_center: 0.681  loss_offset: 0.9441  time: 0.3540  data_time: 0.0294  lr: 0.0014175  max_mem: 7418M
[12/11 19:55:51 d2.utils.events]:  eta: 0:55:08  iter: 619  total_loss: 3.37  loss_sem_seg: 1.873  loss_center: 0.5611  loss_offset: 0.7574  time: 0.3539  data_time: 0.0275  lr: 0.0014619  max_mem: 7418M
[12/11 19:55:58 d2.utils.events]:  eta: 0:55:01  iter: 639  total_loss: 3.357  loss_sem_seg: 1.899  loss_center: 0.6014  loss_offset: 0.8621  time: 0.3539  data_time: 0.0283  lr: 0.0015062  max_mem: 7418M
[12/11 19:56:05 d2.utils.events]:  eta: 0:54:54  iter: 659  total_loss: 3.617  loss_sem_seg: 2.009  loss_center: 0.6833  loss_offset: 0.9608  time: 0.3538  data_time: 0.0272  lr: 0.0015503  max_mem: 7418M
[12/11 19:56:12 d2.utils.events]:  eta: 0:54:47  iter: 679  total_loss: 3.611  loss_sem_seg: 2.09  loss_center: 0.5811  loss_offset: 0.8679  time: 0.3539  data_time: 0.0300  lr: 0.0015942  max_mem: 7418M
[12/11 19:56:19 d2.utils.events]:  eta: 0:54:39  iter: 699  total_loss: 3.348  loss_sem_seg: 1.801  loss_center: 0.5368  loss_offset: 0.9971  time: 0.3539  data_time: 0.0296  lr: 0.0016379  max_mem: 7418M
[12/11 19:56:26 d2.utils.events]:  eta: 0:54:32  iter: 719  total_loss: 3.422  loss_sem_seg: 1.822  loss_center: 0.5761  loss_offset: 0.8663  time: 0.3539  data_time: 0.0275  lr: 0.0016814  max_mem: 7418M
[12/11 19:56:34 d2.utils.events]:  eta: 0:54:25  iter: 739  total_loss: 3.242  loss_sem_seg: 1.738  loss_center: 0.6016  loss_offset: 0.8327  time: 0.3540  data_time: 0.0298  lr: 0.0017248  max_mem: 7418M
[12/11 19:56:41 d2.utils.events]:  eta: 0:54:18  iter: 759  total_loss: 3.748  loss_sem_seg: 2.019  loss_center: 0.6256  loss_offset: 1.05  time: 0.3539  data_time: 0.0274  lr: 0.0017679  max_mem: 7418M
[12/11 19:56:48 d2.utils.events]:  eta: 0:54:11  iter: 779  total_loss: 3.175  loss_sem_seg: 1.845  loss_center: 0.5762  loss_offset: 0.8616  time: 0.3538  data_time: 0.0282  lr: 0.0018109  max_mem: 7418M
[12/11 19:56:55 d2.utils.events]:  eta: 0:54:05  iter: 799  total_loss: 3.547  loss_sem_seg: 2.069  loss_center: 0.6401  loss_offset: 0.92  time: 0.3539  data_time: 0.0293  lr: 0.0018537  max_mem: 7418M
[12/11 19:57:02 d2.utils.events]:  eta: 0:54:00  iter: 819  total_loss: 3.106  loss_sem_seg: 1.64  loss_center: 0.693  loss_offset: 0.9062  time: 0.3538  data_time: 0.0282  lr: 0.0018964  max_mem: 7418M
[12/11 19:57:09 d2.utils.events]:  eta: 0:53:53  iter: 839  total_loss: 3.14  loss_sem_seg: 1.584  loss_center: 0.6955  loss_offset: 0.8169  time: 0.3538  data_time: 0.0279  lr: 0.0019388  max_mem: 7418M
[12/11 19:57:16 d2.utils.events]:  eta: 0:53:46  iter: 859  total_loss: 3.202  loss_sem_seg: 1.871  loss_center: 0.5411  loss_offset: 0.7434  time: 0.3538  data_time: 0.0296  lr: 0.0019811  max_mem: 7418M
[12/11 19:57:23 d2.utils.events]:  eta: 0:53:39  iter: 879  total_loss: 3.226  loss_sem_seg: 1.585  loss_center: 0.6197  loss_offset: 0.8357  time: 0.3538  data_time: 0.0284  lr: 0.0020231  max_mem: 7418M
[12/11 19:57:30 d2.utils.events]:  eta: 0:53:32  iter: 899  total_loss: 3.598  loss_sem_seg: 2.124  loss_center: 0.6531  loss_offset: 0.8653  time: 0.3538  data_time: 0.0300  lr: 0.002065  max_mem: 7418M
[12/11 19:57:37 d2.utils.events]:  eta: 0:53:24  iter: 919  total_loss: 3.108  loss_sem_seg: 1.61  loss_center: 0.6693  loss_offset: 0.7656  time: 0.3538  data_time: 0.0278  lr: 0.0021068  max_mem: 7418M
[12/11 19:57:44 d2.utils.events]:  eta: 0:53:18  iter: 939  total_loss: 3.169  loss_sem_seg: 1.485  loss_center: 0.6912  loss_offset: 0.6805  time: 0.3538  data_time: 0.0280  lr: 0.0021483  max_mem: 7418M
[12/11 19:57:51 d2.utils.events]:  eta: 0:53:11  iter: 959  total_loss: 3.143  loss_sem_seg: 1.528  loss_center: 0.519  loss_offset: 0.7916  time: 0.3538  data_time: 0.0286  lr: 0.0021896  max_mem: 7418M
[12/11 19:57:58 d2.utils.events]:  eta: 0:53:04  iter: 979  total_loss: 3.422  loss_sem_seg: 1.848  loss_center: 0.6774  loss_offset: 0.7511  time: 0.3537  data_time: 0.0279  lr: 0.0022308  max_mem: 7418M
[12/11 19:58:05 d2.utils.events]:  eta: 0:52:57  iter: 999  total_loss: 3.087  loss_sem_seg: 1.661  loss_center: 0.7242  loss_offset: 0.7593  time: 0.3537  data_time: 0.0291  lr: 0.0022718  max_mem: 7418M
[12/11 19:58:13 d2.utils.events]:  eta: 0:52:50  iter: 1019  total_loss: 2.789  loss_sem_seg: 1.604  loss_center: 0.5392  loss_offset: 0.7458  time: 0.3537  data_time: 0.0290  lr: 0.0022695  max_mem: 7418M
[12/11 19:58:20 d2.utils.events]:  eta: 0:52:43  iter: 1039  total_loss: 3.091  loss_sem_seg: 1.644  loss_center: 0.6569  loss_offset: 0.7806  time: 0.3537  data_time: 0.0278  lr: 0.002265  max_mem: 7418M
[12/11 19:58:27 d2.utils.events]:  eta: 0:52:36  iter: 1059  total_loss: 2.95  loss_sem_seg: 1.576  loss_center: 0.5552  loss_offset: 0.742  time: 0.3537  data_time: 0.0286  lr: 0.0022604  max_mem: 7418M
[12/11 19:58:34 d2.utils.events]:  eta: 0:52:29  iter: 1079  total_loss: 3.207  loss_sem_seg: 1.674  loss_center: 0.6565  loss_offset: 0.7736  time: 0.3537  data_time: 0.0287  lr: 0.0022559  max_mem: 7418M
[12/11 19:58:41 d2.utils.events]:  eta: 0:52:23  iter: 1099  total_loss: 3.053  loss_sem_seg: 1.624  loss_center: 0.5348  loss_offset: 0.8588  time: 0.3537  data_time: 0.0270  lr: 0.0022513  max_mem: 7418M
[12/11 19:58:48 d2.utils.events]:  eta: 0:52:15  iter: 1119  total_loss: 3.207  loss_sem_seg: 1.643  loss_center: 0.6977  loss_offset: 0.7194  time: 0.3537  data_time: 0.0292  lr: 0.0022468  max_mem: 7418M
[12/11 19:58:55 d2.utils.events]:  eta: 0:52:09  iter: 1139  total_loss: 3.08  loss_sem_seg: 1.74  loss_center: 0.5753  loss_offset: 0.7702  time: 0.3537  data_time: 0.0288  lr: 0.0022422  max_mem: 7418M
[12/11 19:59:02 d2.utils.events]:  eta: 0:52:02  iter: 1159  total_loss: 3.308  loss_sem_seg: 1.643  loss_center: 0.6225  loss_offset: 0.8489  time: 0.3537  data_time: 0.0290  lr: 0.0022376  max_mem: 7418M
[12/11 19:59:09 d2.utils.events]:  eta: 0:51:55  iter: 1179  total_loss: 3.197  loss_sem_seg: 1.722  loss_center: 0.5591  loss_offset: 0.8871  time: 0.3537  data_time: 0.0280  lr: 0.0022331  max_mem: 7418M
[12/11 19:59:16 d2.utils.events]:  eta: 0:51:48  iter: 1199  total_loss: 2.985  loss_sem_seg: 1.538  loss_center: 0.6661  loss_offset: 0.7719  time: 0.3537  data_time: 0.0293  lr: 0.0022285  max_mem: 7418M
[12/11 19:59:23 d2.utils.events]:  eta: 0:51:42  iter: 1219  total_loss: 3.231  loss_sem_seg: 1.926  loss_center: 0.5248  loss_offset: 0.7097  time: 0.3537  data_time: 0.0266  lr: 0.002224  max_mem: 7418M
[12/11 19:59:30 d2.utils.events]:  eta: 0:51:35  iter: 1239  total_loss: 2.895  loss_sem_seg: 1.587  loss_center: 0.5343  loss_offset: 0.7821  time: 0.3537  data_time: 0.0290  lr: 0.0022194  max_mem: 7418M
[12/11 19:59:37 d2.utils.events]:  eta: 0:51:28  iter: 1259  total_loss: 2.817  loss_sem_seg: 1.4  loss_center: 0.6288  loss_offset: 0.7409  time: 0.3537  data_time: 0.0288  lr: 0.0022149  max_mem: 7418M
[12/11 19:59:45 d2.utils.events]:  eta: 0:51:21  iter: 1279  total_loss: 2.917  loss_sem_seg: 1.363  loss_center: 0.7629  loss_offset: 0.7409  time: 0.3537  data_time: 0.0285  lr: 0.0022103  max_mem: 7418M
[12/11 19:59:52 d2.utils.events]:  eta: 0:51:14  iter: 1299  total_loss: 2.77  loss_sem_seg: 1.546  loss_center: 0.4756  loss_offset: 0.7102  time: 0.3537  data_time: 0.0285  lr: 0.0022057  max_mem: 7418M
[12/11 19:59:59 d2.utils.events]:  eta: 0:51:07  iter: 1319  total_loss: 2.877  loss_sem_seg: 1.453  loss_center: 0.6014  loss_offset: 0.7553  time: 0.3536  data_time: 0.0264  lr: 0.0022012  max_mem: 7418M
[12/11 20:00:06 d2.utils.events]:  eta: 0:51:00  iter: 1339  total_loss: 2.653  loss_sem_seg: 1.444  loss_center: 0.5587  loss_offset: 0.6689  time: 0.3536  data_time: 0.0289  lr: 0.0021966  max_mem: 7418M
[12/11 20:00:13 d2.utils.events]:  eta: 0:50:54  iter: 1359  total_loss: 3.138  loss_sem_seg: 1.533  loss_center: 0.533  loss_offset: 0.7855  time: 0.3537  data_time: 0.0292  lr: 0.002192  max_mem: 7418M
[12/11 20:00:20 d2.utils.events]:  eta: 0:50:46  iter: 1379  total_loss: 3.052  loss_sem_seg: 1.587  loss_center: 0.6022  loss_offset: 0.8343  time: 0.3537  data_time: 0.0277  lr: 0.0021875  max_mem: 7418M
[12/11 20:00:27 d2.utils.events]:  eta: 0:50:39  iter: 1399  total_loss: 3.208  loss_sem_seg: 1.846  loss_center: 0.7145  loss_offset: 0.743  time: 0.3537  data_time: 0.0311  lr: 0.0021829  max_mem: 7418M
[12/11 20:00:34 d2.utils.events]:  eta: 0:50:31  iter: 1419  total_loss: 2.815  loss_sem_seg: 1.494  loss_center: 0.5369  loss_offset: 0.7205  time: 0.3537  data_time: 0.0280  lr: 0.0021783  max_mem: 7418M
[12/11 20:00:41 d2.utils.events]:  eta: 0:50:24  iter: 1439  total_loss: 2.945  loss_sem_seg: 1.418  loss_center: 0.6445  loss_offset: 0.7795  time: 0.3537  data_time: 0.0291  lr: 0.0021738  max_mem: 7418M
[12/11 20:00:48 d2.utils.events]:  eta: 0:50:17  iter: 1459  total_loss: 3.232  loss_sem_seg: 1.682  loss_center: 0.5879  loss_offset: 0.8445  time: 0.3537  data_time: 0.0305  lr: 0.0021692  max_mem: 7418M
[12/11 20:00:55 d2.utils.events]:  eta: 0:50:10  iter: 1479  total_loss: 3.101  loss_sem_seg: 1.635  loss_center: 0.6671  loss_offset: 0.7816  time: 0.3537  data_time: 0.0274  lr: 0.0021646  max_mem: 7418M
[12/11 20:01:02 d2.utils.events]:  eta: 0:50:03  iter: 1499  total_loss: 2.856  loss_sem_seg: 1.555  loss_center: 0.5637  loss_offset: 0.8007  time: 0.3537  data_time: 0.0284  lr: 0.00216  max_mem: 7418M
[12/11 20:01:10 d2.utils.events]:  eta: 0:49:56  iter: 1519  total_loss: 2.776  loss_sem_seg: 1.648  loss_center: 0.6277  loss_offset: 0.7575  time: 0.3537  data_time: 0.0292  lr: 0.0021555  max_mem: 7418M
[12/11 20:01:17 d2.utils.events]:  eta: 0:49:49  iter: 1539  total_loss: 3.019  loss_sem_seg: 1.609  loss_center: 0.7162  loss_offset: 0.6777  time: 0.3537  data_time: 0.0289  lr: 0.0021509  max_mem: 7418M
[12/11 20:01:24 d2.utils.events]:  eta: 0:49:42  iter: 1559  total_loss: 3.219  loss_sem_seg: 1.631  loss_center: 0.5997  loss_offset: 0.675  time: 0.3537  data_time: 0.0279  lr: 0.0021463  max_mem: 7418M
[12/11 20:01:31 d2.utils.events]:  eta: 0:49:34  iter: 1579  total_loss: 3.096  loss_sem_seg: 1.577  loss_center: 0.5824  loss_offset: 0.6718  time: 0.3537  data_time: 0.0291  lr: 0.0021417  max_mem: 7418M
[12/11 20:01:38 d2.utils.events]:  eta: 0:49:27  iter: 1599  total_loss: 2.862  loss_sem_seg: 1.661  loss_center: 0.5494  loss_offset: 0.8596  time: 0.3537  data_time: 0.0291  lr: 0.0021372  max_mem: 7418M
[12/11 20:01:45 d2.utils.events]:  eta: 0:49:21  iter: 1619  total_loss: 2.822  loss_sem_seg: 1.647  loss_center: 0.5549  loss_offset: 0.7132  time: 0.3537  data_time: 0.0291  lr: 0.0021326  max_mem: 7418M
[12/11 20:01:52 d2.utils.events]:  eta: 0:49:14  iter: 1639  total_loss: 2.815  loss_sem_seg: 1.551  loss_center: 0.6534  loss_offset: 0.6876  time: 0.3537  data_time: 0.0263  lr: 0.002128  max_mem: 7418M
[12/11 20:01:59 d2.utils.events]:  eta: 0:49:07  iter: 1659  total_loss: 2.8  loss_sem_seg: 1.446  loss_center: 0.596  loss_offset: 0.725  time: 0.3537  data_time: 0.0286  lr: 0.0021234  max_mem: 7418M
[12/11 20:02:06 d2.utils.events]:  eta: 0:49:00  iter: 1679  total_loss: 2.705  loss_sem_seg: 1.437  loss_center: 0.6656  loss_offset: 0.6293  time: 0.3537  data_time: 0.0288  lr: 0.0021188  max_mem: 7418M
[12/11 20:02:13 d2.utils.events]:  eta: 0:48:53  iter: 1699  total_loss: 2.887  loss_sem_seg: 1.488  loss_center: 0.6404  loss_offset: 0.7288  time: 0.3537  data_time: 0.0286  lr: 0.0021143  max_mem: 7418M
[12/11 20:02:20 d2.utils.events]:  eta: 0:48:46  iter: 1719  total_loss: 2.822  loss_sem_seg: 1.416  loss_center: 0.5264  loss_offset: 0.7664  time: 0.3537  data_time: 0.0292  lr: 0.0021097  max_mem: 7418M
[12/11 20:02:28 d2.utils.events]:  eta: 0:48:39  iter: 1739  total_loss: 2.72  loss_sem_seg: 1.467  loss_center: 0.5835  loss_offset: 0.732  time: 0.3537  data_time: 0.0288  lr: 0.0021051  max_mem: 7418M
[12/11 20:02:35 d2.utils.events]:  eta: 0:48:32  iter: 1759  total_loss: 2.826  loss_sem_seg: 1.306  loss_center: 0.6441  loss_offset: 0.6945  time: 0.3537  data_time: 0.0275  lr: 0.0021005  max_mem: 7418M
[12/11 20:02:42 d2.utils.events]:  eta: 0:48:24  iter: 1779  total_loss: 2.873  loss_sem_seg: 1.322  loss_center: 0.8826  loss_offset: 0.7212  time: 0.3537  data_time: 0.0285  lr: 0.0020959  max_mem: 7418M
[12/11 20:02:49 d2.utils.events]:  eta: 0:48:17  iter: 1799  total_loss: 2.894  loss_sem_seg: 1.491  loss_center: 0.5229  loss_offset: 0.8391  time: 0.3537  data_time: 0.0282  lr: 0.0020913  max_mem: 7418M
[12/11 20:02:56 d2.utils.events]:  eta: 0:48:09  iter: 1819  total_loss: 2.665  loss_sem_seg: 1.329  loss_center: 0.5114  loss_offset: 0.6514  time: 0.3536  data_time: 0.0269  lr: 0.0020867  max_mem: 7418M
[12/11 20:03:03 d2.utils.events]:  eta: 0:48:02  iter: 1839  total_loss: 2.666  loss_sem_seg: 1.25  loss_center: 0.6424  loss_offset: 0.747  time: 0.3536  data_time: 0.0268  lr: 0.0020821  max_mem: 7418M
[12/11 20:03:10 d2.utils.events]:  eta: 0:47:55  iter: 1859  total_loss: 2.801  loss_sem_seg: 1.334  loss_center: 0.5407  loss_offset: 0.8382  time: 0.3536  data_time: 0.0281  lr: 0.0020775  max_mem: 7418M
[12/11 20:03:17 d2.utils.events]:  eta: 0:47:48  iter: 1879  total_loss: 2.952  loss_sem_seg: 1.43  loss_center: 0.5553  loss_offset: 0.749  time: 0.3536  data_time: 0.0271  lr: 0.0020729  max_mem: 7418M
[12/11 20:03:24 d2.utils.events]:  eta: 0:47:41  iter: 1899  total_loss: 2.738  loss_sem_seg: 1.418  loss_center: 0.5572  loss_offset: 0.6933  time: 0.3536  data_time: 0.0289  lr: 0.0020684  max_mem: 7418M
[12/11 20:03:31 d2.utils.events]:  eta: 0:47:34  iter: 1919  total_loss: 2.9  loss_sem_seg: 1.429  loss_center: 0.5132  loss_offset: 0.767  time: 0.3536  data_time: 0.0283  lr: 0.0020638  max_mem: 7418M
[12/11 20:03:38 d2.utils.events]:  eta: 0:47:27  iter: 1939  total_loss: 2.893  loss_sem_seg: 1.493  loss_center: 0.7438  loss_offset: 0.6343  time: 0.3536  data_time: 0.0297  lr: 0.0020592  max_mem: 7418M
[12/11 20:03:45 d2.utils.events]:  eta: 0:47:20  iter: 1959  total_loss: 2.663  loss_sem_seg: 1.349  loss_center: 0.5747  loss_offset: 0.6765  time: 0.3536  data_time: 0.0287  lr: 0.0020546  max_mem: 7418M
[12/11 20:03:52 d2.utils.events]:  eta: 0:47:12  iter: 1979  total_loss: 2.446  loss_sem_seg: 1.146  loss_center: 0.5413  loss_offset: 0.6948  time: 0.3535  data_time: 0.0273  lr: 0.00205  max_mem: 7418M
[12/11 20:03:59 d2.utils.events]:  eta: 0:47:04  iter: 1999  total_loss: 2.84  loss_sem_seg: 1.241  loss_center: 0.5737  loss_offset: 0.7838  time: 0.3535  data_time: 0.0276  lr: 0.0020454  max_mem: 7418M
[12/11 20:04:06 d2.utils.events]:  eta: 0:46:57  iter: 2019  total_loss: 2.882  loss_sem_seg: 1.5  loss_center: 0.6175  loss_offset: 0.7508  time: 0.3535  data_time: 0.0281  lr: 0.0020408  max_mem: 7418M
[12/11 20:04:13 d2.utils.events]:  eta: 0:46:50  iter: 2039  total_loss: 2.759  loss_sem_seg: 1.345  loss_center: 0.5218  loss_offset: 0.8058  time: 0.3535  data_time: 0.0279  lr: 0.0020362  max_mem: 7418M
[12/11 20:04:21 d2.utils.events]:  eta: 0:46:43  iter: 2059  total_loss: 2.786  loss_sem_seg: 1.272  loss_center: 0.5605  loss_offset: 0.7953  time: 0.3535  data_time: 0.0302  lr: 0.0020316  max_mem: 7418M
[12/11 20:04:28 d2.utils.events]:  eta: 0:46:35  iter: 2079  total_loss: 2.697  loss_sem_seg: 1.455  loss_center: 0.5478  loss_offset: 0.5981  time: 0.3536  data_time: 0.0290  lr: 0.0020269  max_mem: 7418M
[12/11 20:04:35 d2.utils.events]:  eta: 0:46:28  iter: 2099  total_loss: 2.982  loss_sem_seg: 1.652  loss_center: 0.6703  loss_offset: 0.7893  time: 0.3535  data_time: 0.0279  lr: 0.0020223  max_mem: 7418M
[12/11 20:04:42 d2.utils.events]:  eta: 0:46:20  iter: 2119  total_loss: 2.733  loss_sem_seg: 1.318  loss_center: 0.5907  loss_offset: 0.7294  time: 0.3535  data_time: 0.0288  lr: 0.0020177  max_mem: 7418M
[12/11 20:04:49 d2.utils.events]:  eta: 0:46:13  iter: 2139  total_loss: 2.409  loss_sem_seg: 1.208  loss_center: 0.5161  loss_offset: 0.7412  time: 0.3536  data_time: 0.0293  lr: 0.0020131  max_mem: 7418M
[12/11 20:04:56 d2.utils.events]:  eta: 0:46:05  iter: 2159  total_loss: 2.412  loss_sem_seg: 1.283  loss_center: 0.5658  loss_offset: 0.5897  time: 0.3536  data_time: 0.0269  lr: 0.0020085  max_mem: 7418M
[12/11 20:05:03 d2.utils.events]:  eta: 0:45:57  iter: 2179  total_loss: 2.78  loss_sem_seg: 1.379  loss_center: 0.5719  loss_offset: 0.6922  time: 0.3536  data_time: 0.0287  lr: 0.0020039  max_mem: 7418M
[12/11 20:05:10 d2.utils.events]:  eta: 0:45:50  iter: 2199  total_loss: 2.645  loss_sem_seg: 1.425  loss_center: 0.441  loss_offset: 0.69  time: 0.3536  data_time: 0.0289  lr: 0.0019993  max_mem: 7418M
[12/11 20:05:17 d2.utils.events]:  eta: 0:45:42  iter: 2219  total_loss: 2.384  loss_sem_seg: 1.139  loss_center: 0.5931  loss_offset: 0.6345  time: 0.3535  data_time: 0.0277  lr: 0.0019947  max_mem: 7418M
[12/11 20:05:24 d2.utils.events]:  eta: 0:45:35  iter: 2239  total_loss: 2.603  loss_sem_seg: 1.271  loss_center: 0.5342  loss_offset: 0.6985  time: 0.3535  data_time: 0.0292  lr: 0.0019901  max_mem: 7418M
[12/11 20:05:31 d2.utils.events]:  eta: 0:45:28  iter: 2259  total_loss: 2.528  loss_sem_seg: 1.367  loss_center: 0.4674  loss_offset: 0.6885  time: 0.3535  data_time: 0.0285  lr: 0.0019854  max_mem: 7418M
[12/11 20:05:38 d2.utils.events]:  eta: 0:45:21  iter: 2279  total_loss: 2.699  loss_sem_seg: 1.311  loss_center: 0.6576  loss_offset: 0.7045  time: 0.3536  data_time: 0.0290  lr: 0.0019808  max_mem: 7418M
[12/11 20:05:46 d2.utils.events]:  eta: 0:45:15  iter: 2299  total_loss: 2.483  loss_sem_seg: 1.366  loss_center: 0.5314  loss_offset: 0.7045  time: 0.3536  data_time: 0.0284  lr: 0.0019762  max_mem: 7418M
[12/11 20:05:53 d2.utils.events]:  eta: 0:45:07  iter: 2319  total_loss: 2.454  loss_sem_seg: 1.221  loss_center: 0.4437  loss_offset: 0.7096  time: 0.3535  data_time: 0.0274  lr: 0.0019716  max_mem: 7418M
[12/11 20:06:00 d2.utils.events]:  eta: 0:45:01  iter: 2339  total_loss: 2.486  loss_sem_seg: 1.377  loss_center: 0.6077  loss_offset: 0.673  time: 0.3535  data_time: 0.0277  lr: 0.001967  max_mem: 7418M
[12/11 20:06:07 d2.utils.events]:  eta: 0:44:53  iter: 2359  total_loss: 2.746  loss_sem_seg: 1.417  loss_center: 0.4443  loss_offset: 0.6685  time: 0.3535  data_time: 0.0300  lr: 0.0019623  max_mem: 7418M
[12/11 20:06:14 d2.utils.events]:  eta: 0:44:46  iter: 2379  total_loss: 2.806  loss_sem_seg: 1.415  loss_center: 0.5346  loss_offset: 0.6695  time: 0.3535  data_time: 0.0279  lr: 0.0019577  max_mem: 7418M
[12/11 20:06:21 d2.utils.events]:  eta: 0:44:40  iter: 2399  total_loss: 2.638  loss_sem_seg: 1.279  loss_center: 0.7092  loss_offset: 0.6677  time: 0.3536  data_time: 0.0292  lr: 0.0019531  max_mem: 7418M
[12/11 20:06:28 d2.utils.events]:  eta: 0:44:33  iter: 2419  total_loss: 2.521  loss_sem_seg: 1.298  loss_center: 0.5001  loss_offset: 0.596  time: 0.3535  data_time: 0.0277  lr: 0.0019485  max_mem: 7418M
[12/11 20:06:35 d2.utils.events]:  eta: 0:44:26  iter: 2439  total_loss: 2.63  loss_sem_seg: 1.336  loss_center: 0.4689  loss_offset: 0.643  time: 0.3536  data_time: 0.0273  lr: 0.0019438  max_mem: 7418M
[12/11 20:06:42 d2.utils.events]:  eta: 0:44:18  iter: 2459  total_loss: 2.543  loss_sem_seg: 1.216  loss_center: 0.5698  loss_offset: 0.6197  time: 0.3535  data_time: 0.0265  lr: 0.0019392  max_mem: 7418M
[12/11 20:06:49 d2.utils.events]:  eta: 0:44:11  iter: 2479  total_loss: 2.358  loss_sem_seg: 1.223  loss_center: 0.5523  loss_offset: 0.597  time: 0.3535  data_time: 0.0291  lr: 0.0019346  max_mem: 7418M
[12/11 20:06:56 d2.utils.events]:  eta: 0:44:04  iter: 2499  total_loss: 2.794  loss_sem_seg: 1.425  loss_center: 0.6093  loss_offset: 0.7471  time: 0.3535  data_time: 0.0270  lr: 0.00193  max_mem: 7418M
[12/11 20:07:03 d2.utils.events]:  eta: 0:43:57  iter: 2519  total_loss: 2.637  loss_sem_seg: 1.29  loss_center: 0.5837  loss_offset: 0.6338  time: 0.3535  data_time: 0.0281  lr: 0.0019253  max_mem: 7418M
[12/11 20:07:10 d2.utils.events]:  eta: 0:43:50  iter: 2539  total_loss: 2.655  loss_sem_seg: 1.246  loss_center: 0.5682  loss_offset: 0.7262  time: 0.3535  data_time: 0.0270  lr: 0.0019207  max_mem: 7418M
[12/11 20:07:17 d2.utils.events]:  eta: 0:43:43  iter: 2559  total_loss: 2.455  loss_sem_seg: 1.338  loss_center: 0.5452  loss_offset: 0.6327  time: 0.3535  data_time: 0.0277  lr: 0.0019161  max_mem: 7418M
[12/11 20:07:25 d2.utils.events]:  eta: 0:43:36  iter: 2579  total_loss: 2.494  loss_sem_seg: 1.18  loss_center: 0.5748  loss_offset: 0.6955  time: 0.3535  data_time: 0.0295  lr: 0.0019114  max_mem: 7418M
[12/11 20:07:32 d2.utils.events]:  eta: 0:43:30  iter: 2599  total_loss: 2.457  loss_sem_seg: 1.178  loss_center: 0.6081  loss_offset: 0.6705  time: 0.3535  data_time: 0.0290  lr: 0.0019068  max_mem: 7418M
[12/11 20:07:39 d2.utils.events]:  eta: 0:43:22  iter: 2619  total_loss: 2.405  loss_sem_seg: 1.212  loss_center: 0.4972  loss_offset: 0.6213  time: 0.3535  data_time: 0.0291  lr: 0.0019021  max_mem: 7418M
[12/11 20:07:46 d2.utils.events]:  eta: 0:43:15  iter: 2639  total_loss: 2.707  loss_sem_seg: 1.275  loss_center: 0.7298  loss_offset: 0.7065  time: 0.3535  data_time: 0.0275  lr: 0.0018975  max_mem: 7418M
[12/11 20:07:53 d2.utils.events]:  eta: 0:43:08  iter: 2659  total_loss: 2.682  loss_sem_seg: 1.394  loss_center: 0.5674  loss_offset: 0.6421  time: 0.3535  data_time: 0.0276  lr: 0.0018929  max_mem: 7418M
[12/11 20:08:00 d2.utils.events]:  eta: 0:43:02  iter: 2679  total_loss: 2.889  loss_sem_seg: 1.531  loss_center: 0.5349  loss_offset: 0.6029  time: 0.3535  data_time: 0.0287  lr: 0.0018882  max_mem: 7418M
[12/11 20:08:07 d2.utils.events]:  eta: 0:42:53  iter: 2699  total_loss: 2.467  loss_sem_seg: 1.235  loss_center: 0.615  loss_offset: 0.7522  time: 0.3535  data_time: 0.0273  lr: 0.0018836  max_mem: 7418M
[12/11 20:08:14 d2.utils.events]:  eta: 0:42:46  iter: 2719  total_loss: 2.546  loss_sem_seg: 1.028  loss_center: 0.5512  loss_offset: 0.6719  time: 0.3535  data_time: 0.0290  lr: 0.0018789  max_mem: 7418M
[12/11 20:08:21 d2.utils.events]:  eta: 0:42:39  iter: 2739  total_loss: 2.288  loss_sem_seg: 1.168  loss_center: 0.4614  loss_offset: 0.5787  time: 0.3535  data_time: 0.0279  lr: 0.0018743  max_mem: 7418M
[12/11 20:08:28 d2.utils.events]:  eta: 0:42:32  iter: 2759  total_loss: 2.489  loss_sem_seg: 1.242  loss_center: 0.457  loss_offset: 0.6314  time: 0.3535  data_time: 0.0283  lr: 0.0018696  max_mem: 7418M
[12/11 20:08:35 d2.utils.events]:  eta: 0:42:26  iter: 2779  total_loss: 2.336  loss_sem_seg: 1.175  loss_center: 0.5066  loss_offset: 0.5939  time: 0.3535  data_time: 0.0272  lr: 0.001865  max_mem: 7418M
[12/11 20:08:42 d2.utils.events]:  eta: 0:42:18  iter: 2799  total_loss: 2.495  loss_sem_seg: 1.16  loss_center: 0.6243  loss_offset: 0.6901  time: 0.3535  data_time: 0.0267  lr: 0.0018603  max_mem: 7418M
[12/11 20:08:50 d2.utils.events]:  eta: 0:42:12  iter: 2819  total_loss: 2.724  loss_sem_seg: 1.586  loss_center: 0.5051  loss_offset: 0.7192  time: 0.3535  data_time: 0.0289  lr: 0.0018557  max_mem: 7418M
[12/11 20:08:57 d2.utils.events]:  eta: 0:42:05  iter: 2839  total_loss: 2.671  loss_sem_seg: 1.362  loss_center: 0.6696  loss_offset: 0.643  time: 0.3535  data_time: 0.0277  lr: 0.001851  max_mem: 7418M
[12/11 20:09:04 d2.utils.events]:  eta: 0:41:58  iter: 2859  total_loss: 2.611  loss_sem_seg: 1.362  loss_center: 0.5289  loss_offset: 0.6492  time: 0.3535  data_time: 0.0283  lr: 0.0018464  max_mem: 7418M
[12/11 20:09:11 d2.utils.events]:  eta: 0:41:50  iter: 2879  total_loss: 2.532  loss_sem_seg: 1.339  loss_center: 0.5344  loss_offset: 0.677  time: 0.3535  data_time: 0.0284  lr: 0.0018417  max_mem: 7418M
[12/11 20:09:18 d2.utils.events]:  eta: 0:41:43  iter: 2899  total_loss: 2.331  loss_sem_seg: 1.256  loss_center: 0.4151  loss_offset: 0.6013  time: 0.3535  data_time: 0.0279  lr: 0.0018371  max_mem: 7418M
[12/11 20:09:25 d2.utils.events]:  eta: 0:41:36  iter: 2919  total_loss: 2.496  loss_sem_seg: 1.147  loss_center: 0.5753  loss_offset: 0.7473  time: 0.3535  data_time: 0.0280  lr: 0.0018324  max_mem: 7418M
[12/11 20:09:32 d2.utils.events]:  eta: 0:41:28  iter: 2939  total_loss: 2.304  loss_sem_seg: 1.185  loss_center: 0.4805  loss_offset: 0.6031  time: 0.3535  data_time: 0.0292  lr: 0.0018278  max_mem: 7418M
[12/11 20:09:39 d2.utils.events]:  eta: 0:41:21  iter: 2959  total_loss: 2.766  loss_sem_seg: 1.405  loss_center: 0.6361  loss_offset: 0.7056  time: 0.3535  data_time: 0.0272  lr: 0.0018231  max_mem: 7418M
[12/11 20:09:46 d2.utils.events]:  eta: 0:41:15  iter: 2979  total_loss: 2.482  loss_sem_seg: 1.353  loss_center: 0.5483  loss_offset: 0.6068  time: 0.3535  data_time: 0.0279  lr: 0.0018184  max_mem: 7418M
[12/11 20:09:53 d2.utils.events]:  eta: 0:41:08  iter: 2999  total_loss: 2.509  loss_sem_seg: 1.229  loss_center: 0.5871  loss_offset: 0.6345  time: 0.3535  data_time: 0.0300  lr: 0.0018138  max_mem: 7418M
[12/11 20:10:00 d2.utils.events]:  eta: 0:41:01  iter: 3019  total_loss: 2.695  loss_sem_seg: 1.219  loss_center: 0.529  loss_offset: 0.7212  time: 0.3535  data_time: 0.0301  lr: 0.0018091  max_mem: 7418M
[12/11 20:10:07 d2.utils.events]:  eta: 0:40:54  iter: 3039  total_loss: 2.457  loss_sem_seg: 1.239  loss_center: 0.457  loss_offset: 0.7285  time: 0.3535  data_time: 0.0281  lr: 0.0018044  max_mem: 7418M
[12/11 20:10:15 d2.utils.events]:  eta: 0:40:47  iter: 3059  total_loss: 2.847  loss_sem_seg: 1.395  loss_center: 0.6028  loss_offset: 0.7888  time: 0.3535  data_time: 0.0300  lr: 0.0017998  max_mem: 7418M
[12/11 20:10:22 d2.utils.events]:  eta: 0:40:40  iter: 3079  total_loss: 2.794  loss_sem_seg: 1.332  loss_center: 0.5302  loss_offset: 0.5926  time: 0.3535  data_time: 0.0266  lr: 0.0017951  max_mem: 7418M
[12/11 20:10:29 d2.utils.events]:  eta: 0:40:33  iter: 3099  total_loss: 2.714  loss_sem_seg: 1.216  loss_center: 0.6641  loss_offset: 0.6687  time: 0.3535  data_time: 0.0292  lr: 0.0017904  max_mem: 7418M
[12/11 20:10:36 d2.utils.events]:  eta: 0:40:26  iter: 3119  total_loss: 2.681  loss_sem_seg: 1.374  loss_center: 0.4757  loss_offset: 0.7153  time: 0.3535  data_time: 0.0278  lr: 0.0017858  max_mem: 7418M
[12/11 20:10:43 d2.utils.events]:  eta: 0:40:18  iter: 3139  total_loss: 2.642  loss_sem_seg: 1.384  loss_center: 0.609  loss_offset: 0.6644  time: 0.3535  data_time: 0.0268  lr: 0.0017811  max_mem: 7418M
[12/11 20:10:50 d2.utils.events]:  eta: 0:40:11  iter: 3159  total_loss: 2.569  loss_sem_seg: 1.302  loss_center: 0.6711  loss_offset: 0.6523  time: 0.3535  data_time: 0.0287  lr: 0.0017764  max_mem: 7418M
[12/11 20:10:57 d2.utils.events]:  eta: 0:40:05  iter: 3179  total_loss: 2.486  loss_sem_seg: 1.298  loss_center: 0.6216  loss_offset: 0.556  time: 0.3535  data_time: 0.0284  lr: 0.0017718  max_mem: 7418M
[12/11 20:11:04 d2.utils.events]:  eta: 0:39:58  iter: 3199  total_loss: 2.566  loss_sem_seg: 1.399  loss_center: 0.4745  loss_offset: 0.6127  time: 0.3535  data_time: 0.0279  lr: 0.0017671  max_mem: 7418M
[12/11 20:11:11 d2.utils.events]:  eta: 0:39:51  iter: 3219  total_loss: 2.595  loss_sem_seg: 1.198  loss_center: 0.5032  loss_offset: 0.6122  time: 0.3535  data_time: 0.0284  lr: 0.0017624  max_mem: 7418M
[12/11 20:11:18 d2.utils.events]:  eta: 0:39:44  iter: 3239  total_loss: 2.516  loss_sem_seg: 1.171  loss_center: 0.6023  loss_offset: 0.6493  time: 0.3535  data_time: 0.0274  lr: 0.0017577  max_mem: 7418M
[12/11 20:11:25 d2.utils.events]:  eta: 0:39:37  iter: 3259  total_loss: 2.382  loss_sem_seg: 1.076  loss_center: 0.5559  loss_offset: 0.6127  time: 0.3535  data_time: 0.0275  lr: 0.001753  max_mem: 7418M
[12/11 20:11:32 d2.utils.events]:  eta: 0:39:30  iter: 3279  total_loss: 2.6  loss_sem_seg: 1.31  loss_center: 0.5983  loss_offset: 0.6919  time: 0.3535  data_time: 0.0297  lr: 0.0017484  max_mem: 7418M
[12/11 20:11:39 d2.utils.events]:  eta: 0:39:22  iter: 3299  total_loss: 2.739  loss_sem_seg: 1.335  loss_center: 0.5765  loss_offset: 0.6888  time: 0.3535  data_time: 0.0268  lr: 0.0017437  max_mem: 7418M
[12/11 20:11:46 d2.utils.events]:  eta: 0:39:15  iter: 3319  total_loss: 2.449  loss_sem_seg: 1.219  loss_center: 0.6177  loss_offset: 0.6864  time: 0.3535  data_time: 0.0282  lr: 0.001739  max_mem: 7418M
[12/11 20:11:53 d2.utils.events]:  eta: 0:39:08  iter: 3339  total_loss: 2.506  loss_sem_seg: 1.237  loss_center: 0.4182  loss_offset: 0.6677  time: 0.3535  data_time: 0.0273  lr: 0.0017343  max_mem: 7418M
[12/11 20:12:01 d2.utils.events]:  eta: 0:39:01  iter: 3359  total_loss: 2.643  loss_sem_seg: 1.411  loss_center: 0.5994  loss_offset: 0.6197  time: 0.3535  data_time: 0.0286  lr: 0.0017296  max_mem: 7418M
[12/11 20:12:08 d2.utils.events]:  eta: 0:38:54  iter: 3379  total_loss: 2.54  loss_sem_seg: 1.303  loss_center: 0.5789  loss_offset: 0.5795  time: 0.3535  data_time: 0.0284  lr: 0.0017249  max_mem: 7418M
[12/11 20:12:15 d2.utils.events]:  eta: 0:38:47  iter: 3399  total_loss: 2.702  loss_sem_seg: 1.353  loss_center: 0.5664  loss_offset: 0.6944  time: 0.3535  data_time: 0.0281  lr: 0.0017202  max_mem: 7418M
[12/11 20:12:22 d2.utils.events]:  eta: 0:38:39  iter: 3419  total_loss: 2.642  loss_sem_seg: 1.522  loss_center: 0.5097  loss_offset: 0.6157  time: 0.3535  data_time: 0.0278  lr: 0.0017155  max_mem: 7418M
[12/11 20:12:29 d2.utils.events]:  eta: 0:38:32  iter: 3439  total_loss: 2.468  loss_sem_seg: 1.108  loss_center: 0.4898  loss_offset: 0.6932  time: 0.3535  data_time: 0.0280  lr: 0.0017109  max_mem: 7418M
[12/11 20:12:36 d2.utils.events]:  eta: 0:38:25  iter: 3459  total_loss: 2.297  loss_sem_seg: 1.048  loss_center: 0.5434  loss_offset: 0.5681  time: 0.3535  data_time: 0.0288  lr: 0.0017062  max_mem: 7418M
[12/11 20:12:43 d2.utils.events]:  eta: 0:38:18  iter: 3479  total_loss: 2.601  loss_sem_seg: 1.182  loss_center: 0.5911  loss_offset: 0.683  time: 0.3534  data_time: 0.0269  lr: 0.0017015  max_mem: 7418M
[12/11 20:12:50 d2.utils.events]:  eta: 0:38:12  iter: 3499  total_loss: 2.523  loss_sem_seg: 1.297  loss_center: 0.4766  loss_offset: 0.7034  time: 0.3535  data_time: 0.0303  lr: 0.0016968  max_mem: 7418M
[12/11 20:12:57 d2.utils.events]:  eta: 0:38:04  iter: 3519  total_loss: 2.16  loss_sem_seg: 1.075  loss_center: 0.5002  loss_offset: 0.5306  time: 0.3535  data_time: 0.0290  lr: 0.0016921  max_mem: 7418M
[12/11 20:13:04 d2.utils.events]:  eta: 0:37:58  iter: 3539  total_loss: 2.672  loss_sem_seg: 1.227  loss_center: 0.538  loss_offset: 0.6617  time: 0.3535  data_time: 0.0290  lr: 0.0016874  max_mem: 7418M
[12/11 20:13:11 d2.utils.events]:  eta: 0:37:50  iter: 3559  total_loss: 2.48  loss_sem_seg: 1.193  loss_center: 0.7  loss_offset: 0.578  time: 0.3535  data_time: 0.0262  lr: 0.0016827  max_mem: 7418M
[12/11 20:13:18 d2.utils.events]:  eta: 0:37:43  iter: 3579  total_loss: 2.412  loss_sem_seg: 1.166  loss_center: 0.5135  loss_offset: 0.546  time: 0.3535  data_time: 0.0290  lr: 0.001678  max_mem: 7418M
[12/11 20:13:26 d2.utils.events]:  eta: 0:37:36  iter: 3599  total_loss: 2.56  loss_sem_seg: 1.182  loss_center: 0.4331  loss_offset: 0.635  time: 0.3535  data_time: 0.0275  lr: 0.0016733  max_mem: 7418M
[12/11 20:13:33 d2.utils.events]:  eta: 0:37:29  iter: 3619  total_loss: 2.519  loss_sem_seg: 1.13  loss_center: 0.4401  loss_offset: 0.7522  time: 0.3535  data_time: 0.0299  lr: 0.0016686  max_mem: 7418M
[12/11 20:13:40 d2.utils.events]:  eta: 0:37:22  iter: 3639  total_loss: 2.481  loss_sem_seg: 1.251  loss_center: 0.6156  loss_offset: 0.5364  time: 0.3535  data_time: 0.0277  lr: 0.0016638  max_mem: 7418M
[12/11 20:13:47 d2.utils.events]:  eta: 0:37:14  iter: 3659  total_loss: 2.31  loss_sem_seg: 1.192  loss_center: 0.5248  loss_offset: 0.6425  time: 0.3535  data_time: 0.0277  lr: 0.0016591  max_mem: 7418M
[12/11 20:13:54 d2.utils.events]:  eta: 0:37:07  iter: 3679  total_loss: 2.214  loss_sem_seg: 1.185  loss_center: 0.4882  loss_offset: 0.7053  time: 0.3535  data_time: 0.0289  lr: 0.0016544  max_mem: 7418M
[12/11 20:14:01 d2.utils.events]:  eta: 0:37:00  iter: 3699  total_loss: 2.236  loss_sem_seg: 1.223  loss_center: 0.4774  loss_offset: 0.5509  time: 0.3535  data_time: 0.0275  lr: 0.0016497  max_mem: 7418M
[12/11 20:14:08 d2.utils.events]:  eta: 0:36:53  iter: 3719  total_loss: 2.452  loss_sem_seg: 1.168  loss_center: 0.6689  loss_offset: 0.6414  time: 0.3535  data_time: 0.0300  lr: 0.001645  max_mem: 7418M
[12/11 20:14:15 d2.utils.events]:  eta: 0:36:46  iter: 3739  total_loss: 2.278  loss_sem_seg: 1.195  loss_center: 0.5215  loss_offset: 0.6557  time: 0.3535  data_time: 0.0268  lr: 0.0016403  max_mem: 7418M
[12/11 20:14:22 d2.utils.events]:  eta: 0:36:40  iter: 3759  total_loss: 2.354  loss_sem_seg: 1.151  loss_center: 0.4787  loss_offset: 0.6295  time: 0.3535  data_time: 0.0296  lr: 0.0016356  max_mem: 7418M
[12/11 20:14:29 d2.utils.events]:  eta: 0:36:32  iter: 3779  total_loss: 2.449  loss_sem_seg: 1.227  loss_center: 0.6545  loss_offset: 0.6252  time: 0.3535  data_time: 0.0292  lr: 0.0016309  max_mem: 7418M
[12/11 20:14:37 d2.utils.events]:  eta: 0:36:26  iter: 3799  total_loss: 2.385  loss_sem_seg: 1.128  loss_center: 0.5982  loss_offset: 0.5953  time: 0.3535  data_time: 0.0276  lr: 0.0016261  max_mem: 7418M
[12/11 20:14:44 d2.utils.events]:  eta: 0:36:19  iter: 3819  total_loss: 2.269  loss_sem_seg: 1.139  loss_center: 0.4767  loss_offset: 0.6697  time: 0.3535  data_time: 0.0279  lr: 0.0016214  max_mem: 7418M
[12/11 20:14:51 d2.utils.events]:  eta: 0:36:12  iter: 3839  total_loss: 2.593  loss_sem_seg: 1.236  loss_center: 0.6008  loss_offset: 0.6341  time: 0.3535  data_time: 0.0298  lr: 0.0016167  max_mem: 7418M
[12/11 20:14:58 d2.utils.events]:  eta: 0:36:04  iter: 3859  total_loss: 2.55  loss_sem_seg: 1.264  loss_center: 0.5729  loss_offset: 0.7163  time: 0.3535  data_time: 0.0287  lr: 0.001612  max_mem: 7418M
[12/11 20:15:05 d2.utils.events]:  eta: 0:35:57  iter: 3879  total_loss: 2.317  loss_sem_seg: 1.114  loss_center: 0.5915  loss_offset: 0.5636  time: 0.3535  data_time: 0.0282  lr: 0.0016072  max_mem: 7418M
[12/11 20:15:12 d2.utils.events]:  eta: 0:35:50  iter: 3899  total_loss: 2.21  loss_sem_seg: 1.025  loss_center: 0.5815  loss_offset: 0.6296  time: 0.3535  data_time: 0.0299  lr: 0.0016025  max_mem: 7418M
[12/11 20:15:19 d2.utils.events]:  eta: 0:35:44  iter: 3919  total_loss: 2.45  loss_sem_seg: 1.192  loss_center: 0.6045  loss_offset: 0.5935  time: 0.3535  data_time: 0.0279  lr: 0.0015978  max_mem: 7418M
[12/11 20:15:26 d2.utils.events]:  eta: 0:35:37  iter: 3939  total_loss: 2.774  loss_sem_seg: 1.521  loss_center: 0.4612  loss_offset: 0.6715  time: 0.3535  data_time: 0.0284  lr: 0.0015931  max_mem: 7418M
[12/11 20:15:33 d2.utils.events]:  eta: 0:35:32  iter: 3959  total_loss: 2.606  loss_sem_seg: 1.316  loss_center: 0.6828  loss_offset: 0.6291  time: 0.3535  data_time: 0.0283  lr: 0.0015883  max_mem: 7418M
[12/11 20:15:40 d2.utils.events]:  eta: 0:35:23  iter: 3979  total_loss: 2.407  loss_sem_seg: 1.131  loss_center: 0.6077  loss_offset: 0.5337  time: 0.3535  data_time: 0.0273  lr: 0.0015836  max_mem: 7418M
[12/11 20:15:47 d2.utils.events]:  eta: 0:35:16  iter: 3999  total_loss: 2.79  loss_sem_seg: 1.242  loss_center: 0.5342  loss_offset: 0.7548  time: 0.3535  data_time: 0.0305  lr: 0.0015789  max_mem: 7418M
[12/11 20:15:54 d2.utils.events]:  eta: 0:35:09  iter: 4019  total_loss: 2.549  loss_sem_seg: 1.274  loss_center: 0.503  loss_offset: 0.7166  time: 0.3535  data_time: 0.0288  lr: 0.0015741  max_mem: 7418M
[12/11 20:16:01 d2.utils.events]:  eta: 0:35:02  iter: 4039  total_loss: 2.094  loss_sem_seg: 0.9916  loss_center: 0.5979  loss_offset: 0.5363  time: 0.3535  data_time: 0.0296  lr: 0.0015694  max_mem: 7418M
[12/11 20:16:09 d2.utils.events]:  eta: 0:34:55  iter: 4059  total_loss: 2.487  loss_sem_seg: 1.233  loss_center: 0.5301  loss_offset: 0.6018  time: 0.3535  data_time: 0.0290  lr: 0.0015646  max_mem: 7418M
[12/11 20:16:16 d2.utils.events]:  eta: 0:34:48  iter: 4079  total_loss: 2.771  loss_sem_seg: 1.232  loss_center: 0.6321  loss_offset: 0.6627  time: 0.3535  data_time: 0.0278  lr: 0.0015599  max_mem: 7418M
[12/11 20:16:23 d2.utils.events]:  eta: 0:34:41  iter: 4099  total_loss: 2.3  loss_sem_seg: 1.048  loss_center: 0.4897  loss_offset: 0.5623  time: 0.3535  data_time: 0.0294  lr: 0.0015552  max_mem: 7418M
[12/11 20:16:30 d2.utils.events]:  eta: 0:34:34  iter: 4119  total_loss: 2.26  loss_sem_seg: 1.091  loss_center: 0.4762  loss_offset: 0.5941  time: 0.3535  data_time: 0.0283  lr: 0.0015504  max_mem: 7418M
[12/11 20:16:37 d2.utils.events]:  eta: 0:34:30  iter: 4139  total_loss: 2.181  loss_sem_seg: 1.022  loss_center: 0.7875  loss_offset: 0.4335  time: 0.3536  data_time: 0.0283  lr: 0.0015457  max_mem: 7418M
[12/11 20:16:44 d2.utils.events]:  eta: 0:34:21  iter: 4159  total_loss: 2.265  loss_sem_seg: 1.237  loss_center: 0.4311  loss_offset: 0.5766  time: 0.3536  data_time: 0.0281  lr: 0.0015409  max_mem: 7418M
[12/11 20:16:51 d2.utils.events]:  eta: 0:34:12  iter: 4179  total_loss: 2.235  loss_sem_seg: 1.128  loss_center: 0.4835  loss_offset: 0.5692  time: 0.3536  data_time: 0.0288  lr: 0.0015362  max_mem: 7418M
[12/11 20:16:58 d2.utils.events]:  eta: 0:34:06  iter: 4199  total_loss: 2.443  loss_sem_seg: 1.078  loss_center: 0.6044  loss_offset: 0.6295  time: 0.3536  data_time: 0.0268  lr: 0.0015314  max_mem: 7418M
[12/11 20:17:05 d2.utils.events]:  eta: 0:33:58  iter: 4219  total_loss: 2.234  loss_sem_seg: 1.056  loss_center: 0.441  loss_offset: 0.5587  time: 0.3535  data_time: 0.0271  lr: 0.0015267  max_mem: 7418M
[12/11 20:17:12 d2.utils.events]:  eta: 0:33:54  iter: 4239  total_loss: 2.399  loss_sem_seg: 1.143  loss_center: 0.541  loss_offset: 0.5488  time: 0.3535  data_time: 0.0292  lr: 0.0015219  max_mem: 7418M
[12/11 20:17:20 d2.utils.events]:  eta: 0:33:46  iter: 4259  total_loss: 2.367  loss_sem_seg: 1.07  loss_center: 0.5488  loss_offset: 0.5993  time: 0.3536  data_time: 0.0307  lr: 0.0015172  max_mem: 7418M
[12/11 20:17:27 d2.utils.events]:  eta: 0:33:39  iter: 4279  total_loss: 2.068  loss_sem_seg: 0.9767  loss_center: 0.4536  loss_offset: 0.4969  time: 0.3536  data_time: 0.0288  lr: 0.0015124  max_mem: 7418M
[12/11 20:17:34 d2.utils.events]:  eta: 0:33:32  iter: 4299  total_loss: 2.384  loss_sem_seg: 1.206  loss_center: 0.5575  loss_offset: 0.6217  time: 0.3536  data_time: 0.0281  lr: 0.0015076  max_mem: 7418M
[12/11 20:17:41 d2.utils.events]:  eta: 0:33:26  iter: 4319  total_loss: 2.311  loss_sem_seg: 1.148  loss_center: 0.6561  loss_offset: 0.5108  time: 0.3535  data_time: 0.0280  lr: 0.0015029  max_mem: 7418M
[12/11 20:17:48 d2.utils.events]:  eta: 0:33:17  iter: 4339  total_loss: 2.705  loss_sem_seg: 1.257  loss_center: 0.5901  loss_offset: 0.6402  time: 0.3535  data_time: 0.0276  lr: 0.0014981  max_mem: 7418M
[12/11 20:17:55 d2.utils.events]:  eta: 0:33:11  iter: 4359  total_loss: 2.434  loss_sem_seg: 1.216  loss_center: 0.58  loss_offset: 0.647  time: 0.3535  data_time: 0.0278  lr: 0.0014933  max_mem: 7418M
[12/11 20:18:02 d2.utils.events]:  eta: 0:33:03  iter: 4379  total_loss: 2.299  loss_sem_seg: 1.035  loss_center: 0.4784  loss_offset: 0.5443  time: 0.3535  data_time: 0.0282  lr: 0.0014886  max_mem: 7418M
[12/11 20:18:09 d2.utils.events]:  eta: 0:32:56  iter: 4399  total_loss: 2.174  loss_sem_seg: 1.044  loss_center: 0.5864  loss_offset: 0.5833  time: 0.3535  data_time: 0.0288  lr: 0.0014838  max_mem: 7418M
[12/11 20:18:16 d2.utils.events]:  eta: 0:32:50  iter: 4419  total_loss: 2.226  loss_sem_seg: 1.15  loss_center: 0.3935  loss_offset: 0.5523  time: 0.3535  data_time: 0.0269  lr: 0.001479  max_mem: 7418M
[12/11 20:18:23 d2.utils.events]:  eta: 0:32:44  iter: 4439  total_loss: 2.463  loss_sem_seg: 1.212  loss_center: 0.6788  loss_offset: 0.6004  time: 0.3535  data_time: 0.0294  lr: 0.0014743  max_mem: 7418M
[12/11 20:18:30 d2.utils.events]:  eta: 0:32:37  iter: 4459  total_loss: 2.168  loss_sem_seg: 0.9703  loss_center: 0.5605  loss_offset: 0.574  time: 0.3535  data_time: 0.0299  lr: 0.0014695  max_mem: 7418M
[12/11 20:18:37 d2.utils.events]:  eta: 0:32:30  iter: 4479  total_loss: 2.503  loss_sem_seg: 1.186  loss_center: 0.5555  loss_offset: 0.6442  time: 0.3535  data_time: 0.0282  lr: 0.0014647  max_mem: 7418M
[12/11 20:18:44 d2.utils.events]:  eta: 0:32:20  iter: 4499  total_loss: 2.177  loss_sem_seg: 0.963  loss_center: 0.6591  loss_offset: 0.5845  time: 0.3535  data_time: 0.0274  lr: 0.0014599  max_mem: 7418M
[12/11 20:18:51 d2.utils.events]:  eta: 0:32:13  iter: 4519  total_loss: 2.198  loss_sem_seg: 1.136  loss_center: 0.4399  loss_offset: 0.6254  time: 0.3535  data_time: 0.0281  lr: 0.0014552  max_mem: 7418M
[12/11 20:18:59 d2.utils.events]:  eta: 0:32:05  iter: 4539  total_loss: 2.533  loss_sem_seg: 1.131  loss_center: 0.5808  loss_offset: 0.6528  time: 0.3535  data_time: 0.0288  lr: 0.0014504  max_mem: 7418M
[12/11 20:19:06 d2.utils.events]:  eta: 0:31:58  iter: 4559  total_loss: 2.2  loss_sem_seg: 1.12  loss_center: 0.6202  loss_offset: 0.5007  time: 0.3535  data_time: 0.0274  lr: 0.0014456  max_mem: 7418M
[12/11 20:19:13 d2.utils.events]:  eta: 0:31:51  iter: 4579  total_loss: 2.248  loss_sem_seg: 1.067  loss_center: 0.487  loss_offset: 0.567  time: 0.3535  data_time: 0.0292  lr: 0.0014408  max_mem: 7418M
[12/11 20:19:20 d2.utils.events]:  eta: 0:31:45  iter: 4599  total_loss: 2.323  loss_sem_seg: 1.141  loss_center: 0.5756  loss_offset: 0.5298  time: 0.3535  data_time: 0.0271  lr: 0.001436  max_mem: 7418M
[12/11 20:19:27 d2.utils.events]:  eta: 0:31:37  iter: 4619  total_loss: 2.325  loss_sem_seg: 1.079  loss_center: 0.561  loss_offset: 0.5644  time: 0.3535  data_time: 0.0282  lr: 0.0014313  max_mem: 7418M
[12/11 20:19:34 d2.utils.events]:  eta: 0:31:29  iter: 4639  total_loss: 2.424  loss_sem_seg: 1.221  loss_center: 0.4418  loss_offset: 0.6145  time: 0.3535  data_time: 0.0277  lr: 0.0014265  max_mem: 7418M
[12/11 20:19:41 d2.utils.events]:  eta: 0:31:23  iter: 4659  total_loss: 2.42  loss_sem_seg: 1.076  loss_center: 0.5672  loss_offset: 0.6686  time: 0.3535  data_time: 0.0277  lr: 0.0014217  max_mem: 7418M
[12/11 20:19:48 d2.utils.events]:  eta: 0:31:14  iter: 4679  total_loss: 2.129  loss_sem_seg: 0.9396  loss_center: 0.5586  loss_offset: 0.5742  time: 0.3535  data_time: 0.0287  lr: 0.0014169  max_mem: 7418M
[12/11 20:19:55 d2.utils.events]:  eta: 0:31:08  iter: 4699  total_loss: 2.343  loss_sem_seg: 1.152  loss_center: 0.5248  loss_offset: 0.6764  time: 0.3535  data_time: 0.0311  lr: 0.0014121  max_mem: 7418M
[12/11 20:20:02 d2.utils.events]:  eta: 0:31:00  iter: 4719  total_loss: 2.183  loss_sem_seg: 0.9567  loss_center: 0.5784  loss_offset: 0.5804  time: 0.3535  data_time: 0.0291  lr: 0.0014073  max_mem: 7418M
[12/11 20:20:09 d2.utils.events]:  eta: 0:30:54  iter: 4739  total_loss: 2.135  loss_sem_seg: 0.9549  loss_center: 0.5588  loss_offset: 0.477  time: 0.3535  data_time: 0.0275  lr: 0.0014025  max_mem: 7418M
[12/11 20:20:16 d2.utils.events]:  eta: 0:30:46  iter: 4759  total_loss: 2.054  loss_sem_seg: 0.9982  loss_center: 0.5799  loss_offset: 0.5395  time: 0.3535  data_time: 0.0297  lr: 0.0013977  max_mem: 7418M
[12/11 20:20:24 d2.utils.events]:  eta: 0:30:39  iter: 4779  total_loss: 2.062  loss_sem_seg: 0.9889  loss_center: 0.5027  loss_offset: 0.5377  time: 0.3535  data_time: 0.0284  lr: 0.0013929  max_mem: 7418M
[12/11 20:20:31 d2.utils.events]:  eta: 0:30:32  iter: 4799  total_loss: 2.24  loss_sem_seg: 0.9662  loss_center: 0.6149  loss_offset: 0.5802  time: 0.3535  data_time: 0.0288  lr: 0.0013881  max_mem: 7418M
[12/11 20:20:38 d2.utils.events]:  eta: 0:30:25  iter: 4819  total_loss: 2.225  loss_sem_seg: 1.037  loss_center: 0.4845  loss_offset: 0.4729  time: 0.3535  data_time: 0.0273  lr: 0.0013833  max_mem: 7418M
[12/11 20:20:45 d2.utils.events]:  eta: 0:30:18  iter: 4839  total_loss: 2.382  loss_sem_seg: 1.018  loss_center: 0.5855  loss_offset: 0.5849  time: 0.3535  data_time: 0.0270  lr: 0.0013785  max_mem: 7418M
[12/11 20:20:52 d2.utils.events]:  eta: 0:30:11  iter: 4859  total_loss: 2.61  loss_sem_seg: 1.191  loss_center: 0.6478  loss_offset: 0.6861  time: 0.3535  data_time: 0.0278  lr: 0.0013737  max_mem: 7418M
[12/11 20:20:59 d2.utils.events]:  eta: 0:30:04  iter: 4879  total_loss: 2.382  loss_sem_seg: 1.048  loss_center: 0.5035  loss_offset: 0.5747  time: 0.3535  data_time: 0.0274  lr: 0.0013689  max_mem: 7418M
[12/11 20:21:06 d2.utils.events]:  eta: 0:29:56  iter: 4899  total_loss: 2.267  loss_sem_seg: 1.211  loss_center: 0.5513  loss_offset: 0.6004  time: 0.3535  data_time: 0.0290  lr: 0.001364  max_mem: 7418M
[12/11 20:21:13 d2.utils.events]:  eta: 0:29:49  iter: 4919  total_loss: 2.217  loss_sem_seg: 1.129  loss_center: 0.5135  loss_offset: 0.4959  time: 0.3535  data_time: 0.0285  lr: 0.0013592  max_mem: 7418M
[12/11 20:21:20 d2.utils.events]:  eta: 0:29:41  iter: 4939  total_loss: 2.168  loss_sem_seg: 1.001  loss_center: 0.4725  loss_offset: 0.5821  time: 0.3535  data_time: 0.0278  lr: 0.0013544  max_mem: 7418M
[12/11 20:21:27 d2.utils.events]:  eta: 0:29:34  iter: 4959  total_loss: 2.284  loss_sem_seg: 1.017  loss_center: 0.4743  loss_offset: 0.5639  time: 0.3535  data_time: 0.0276  lr: 0.0013496  max_mem: 7418M
[12/11 20:21:34 d2.utils.events]:  eta: 0:29:27  iter: 4979  total_loss: 2.227  loss_sem_seg: 1.083  loss_center: 0.624  loss_offset: 0.559  time: 0.3535  data_time: 0.0274  lr: 0.0013448  max_mem: 7418M
[12/11 20:21:41 fvcore.common.checkpoint]: Saving checkpoint to ./output/model_0004999.pth
[12/11 20:21:42 d2.utils.events]:  eta: 0:29:19  iter: 4999  total_loss: 2.335  loss_sem_seg: 1.013  loss_center: 0.7098  loss_offset: 0.616  time: 0.3535  data_time: 0.0273  lr: 0.00134  max_mem: 7418M
[12/11 20:21:50 d2.utils.events]:  eta: 0:29:12  iter: 5019  total_loss: 2.272  loss_sem_seg: 1.051  loss_center: 0.5431  loss_offset: 0.5429  time: 0.3535  data_time: 0.0285  lr: 0.0013351  max_mem: 7418M
[12/11 20:21:57 d2.utils.events]:  eta: 0:29:05  iter: 5039  total_loss: 2.223  loss_sem_seg: 0.9599  loss_center: 0.6841  loss_offset: 0.4926  time: 0.3535  data_time: 0.0283  lr: 0.0013303  max_mem: 7418M
[12/11 20:22:04 d2.utils.events]:  eta: 0:28:59  iter: 5059  total_loss: 2.189  loss_sem_seg: 0.964  loss_center: 0.505  loss_offset: 0.5566  time: 0.3535  data_time: 0.0287  lr: 0.0013255  max_mem: 7418M
[12/11 20:22:11 d2.utils.events]:  eta: 0:28:52  iter: 5079  total_loss: 2.558  loss_sem_seg: 1.227  loss_center: 0.6762  loss_offset: 0.6848  time: 0.3535  data_time: 0.0290  lr: 0.0013207  max_mem: 7418M
[12/11 20:22:18 d2.utils.events]:  eta: 0:28:44  iter: 5099  total_loss: 2.065  loss_sem_seg: 1.098  loss_center: 0.4294  loss_offset: 0.5413  time: 0.3535  data_time: 0.0304  lr: 0.0013158  max_mem: 7418M
[12/11 20:22:25 d2.utils.events]:  eta: 0:28:37  iter: 5119  total_loss: 2.094  loss_sem_seg: 0.9524  loss_center: 0.4979  loss_offset: 0.4483  time: 0.3535  data_time: 0.0288  lr: 0.001311  max_mem: 7418M
[12/11 20:22:32 d2.utils.events]:  eta: 0:28:29  iter: 5139  total_loss: 2.214  loss_sem_seg: 0.8367  loss_center: 0.5418  loss_offset: 0.6635  time: 0.3535  data_time: 0.0269  lr: 0.0013062  max_mem: 7418M
[12/11 20:22:39 d2.utils.events]:  eta: 0:28:22  iter: 5159  total_loss: 2.193  loss_sem_seg: 1.135  loss_center: 0.4651  loss_offset: 0.5295  time: 0.3535  data_time: 0.0285  lr: 0.0013013  max_mem: 7418M
[12/11 20:22:46 d2.utils.events]:  eta: 0:28:16  iter: 5179  total_loss: 2.293  loss_sem_seg: 1.228  loss_center: 0.5507  loss_offset: 0.5892  time: 0.3535  data_time: 0.0284  lr: 0.0012965  max_mem: 7418M
[12/11 20:22:53 d2.utils.events]:  eta: 0:28:08  iter: 5199  total_loss: 2.343  loss_sem_seg: 1.048  loss_center: 0.5869  loss_offset: 0.6275  time: 0.3535  data_time: 0.0295  lr: 0.0012916  max_mem: 7418M
[12/11 20:23:01 d2.utils.events]:  eta: 0:28:01  iter: 5219  total_loss: 2.016  loss_sem_seg: 0.9756  loss_center: 0.4804  loss_offset: 0.5156  time: 0.3535  data_time: 0.0286  lr: 0.0012868  max_mem: 7418M
[12/11 20:23:08 d2.utils.events]:  eta: 0:27:54  iter: 5239  total_loss: 2.165  loss_sem_seg: 1.019  loss_center: 0.5128  loss_offset: 0.5776  time: 0.3535  data_time: 0.0297  lr: 0.0012819  max_mem: 7418M
[12/11 20:23:15 d2.utils.events]:  eta: 0:27:47  iter: 5259  total_loss: 2.207  loss_sem_seg: 1.072  loss_center: 0.6899  loss_offset: 0.4376  time: 0.3535  data_time: 0.0283  lr: 0.0012771  max_mem: 7418M
[12/11 20:23:22 d2.utils.events]:  eta: 0:27:40  iter: 5279  total_loss: 2.246  loss_sem_seg: 1.05  loss_center: 0.516  loss_offset: 0.5548  time: 0.3535  data_time: 0.0283  lr: 0.0012722  max_mem: 7418M
[12/11 20:23:29 d2.utils.events]:  eta: 0:27:33  iter: 5299  total_loss: 2.17  loss_sem_seg: 1.162  loss_center: 0.5357  loss_offset: 0.5567  time: 0.3535  data_time: 0.0320  lr: 0.0012674  max_mem: 7418M
[12/11 20:23:36 d2.utils.events]:  eta: 0:27:26  iter: 5319  total_loss: 2.167  loss_sem_seg: 1.048  loss_center: 0.5332  loss_offset: 0.6006  time: 0.3535  data_time: 0.0276  lr: 0.0012625  max_mem: 7418M
[12/11 20:23:43 d2.utils.events]:  eta: 0:27:19  iter: 5339  total_loss: 2.237  loss_sem_seg: 1.064  loss_center: 0.5657  loss_offset: 0.444  time: 0.3535  data_time: 0.0276  lr: 0.0012577  max_mem: 7418M
[12/11 20:23:50 d2.utils.events]:  eta: 0:27:13  iter: 5359  total_loss: 2.313  loss_sem_seg: 1.151  loss_center: 0.5781  loss_offset: 0.5591  time: 0.3535  data_time: 0.0292  lr: 0.0012528  max_mem: 7418M
[12/11 20:23:57 d2.utils.events]:  eta: 0:27:06  iter: 5379  total_loss: 2.335  loss_sem_seg: 1.049  loss_center: 0.5046  loss_offset: 0.6523  time: 0.3535  data_time: 0.0279  lr: 0.001248  max_mem: 7418M
[12/11 20:24:04 d2.utils.events]:  eta: 0:26:59  iter: 5399  total_loss: 2.111  loss_sem_seg: 1.066  loss_center: 0.4258  loss_offset: 0.4875  time: 0.3535  data_time: 0.0293  lr: 0.0012431  max_mem: 7418M
[12/11 20:24:11 d2.utils.events]:  eta: 0:26:51  iter: 5419  total_loss: 2.079  loss_sem_seg: 0.9925  loss_center: 0.4111  loss_offset: 0.5757  time: 0.3535  data_time: 0.0267  lr: 0.0012382  max_mem: 7418M
[12/11 20:24:19 d2.utils.events]:  eta: 0:26:44  iter: 5439  total_loss: 2.139  loss_sem_seg: 1.142  loss_center: 0.5306  loss_offset: 0.5195  time: 0.3535  data_time: 0.0277  lr: 0.0012334  max_mem: 7418M
[12/11 20:24:26 d2.utils.events]:  eta: 0:26:37  iter: 5459  total_loss: 2.313  loss_sem_seg: 0.9712  loss_center: 0.5474  loss_offset: 0.5815  time: 0.3535  data_time: 0.0271  lr: 0.0012285  max_mem: 7418M
[12/11 20:24:33 d2.utils.events]:  eta: 0:26:30  iter: 5479  total_loss: 2.198  loss_sem_seg: 1.064  loss_center: 0.6054  loss_offset: 0.4899  time: 0.3535  data_time: 0.0275  lr: 0.0012236  max_mem: 7418M
[12/11 20:24:40 d2.utils.events]:  eta: 0:26:23  iter: 5499  total_loss: 2.293  loss_sem_seg: 1.145  loss_center: 0.6567  loss_offset: 0.4465  time: 0.3535  data_time: 0.0302  lr: 0.0012188  max_mem: 7418M
[12/11 20:24:47 d2.utils.events]:  eta: 0:26:16  iter: 5519  total_loss: 2.304  loss_sem_seg: 1.138  loss_center: 0.5547  loss_offset: 0.575  time: 0.3535  data_time: 0.0279  lr: 0.0012139  max_mem: 7418M
[12/11 20:24:54 d2.utils.events]:  eta: 0:26:09  iter: 5539  total_loss: 2.216  loss_sem_seg: 1.073  loss_center: 0.5564  loss_offset: 0.5586  time: 0.3535  data_time: 0.0292  lr: 0.001209  max_mem: 7418M
[12/11 20:25:01 d2.utils.events]:  eta: 0:26:02  iter: 5559  total_loss: 2.193  loss_sem_seg: 0.9564  loss_center: 0.6077  loss_offset: 0.5176  time: 0.3535  data_time: 0.0288  lr: 0.0012041  max_mem: 7418M
[12/11 20:25:08 d2.utils.events]:  eta: 0:25:55  iter: 5579  total_loss: 2.324  loss_sem_seg: 1.044  loss_center: 0.446  loss_offset: 0.6015  time: 0.3535  data_time: 0.0297  lr: 0.0011992  max_mem: 7418M
[12/11 20:25:15 d2.utils.events]:  eta: 0:25:48  iter: 5599  total_loss: 2.065  loss_sem_seg: 0.9752  loss_center: 0.5472  loss_offset: 0.525  time: 0.3535  data_time: 0.0279  lr: 0.0011944  max_mem: 7418M
[12/11 20:25:22 d2.utils.events]:  eta: 0:25:41  iter: 5619  total_loss: 2.13  loss_sem_seg: 1.003  loss_center: 0.5232  loss_offset: 0.5718  time: 0.3535  data_time: 0.0285  lr: 0.0011895  max_mem: 7418M
[12/11 20:25:29 d2.utils.events]:  eta: 0:25:34  iter: 5639  total_loss: 2.046  loss_sem_seg: 1.012  loss_center: 0.4841  loss_offset: 0.5813  time: 0.3535  data_time: 0.0302  lr: 0.0011846  max_mem: 7418M
[12/11 20:25:36 d2.utils.events]:  eta: 0:25:27  iter: 5659  total_loss: 2.085  loss_sem_seg: 1.012  loss_center: 0.5697  loss_offset: 0.5223  time: 0.3535  data_time: 0.0268  lr: 0.0011797  max_mem: 7418M
[12/11 20:25:44 d2.utils.events]:  eta: 0:25:20  iter: 5679  total_loss: 2.329  loss_sem_seg: 1.012  loss_center: 0.4695  loss_offset: 0.5607  time: 0.3535  data_time: 0.0282  lr: 0.0011748  max_mem: 7418M
[12/11 20:25:51 d2.utils.events]:  eta: 0:25:13  iter: 5699  total_loss: 2.232  loss_sem_seg: 0.9704  loss_center: 0.6288  loss_offset: 0.6489  time: 0.3535  data_time: 0.0299  lr: 0.0011699  max_mem: 7418M
[12/11 20:25:58 d2.utils.events]:  eta: 0:25:06  iter: 5719  total_loss: 2.119  loss_sem_seg: 0.9899  loss_center: 0.4913  loss_offset: 0.5629  time: 0.3535  data_time: 0.0286  lr: 0.001165  max_mem: 7418M
[12/11 20:26:05 d2.utils.events]:  eta: 0:24:59  iter: 5739  total_loss: 2.137  loss_sem_seg: 1.016  loss_center: 0.4916  loss_offset: 0.5131  time: 0.3535  data_time: 0.0278  lr: 0.0011601  max_mem: 7418M
[12/11 20:26:12 d2.utils.events]:  eta: 0:24:52  iter: 5759  total_loss: 2.086  loss_sem_seg: 1.001  loss_center: 0.4961  loss_offset: 0.5627  time: 0.3535  data_time: 0.0309  lr: 0.0011552  max_mem: 7418M
[12/11 20:26:19 d2.utils.events]:  eta: 0:24:45  iter: 5779  total_loss: 2.403  loss_sem_seg: 0.9783  loss_center: 0.5754  loss_offset: 0.6755  time: 0.3535  data_time: 0.0283  lr: 0.0011503  max_mem: 7418M
[12/11 20:26:26 d2.utils.events]:  eta: 0:24:38  iter: 5799  total_loss: 1.99  loss_sem_seg: 0.9396  loss_center: 0.4318  loss_offset: 0.5936  time: 0.3535  data_time: 0.0281  lr: 0.0011454  max_mem: 7418M
[12/11 20:26:33 d2.utils.events]:  eta: 0:24:31  iter: 5819  total_loss: 2.096  loss_sem_seg: 0.9768  loss_center: 0.4143  loss_offset: 0.5388  time: 0.3535  data_time: 0.0284  lr: 0.0011405  max_mem: 7418M
[12/11 20:26:40 d2.utils.events]:  eta: 0:24:25  iter: 5839  total_loss: 2.254  loss_sem_seg: 0.9938  loss_center: 0.4833  loss_offset: 0.5587  time: 0.3535  data_time: 0.0289  lr: 0.0011356  max_mem: 7418M
[12/11 20:26:47 d2.utils.events]:  eta: 0:24:19  iter: 5859  total_loss: 2.274  loss_sem_seg: 0.9271  loss_center: 0.5825  loss_offset: 0.5868  time: 0.3535  data_time: 0.0300  lr: 0.0011307  max_mem: 7418M
[12/11 20:26:55 d2.utils.events]:  eta: 0:24:11  iter: 5879  total_loss: 2.208  loss_sem_seg: 1.079  loss_center: 0.5463  loss_offset: 0.612  time: 0.3535  data_time: 0.0274  lr: 0.0011258  max_mem: 7418M
[12/11 20:27:02 d2.utils.events]:  eta: 0:24:05  iter: 5899  total_loss: 2.076  loss_sem_seg: 1.027  loss_center: 0.4422  loss_offset: 0.5445  time: 0.3536  data_time: 0.0291  lr: 0.0011208  max_mem: 7418M
[12/11 20:27:09 d2.utils.events]:  eta: 0:23:59  iter: 5919  total_loss: 2.226  loss_sem_seg: 0.9794  loss_center: 0.6677  loss_offset: 0.6411  time: 0.3535  data_time: 0.0291  lr: 0.0011159  max_mem: 7418M
[12/11 20:27:16 d2.utils.events]:  eta: 0:23:53  iter: 5939  total_loss: 2.34  loss_sem_seg: 1.132  loss_center: 0.5082  loss_offset: 0.6009  time: 0.3535  data_time: 0.0298  lr: 0.001111  max_mem: 7418M
[12/11 20:27:23 d2.utils.events]:  eta: 0:23:45  iter: 5959  total_loss: 2.034  loss_sem_seg: 0.8371  loss_center: 0.5269  loss_offset: 0.532  time: 0.3536  data_time: 0.0287  lr: 0.0011061  max_mem: 7418M
[12/11 20:27:30 d2.utils.events]:  eta: 0:23:38  iter: 5979  total_loss: 2.383  loss_sem_seg: 1.011  loss_center: 0.5122  loss_offset: 0.6601  time: 0.3536  data_time: 0.0273  lr: 0.0011011  max_mem: 7418M
[12/11 20:27:37 d2.utils.events]:  eta: 0:23:32  iter: 5999  total_loss: 2.253  loss_sem_seg: 1.127  loss_center: 0.4921  loss_offset: 0.6498  time: 0.3536  data_time: 0.0283  lr: 0.0010962  max_mem: 7418M
[12/11 20:27:44 d2.utils.events]:  eta: 0:23:25  iter: 6019  total_loss: 2.337  loss_sem_seg: 1.171  loss_center: 0.5807  loss_offset: 0.6508  time: 0.3536  data_time: 0.0273  lr: 0.0010913  max_mem: 7418M
[12/11 20:27:51 d2.utils.events]:  eta: 0:23:18  iter: 6039  total_loss: 2.101  loss_sem_seg: 0.9626  loss_center: 0.6051  loss_offset: 0.537  time: 0.3536  data_time: 0.0291  lr: 0.0010863  max_mem: 7418M
[12/11 20:27:58 d2.utils.events]:  eta: 0:23:11  iter: 6059  total_loss: 2.015  loss_sem_seg: 0.9046  loss_center: 0.4669  loss_offset: 0.5797  time: 0.3536  data_time: 0.0290  lr: 0.0010814  max_mem: 7418M
[12/11 20:28:06 d2.utils.events]:  eta: 0:23:04  iter: 6079  total_loss: 2.321  loss_sem_seg: 0.9646  loss_center: 0.5427  loss_offset: 0.603  time: 0.3536  data_time: 0.0284  lr: 0.0010765  max_mem: 7418M
[12/11 20:28:13 d2.utils.events]:  eta: 0:22:57  iter: 6099  total_loss: 2.226  loss_sem_seg: 1.134  loss_center: 0.4891  loss_offset: 0.5065  time: 0.3536  data_time: 0.0288  lr: 0.0010715  max_mem: 7418M
[12/11 20:28:20 d2.utils.events]:  eta: 0:22:50  iter: 6119  total_loss: 1.889  loss_sem_seg: 0.8726  loss_center: 0.5524  loss_offset: 0.5599  time: 0.3536  data_time: 0.0268  lr: 0.0010666  max_mem: 7418M
[12/11 20:28:27 d2.utils.events]:  eta: 0:22:43  iter: 6139  total_loss: 2.241  loss_sem_seg: 1.094  loss_center: 0.5174  loss_offset: 0.5721  time: 0.3536  data_time: 0.0287  lr: 0.0010616  max_mem: 7418M
[12/11 20:28:34 d2.utils.events]:  eta: 0:22:37  iter: 6159  total_loss: 2.333  loss_sem_seg: 1.072  loss_center: 0.7019  loss_offset: 0.5654  time: 0.3536  data_time: 0.0287  lr: 0.0010567  max_mem: 7418M
[12/11 20:28:41 d2.utils.events]:  eta: 0:22:29  iter: 6179  total_loss: 2.179  loss_sem_seg: 1.009  loss_center: 0.5916  loss_offset: 0.5844  time: 0.3536  data_time: 0.0276  lr: 0.0010517  max_mem: 7418M
[12/11 20:28:48 d2.utils.events]:  eta: 0:22:23  iter: 6199  total_loss: 2.067  loss_sem_seg: 0.9485  loss_center: 0.426  loss_offset: 0.5277  time: 0.3536  data_time: 0.0298  lr: 0.0010468  max_mem: 7418M
[12/11 20:28:55 d2.utils.events]:  eta: 0:22:16  iter: 6219  total_loss: 2.261  loss_sem_seg: 0.9995  loss_center: 0.5717  loss_offset: 0.6333  time: 0.3536  data_time: 0.0291  lr: 0.0010418  max_mem: 7418M
[12/11 20:29:02 d2.utils.events]:  eta: 0:22:09  iter: 6239  total_loss: 2.091  loss_sem_seg: 0.9574  loss_center: 0.5714  loss_offset: 0.6058  time: 0.3536  data_time: 0.0288  lr: 0.0010368  max_mem: 7418M
[12/11 20:29:09 d2.utils.events]:  eta: 0:22:02  iter: 6259  total_loss: 2.219  loss_sem_seg: 1.016  loss_center: 0.5736  loss_offset: 0.5713  time: 0.3536  data_time: 0.0285  lr: 0.0010319  max_mem: 7418M
[12/11 20:29:17 d2.utils.events]:  eta: 0:21:54  iter: 6279  total_loss: 2.154  loss_sem_seg: 1.006  loss_center: 0.5609  loss_offset: 0.5018  time: 0.3536  data_time: 0.0282  lr: 0.0010269  max_mem: 7418M
[12/11 20:29:24 d2.utils.events]:  eta: 0:21:47  iter: 6299  total_loss: 2.233  loss_sem_seg: 0.9124  loss_center: 0.4629  loss_offset: 0.5854  time: 0.3536  data_time: 0.0290  lr: 0.0010219  max_mem: 7418M
[12/11 20:29:31 d2.utils.events]:  eta: 0:21:40  iter: 6319  total_loss: 2.183  loss_sem_seg: 0.8756  loss_center: 0.7053  loss_offset: 0.6784  time: 0.3536  data_time: 0.0284  lr: 0.001017  max_mem: 7418M
[12/11 20:29:38 d2.utils.events]:  eta: 0:21:33  iter: 6339  total_loss: 2.283  loss_sem_seg: 1.1  loss_center: 0.5114  loss_offset: 0.5121  time: 0.3536  data_time: 0.0270  lr: 0.001012  max_mem: 7418M
[12/11 20:29:45 d2.utils.events]:  eta: 0:21:26  iter: 6359  total_loss: 2.165  loss_sem_seg: 0.941  loss_center: 0.5454  loss_offset: 0.6311  time: 0.3536  data_time: 0.0284  lr: 0.001007  max_mem: 7418M
[12/11 20:29:52 d2.utils.events]:  eta: 0:21:19  iter: 6379  total_loss: 2.117  loss_sem_seg: 0.8813  loss_center: 0.6581  loss_offset: 0.4056  time: 0.3536  data_time: 0.0287  lr: 0.001002  max_mem: 7418M
[12/11 20:29:59 d2.utils.events]:  eta: 0:21:12  iter: 6399  total_loss: 2.229  loss_sem_seg: 0.9872  loss_center: 0.5492  loss_offset: 0.4958  time: 0.3536  data_time: 0.0280  lr: 0.00099706  max_mem: 7418M
[12/11 20:30:06 d2.utils.events]:  eta: 0:21:05  iter: 6419  total_loss: 2.14  loss_sem_seg: 1.246  loss_center: 0.4641  loss_offset: 0.5224  time: 0.3536  data_time: 0.0300  lr: 0.00099207  max_mem: 7418M
[12/11 20:30:13 d2.utils.events]:  eta: 0:20:58  iter: 6439  total_loss: 1.965  loss_sem_seg: 1.027  loss_center: 0.4577  loss_offset: 0.6221  time: 0.3536  data_time: 0.0282  lr: 0.00098709  max_mem: 7418M
[12/11 20:30:20 d2.utils.events]:  eta: 0:20:51  iter: 6459  total_loss: 2.341  loss_sem_seg: 1.009  loss_center: 0.6485  loss_offset: 0.5327  time: 0.3536  data_time: 0.0298  lr: 0.00098209  max_mem: 7418M
[12/11 20:30:28 d2.utils.events]:  eta: 0:20:44  iter: 6479  total_loss: 2.485  loss_sem_seg: 1.006  loss_center: 0.5962  loss_offset: 0.7217  time: 0.3536  data_time: 0.0268  lr: 0.0009771  max_mem: 7418M
[12/11 20:30:35 d2.utils.events]:  eta: 0:20:37  iter: 6499  total_loss: 2.14  loss_sem_seg: 1.068  loss_center: 0.4285  loss_offset: 0.5188  time: 0.3536  data_time: 0.0306  lr: 0.0009721  max_mem: 7418M
[12/11 20:30:42 d2.utils.events]:  eta: 0:20:30  iter: 6519  total_loss: 2.148  loss_sem_seg: 0.9497  loss_center: 0.5292  loss_offset: 0.5486  time: 0.3536  data_time: 0.0280  lr: 0.00096711  max_mem: 7418M
[12/11 20:30:49 d2.utils.events]:  eta: 0:20:23  iter: 6539  total_loss: 2.155  loss_sem_seg: 0.9013  loss_center: 0.5319  loss_offset: 0.5631  time: 0.3536  data_time: 0.0296  lr: 0.0009621  max_mem: 7418M
[12/11 20:30:56 d2.utils.events]:  eta: 0:20:16  iter: 6559  total_loss: 2.08  loss_sem_seg: 1.034  loss_center: 0.5477  loss_offset: 0.5476  time: 0.3536  data_time: 0.0292  lr: 0.0009571  max_mem: 7418M
[12/11 20:31:03 d2.utils.events]:  eta: 0:20:09  iter: 6579  total_loss: 2.293  loss_sem_seg: 1.013  loss_center: 0.5283  loss_offset: 0.5754  time: 0.3536  data_time: 0.0300  lr: 0.00095209  max_mem: 7418M
[12/11 20:31:10 d2.utils.events]:  eta: 0:20:02  iter: 6599  total_loss: 2.164  loss_sem_seg: 0.9783  loss_center: 0.5492  loss_offset: 0.5526  time: 0.3536  data_time: 0.0294  lr: 0.00094708  max_mem: 7418M
[12/11 20:31:17 d2.utils.events]:  eta: 0:19:55  iter: 6619  total_loss: 2.188  loss_sem_seg: 0.8892  loss_center: 0.4496  loss_offset: 0.6058  time: 0.3536  data_time: 0.0280  lr: 0.00094206  max_mem: 7418M
[12/11 20:31:24 d2.utils.events]:  eta: 0:19:48  iter: 6639  total_loss: 2.146  loss_sem_seg: 1.03  loss_center: 0.5319  loss_offset: 0.4798  time: 0.3536  data_time: 0.0295  lr: 0.00093705  max_mem: 7418M
[12/11 20:31:31 d2.utils.events]:  eta: 0:19:41  iter: 6659  total_loss: 2.154  loss_sem_seg: 1.011  loss_center: 0.5213  loss_offset: 0.5354  time: 0.3536  data_time: 0.0284  lr: 0.00093203  max_mem: 7418M
[12/11 20:31:39 d2.utils.events]:  eta: 0:19:34  iter: 6679  total_loss: 1.956  loss_sem_seg: 1.147  loss_center: 0.3988  loss_offset: 0.4226  time: 0.3536  data_time: 0.0289  lr: 0.000927  max_mem: 7418M
[12/11 20:31:46 d2.utils.events]:  eta: 0:19:27  iter: 6699  total_loss: 1.998  loss_sem_seg: 0.8236  loss_center: 0.5108  loss_offset: 0.4574  time: 0.3536  data_time: 0.0279  lr: 0.00092198  max_mem: 7418M
[12/11 20:31:53 d2.utils.events]:  eta: 0:19:20  iter: 6719  total_loss: 2.049  loss_sem_seg: 0.9303  loss_center: 0.5302  loss_offset: 0.5347  time: 0.3536  data_time: 0.0293  lr: 0.00091695  max_mem: 7418M
[12/11 20:32:00 d2.utils.events]:  eta: 0:19:13  iter: 6739  total_loss: 2.195  loss_sem_seg: 1.03  loss_center: 0.4873  loss_offset: 0.5263  time: 0.3536  data_time: 0.0282  lr: 0.00091192  max_mem: 7418M
[12/11 20:32:07 d2.utils.events]:  eta: 0:19:06  iter: 6759  total_loss: 2.171  loss_sem_seg: 1.074  loss_center: 0.6265  loss_offset: 0.4212  time: 0.3536  data_time: 0.0289  lr: 0.00090688  max_mem: 7418M
[12/11 20:32:14 d2.utils.events]:  eta: 0:18:58  iter: 6779  total_loss: 2.344  loss_sem_seg: 1.368  loss_center: 0.5019  loss_offset: 0.5244  time: 0.3536  data_time: 0.0282  lr: 0.00090184  max_mem: 7418M
[12/11 20:32:21 d2.utils.events]:  eta: 0:18:52  iter: 6799  total_loss: 2.207  loss_sem_seg: 1.018  loss_center: 0.5605  loss_offset: 0.576  time: 0.3537  data_time: 0.0298  lr: 0.0008968  max_mem: 7418M
[12/11 20:32:28 d2.utils.events]:  eta: 0:18:45  iter: 6819  total_loss: 2.116  loss_sem_seg: 0.9428  loss_center: 0.5241  loss_offset: 0.4917  time: 0.3537  data_time: 0.0302  lr: 0.00089176  max_mem: 7418M
[12/11 20:32:35 d2.utils.events]:  eta: 0:18:38  iter: 6839  total_loss: 2.007  loss_sem_seg: 0.9561  loss_center: 0.425  loss_offset: 0.4951  time: 0.3537  data_time: 0.0288  lr: 0.00088671  max_mem: 7418M
[12/11 20:32:42 d2.utils.events]:  eta: 0:18:31  iter: 6859  total_loss: 2.15  loss_sem_seg: 0.9086  loss_center: 0.5094  loss_offset: 0.6268  time: 0.3537  data_time: 0.0279  lr: 0.00088166  max_mem: 7418M
[12/11 20:32:50 d2.utils.events]:  eta: 0:18:24  iter: 6879  total_loss: 2.215  loss_sem_seg: 1.169  loss_center: 0.5185  loss_offset: 0.5547  time: 0.3537  data_time: 0.0283  lr: 0.00087661  max_mem: 7418M
[12/11 20:32:57 d2.utils.events]:  eta: 0:18:16  iter: 6899  total_loss: 2.236  loss_sem_seg: 1.083  loss_center: 0.4736  loss_offset: 0.6808  time: 0.3537  data_time: 0.0284  lr: 0.00087155  max_mem: 7418M
[12/11 20:33:04 d2.utils.events]:  eta: 0:18:09  iter: 6919  total_loss: 1.768  loss_sem_seg: 0.8351  loss_center: 0.5629  loss_offset: 0.4265  time: 0.3537  data_time: 0.0296  lr: 0.00086649  max_mem: 7418M
[12/11 20:33:11 d2.utils.events]:  eta: 0:18:02  iter: 6939  total_loss: 2.201  loss_sem_seg: 1.072  loss_center: 0.3955  loss_offset: 0.6877  time: 0.3537  data_time: 0.0289  lr: 0.00086142  max_mem: 7418M
[12/11 20:33:18 d2.utils.events]:  eta: 0:17:55  iter: 6959  total_loss: 2.066  loss_sem_seg: 0.8523  loss_center: 0.5306  loss_offset: 0.509  time: 0.3537  data_time: 0.0289  lr: 0.00085636  max_mem: 7418M
[12/11 20:33:25 d2.utils.events]:  eta: 0:17:48  iter: 6979  total_loss: 1.909  loss_sem_seg: 0.9555  loss_center: 0.587  loss_offset: 0.4478  time: 0.3537  data_time: 0.0286  lr: 0.00085129  max_mem: 7418M
[12/11 20:33:32 d2.utils.events]:  eta: 0:17:40  iter: 6999  total_loss: 2.096  loss_sem_seg: 0.9642  loss_center: 0.4521  loss_offset: 0.5823  time: 0.3537  data_time: 0.0280  lr: 0.00084621  max_mem: 7418M
[12/11 20:33:39 d2.utils.events]:  eta: 0:17:34  iter: 7019  total_loss: 2.054  loss_sem_seg: 0.8467  loss_center: 0.5871  loss_offset: 0.4579  time: 0.3537  data_time: 0.0310  lr: 0.00084114  max_mem: 7418M
[12/11 20:33:46 d2.utils.events]:  eta: 0:17:27  iter: 7039  total_loss: 2.179  loss_sem_seg: 0.9745  loss_center: 0.4485  loss_offset: 0.5485  time: 0.3537  data_time: 0.0287  lr: 0.00083605  max_mem: 7418M
[12/11 20:33:53 d2.utils.events]:  eta: 0:17:20  iter: 7059  total_loss: 1.973  loss_sem_seg: 0.9797  loss_center: 0.5356  loss_offset: 0.452  time: 0.3537  data_time: 0.0292  lr: 0.00083097  max_mem: 7418M
[12/11 20:34:01 d2.utils.events]:  eta: 0:17:13  iter: 7079  total_loss: 2.2  loss_sem_seg: 0.9914  loss_center: 0.6257  loss_offset: 0.5155  time: 0.3537  data_time: 0.0293  lr: 0.00082588  max_mem: 7418M
[12/11 20:34:08 d2.utils.events]:  eta: 0:17:06  iter: 7099  total_loss: 1.869  loss_sem_seg: 0.8982  loss_center: 0.4914  loss_offset: 0.3871  time: 0.3537  data_time: 0.0292  lr: 0.00082079  max_mem: 7418M
[12/11 20:34:15 d2.utils.events]:  eta: 0:16:59  iter: 7119  total_loss: 2.391  loss_sem_seg: 1.153  loss_center: 0.6658  loss_offset: 0.5097  time: 0.3537  data_time: 0.0301  lr: 0.0008157  max_mem: 7418M
[12/11 20:34:22 d2.utils.events]:  eta: 0:16:52  iter: 7139  total_loss: 2.008  loss_sem_seg: 0.9387  loss_center: 0.5557  loss_offset: 0.4303  time: 0.3537  data_time: 0.0282  lr: 0.0008106  max_mem: 7418M
[12/11 20:34:29 d2.utils.events]:  eta: 0:16:45  iter: 7159  total_loss: 2.114  loss_sem_seg: 0.952  loss_center: 0.4888  loss_offset: 0.5776  time: 0.3537  data_time: 0.0328  lr: 0.0008055  max_mem: 7418M
[12/11 20:34:36 d2.utils.events]:  eta: 0:16:38  iter: 7179  total_loss: 2.199  loss_sem_seg: 0.8875  loss_center: 0.6011  loss_offset: 0.4992  time: 0.3537  data_time: 0.0296  lr: 0.00080039  max_mem: 7418M
[12/11 20:34:43 d2.utils.events]:  eta: 0:16:30  iter: 7199  total_loss: 2.038  loss_sem_seg: 0.9238  loss_center: 0.4134  loss_offset: 0.5565  time: 0.3537  data_time: 0.0271  lr: 0.00079528  max_mem: 7418M
[12/11 20:34:50 d2.utils.events]:  eta: 0:16:23  iter: 7219  total_loss: 2.056  loss_sem_seg: 0.9546  loss_center: 0.5417  loss_offset: 0.519  time: 0.3537  data_time: 0.0285  lr: 0.00079017  max_mem: 7418M
[12/11 20:34:57 d2.utils.events]:  eta: 0:16:16  iter: 7239  total_loss: 2.102  loss_sem_seg: 0.9984  loss_center: 0.5909  loss_offset: 0.5357  time: 0.3537  data_time: 0.0293  lr: 0.00078505  max_mem: 7418M
[12/11 20:35:05 d2.utils.events]:  eta: 0:16:09  iter: 7259  total_loss: 2.097  loss_sem_seg: 0.9727  loss_center: 0.454  loss_offset: 0.5286  time: 0.3537  data_time: 0.0306  lr: 0.00077993  max_mem: 7418M
[12/11 20:35:12 d2.utils.events]:  eta: 0:16:02  iter: 7279  total_loss: 2.005  loss_sem_seg: 0.8271  loss_center: 0.5417  loss_offset: 0.5093  time: 0.3537  data_time: 0.0287  lr: 0.00077481  max_mem: 7418M
[12/11 20:35:19 d2.utils.events]:  eta: 0:15:55  iter: 7299  total_loss: 2.232  loss_sem_seg: 1.121  loss_center: 0.4911  loss_offset: 0.5011  time: 0.3537  data_time: 0.0289  lr: 0.00076968  max_mem: 7418M
[12/11 20:35:26 d2.utils.events]:  eta: 0:15:48  iter: 7319  total_loss: 2.205  loss_sem_seg: 0.9849  loss_center: 0.6082  loss_offset: 0.5196  time: 0.3537  data_time: 0.0298  lr: 0.00076455  max_mem: 7418M
[12/11 20:35:33 d2.utils.events]:  eta: 0:15:41  iter: 7339  total_loss: 2.011  loss_sem_seg: 1.088  loss_center: 0.4301  loss_offset: 0.4796  time: 0.3538  data_time: 0.0289  lr: 0.00075942  max_mem: 7418M
[12/11 20:35:40 d2.utils.events]:  eta: 0:15:34  iter: 7359  total_loss: 2.336  loss_sem_seg: 1.054  loss_center: 0.5496  loss_offset: 0.5661  time: 0.3538  data_time: 0.0286  lr: 0.00075428  max_mem: 7418M
[12/11 20:35:47 d2.utils.events]:  eta: 0:15:27  iter: 7379  total_loss: 2.285  loss_sem_seg: 1.04  loss_center: 0.5152  loss_offset: 0.5599  time: 0.3538  data_time: 0.0302  lr: 0.00074914  max_mem: 7418M
[12/11 20:35:54 d2.utils.events]:  eta: 0:15:20  iter: 7399  total_loss: 1.978  loss_sem_seg: 0.9631  loss_center: 0.4264  loss_offset: 0.4666  time: 0.3538  data_time: 0.0304  lr: 0.00074399  max_mem: 7418M
[12/11 20:36:02 d2.utils.events]:  eta: 0:15:13  iter: 7419  total_loss: 2.073  loss_sem_seg: 0.9174  loss_center: 0.5841  loss_offset: 0.517  time: 0.3538  data_time: 0.0297  lr: 0.00073884  max_mem: 7418M
[12/11 20:36:09 d2.utils.events]:  eta: 0:15:06  iter: 7439  total_loss: 2.124  loss_sem_seg: 0.9666  loss_center: 0.5926  loss_offset: 0.5199  time: 0.3538  data_time: 0.0311  lr: 0.00073368  max_mem: 7418M
[12/11 20:36:16 d2.utils.events]:  eta: 0:14:59  iter: 7459  total_loss: 2.122  loss_sem_seg: 0.9025  loss_center: 0.5231  loss_offset: 0.4344  time: 0.3538  data_time: 0.0292  lr: 0.00072852  max_mem: 7418M
[12/11 20:36:23 d2.utils.events]:  eta: 0:14:52  iter: 7479  total_loss: 2.034  loss_sem_seg: 1.024  loss_center: 0.5945  loss_offset: 0.4883  time: 0.3538  data_time: 0.0288  lr: 0.00072336  max_mem: 7418M
[12/11 20:36:30 d2.utils.events]:  eta: 0:14:45  iter: 7499  total_loss: 2.231  loss_sem_seg: 0.9867  loss_center: 0.533  loss_offset: 0.5908  time: 0.3538  data_time: 0.0276  lr: 0.00071819  max_mem: 7418M
[12/11 20:36:37 d2.utils.events]:  eta: 0:14:37  iter: 7519  total_loss: 1.897  loss_sem_seg: 0.8756  loss_center: 0.476  loss_offset: 0.468  time: 0.3538  data_time: 0.0271  lr: 0.00071302  max_mem: 7418M
[12/11 20:36:44 d2.utils.events]:  eta: 0:14:30  iter: 7539  total_loss: 1.879  loss_sem_seg: 0.9558  loss_center: 0.4551  loss_offset: 0.5267  time: 0.3538  data_time: 0.0277  lr: 0.00070785  max_mem: 7418M
[12/11 20:36:51 d2.utils.events]:  eta: 0:14:23  iter: 7559  total_loss: 2.112  loss_sem_seg: 0.9664  loss_center: 0.5402  loss_offset: 0.5378  time: 0.3538  data_time: 0.0298  lr: 0.00070267  max_mem: 7418M
[12/11 20:36:58 d2.utils.events]:  eta: 0:14:16  iter: 7579  total_loss: 2.108  loss_sem_seg: 0.9879  loss_center: 0.4388  loss_offset: 0.5376  time: 0.3538  data_time: 0.0299  lr: 0.00069749  max_mem: 7418M
[12/11 20:37:06 d2.utils.events]:  eta: 0:14:09  iter: 7599  total_loss: 2.32  loss_sem_seg: 1.281  loss_center: 0.5107  loss_offset: 0.553  time: 0.3538  data_time: 0.0309  lr: 0.0006923  max_mem: 7418M
[12/11 20:37:13 d2.utils.events]:  eta: 0:14:02  iter: 7619  total_loss: 2.328  loss_sem_seg: 0.9671  loss_center: 0.5926  loss_offset: 0.5642  time: 0.3538  data_time: 0.0278  lr: 0.00068711  max_mem: 7418M
[12/11 20:37:20 d2.utils.events]:  eta: 0:13:55  iter: 7639  total_loss: 2.233  loss_sem_seg: 0.96  loss_center: 0.5594  loss_offset: 0.5714  time: 0.3538  data_time: 0.0274  lr: 0.00068191  max_mem: 7418M
[12/11 20:37:27 d2.utils.events]:  eta: 0:13:48  iter: 7659  total_loss: 2.065  loss_sem_seg: 0.8322  loss_center: 0.448  loss_offset: 0.5453  time: 0.3538  data_time: 0.0292  lr: 0.00067671  max_mem: 7418M
[12/11 20:37:34 d2.utils.events]:  eta: 0:13:41  iter: 7679  total_loss: 2.108  loss_sem_seg: 0.8797  loss_center: 0.661  loss_offset: 0.5095  time: 0.3538  data_time: 0.0286  lr: 0.0006715  max_mem: 7418M
[12/11 20:37:41 d2.utils.events]:  eta: 0:13:34  iter: 7699  total_loss: 2.048  loss_sem_seg: 0.889  loss_center: 0.6086  loss_offset: 0.5122  time: 0.3538  data_time: 0.0277  lr: 0.00066629  max_mem: 7418M
[12/11 20:37:48 d2.utils.events]:  eta: 0:13:27  iter: 7719  total_loss: 2.104  loss_sem_seg: 1.053  loss_center: 0.4954  loss_offset: 0.5274  time: 0.3538  data_time: 0.0290  lr: 0.00066108  max_mem: 7418M
[12/11 20:37:55 d2.utils.events]:  eta: 0:13:21  iter: 7739  total_loss: 2.167  loss_sem_seg: 0.9566  loss_center: 0.4498  loss_offset: 0.5322  time: 0.3538  data_time: 0.0302  lr: 0.00065586  max_mem: 7418M
[12/11 20:38:03 d2.utils.events]:  eta: 0:13:14  iter: 7759  total_loss: 2.158  loss_sem_seg: 1.061  loss_center: 0.512  loss_offset: 0.5622  time: 0.3538  data_time: 0.0300  lr: 0.00065064  max_mem: 7418M
[12/11 20:38:10 d2.utils.events]:  eta: 0:13:06  iter: 7779  total_loss: 2.115  loss_sem_seg: 0.8486  loss_center: 0.5988  loss_offset: 0.5612  time: 0.3538  data_time: 0.0286  lr: 0.00064541  max_mem: 7418M
[12/11 20:38:17 d2.utils.events]:  eta: 0:12:59  iter: 7799  total_loss: 2.003  loss_sem_seg: 1.116  loss_center: 0.556  loss_offset: 0.4236  time: 0.3538  data_time: 0.0277  lr: 0.00064017  max_mem: 7418M
[12/11 20:38:24 d2.utils.events]:  eta: 0:12:51  iter: 7819  total_loss: 2.101  loss_sem_seg: 0.8943  loss_center: 0.6632  loss_offset: 0.6245  time: 0.3538  data_time: 0.0288  lr: 0.00063494  max_mem: 7418M
[12/11 20:38:31 d2.utils.events]:  eta: 0:12:44  iter: 7839  total_loss: 2.085  loss_sem_seg: 0.9479  loss_center: 0.39  loss_offset: 0.6349  time: 0.3538  data_time: 0.0294  lr: 0.00062969  max_mem: 7418M
[12/11 20:38:38 d2.utils.events]:  eta: 0:12:37  iter: 7859  total_loss: 2.149  loss_sem_seg: 0.9853  loss_center: 0.5661  loss_offset: 0.5522  time: 0.3538  data_time: 0.0279  lr: 0.00062445  max_mem: 7418M
[12/11 20:38:45 d2.utils.events]:  eta: 0:12:30  iter: 7879  total_loss: 1.828  loss_sem_seg: 0.842  loss_center: 0.407  loss_offset: 0.4618  time: 0.3538  data_time: 0.0296  lr: 0.00061919  max_mem: 7418M
[12/11 20:38:52 d2.utils.events]:  eta: 0:12:24  iter: 7899  total_loss: 2.158  loss_sem_seg: 1.142  loss_center: 0.5242  loss_offset: 0.5906  time: 0.3539  data_time: 0.0301  lr: 0.00061394  max_mem: 7418M
[12/11 20:38:59 d2.utils.events]:  eta: 0:12:16  iter: 7919  total_loss: 2.139  loss_sem_seg: 0.9829  loss_center: 0.4055  loss_offset: 0.5941  time: 0.3539  data_time: 0.0295  lr: 0.00060867  max_mem: 7418M
[12/11 20:39:06 d2.utils.events]:  eta: 0:12:09  iter: 7939  total_loss: 1.906  loss_sem_seg: 0.9113  loss_center: 0.4894  loss_offset: 0.4571  time: 0.3539  data_time: 0.0294  lr: 0.00060341  max_mem: 7418M
[12/11 20:39:14 d2.utils.events]:  eta: 0:12:02  iter: 7959  total_loss: 2.182  loss_sem_seg: 0.9403  loss_center: 0.5903  loss_offset: 0.5252  time: 0.3539  data_time: 0.0271  lr: 0.00059813  max_mem: 7418M
[12/11 20:39:21 d2.utils.events]:  eta: 0:11:55  iter: 7979  total_loss: 2.112  loss_sem_seg: 1.022  loss_center: 0.5354  loss_offset: 0.5493  time: 0.3539  data_time: 0.0290  lr: 0.00059286  max_mem: 7418M
[12/11 20:39:28 d2.utils.events]:  eta: 0:11:48  iter: 7999  total_loss: 2.286  loss_sem_seg: 0.8636  loss_center: 0.4773  loss_offset: 0.5224  time: 0.3539  data_time: 0.0288  lr: 0.00058757  max_mem: 7418M
[12/11 20:39:35 d2.utils.events]:  eta: 0:11:41  iter: 8019  total_loss: 2.148  loss_sem_seg: 1.001  loss_center: 0.4681  loss_offset: 0.5569  time: 0.3539  data_time: 0.0311  lr: 0.00058229  max_mem: 7418M
[12/11 20:39:42 d2.utils.events]:  eta: 0:11:34  iter: 8039  total_loss: 2.123  loss_sem_seg: 0.9285  loss_center: 0.4333  loss_offset: 0.6703  time: 0.3539  data_time: 0.0285  lr: 0.00057699  max_mem: 7418M
[12/11 20:39:49 d2.utils.events]:  eta: 0:11:26  iter: 8059  total_loss: 2.181  loss_sem_seg: 1.075  loss_center: 0.5572  loss_offset: 0.6143  time: 0.3539  data_time: 0.0279  lr: 0.00057169  max_mem: 7418M
[12/11 20:39:56 d2.utils.events]:  eta: 0:11:19  iter: 8079  total_loss: 2.068  loss_sem_seg: 0.9634  loss_center: 0.4912  loss_offset: 0.5538  time: 0.3539  data_time: 0.0285  lr: 0.00056639  max_mem: 7418M
[12/11 20:40:03 d2.utils.events]:  eta: 0:11:12  iter: 8099  total_loss: 2.098  loss_sem_seg: 0.861  loss_center: 0.4186  loss_offset: 0.6738  time: 0.3539  data_time: 0.0292  lr: 0.00056108  max_mem: 7418M
[12/11 20:40:10 d2.utils.events]:  eta: 0:11:05  iter: 8119  total_loss: 2.036  loss_sem_seg: 0.9238  loss_center: 0.4816  loss_offset: 0.5884  time: 0.3539  data_time: 0.0279  lr: 0.00055576  max_mem: 7418M
[12/11 20:40:17 d2.utils.events]:  eta: 0:10:58  iter: 8139  total_loss: 1.843  loss_sem_seg: 0.8883  loss_center: 0.4373  loss_offset: 0.5058  time: 0.3539  data_time: 0.0296  lr: 0.00055044  max_mem: 7418M
[12/11 20:40:25 d2.utils.events]:  eta: 0:10:50  iter: 8159  total_loss: 2.15  loss_sem_seg: 0.9742  loss_center: 0.5455  loss_offset: 0.4912  time: 0.3539  data_time: 0.0277  lr: 0.00054512  max_mem: 7418M
[12/11 20:40:32 d2.utils.events]:  eta: 0:10:43  iter: 8179  total_loss: 1.967  loss_sem_seg: 0.8883  loss_center: 0.4153  loss_offset: 0.6146  time: 0.3539  data_time: 0.0315  lr: 0.00053978  max_mem: 7418M
[12/11 20:40:39 d2.utils.events]:  eta: 0:10:37  iter: 8199  total_loss: 2.067  loss_sem_seg: 0.9348  loss_center: 0.4364  loss_offset: 0.5748  time: 0.3539  data_time: 0.0305  lr: 0.00053444  max_mem: 7418M
[12/11 20:40:46 d2.utils.events]:  eta: 0:10:30  iter: 8219  total_loss: 2.151  loss_sem_seg: 0.9554  loss_center: 0.4411  loss_offset: 0.609  time: 0.3539  data_time: 0.0285  lr: 0.0005291  max_mem: 7418M
[12/11 20:40:53 d2.utils.events]:  eta: 0:10:23  iter: 8239  total_loss: 2.072  loss_sem_seg: 0.9869  loss_center: 0.457  loss_offset: 0.6057  time: 0.3539  data_time: 0.0288  lr: 0.00052375  max_mem: 7418M
[12/11 20:41:00 d2.utils.events]:  eta: 0:10:16  iter: 8259  total_loss: 2.158  loss_sem_seg: 0.9011  loss_center: 0.4157  loss_offset: 0.6253  time: 0.3539  data_time: 0.0281  lr: 0.00051839  max_mem: 7418M
[12/11 20:41:07 d2.utils.events]:  eta: 0:10:08  iter: 8279  total_loss: 2.115  loss_sem_seg: 1.032  loss_center: 0.4455  loss_offset: 0.6325  time: 0.3539  data_time: 0.0294  lr: 0.00051303  max_mem: 7418M
[12/11 20:41:14 d2.utils.events]:  eta: 0:10:01  iter: 8299  total_loss: 2.215  loss_sem_seg: 0.9529  loss_center: 0.4931  loss_offset: 0.5947  time: 0.3539  data_time: 0.0281  lr: 0.00050766  max_mem: 7418M
[12/11 20:41:22 d2.utils.events]:  eta: 0:09:54  iter: 8319  total_loss: 2.076  loss_sem_seg: 0.7566  loss_center: 0.4664  loss_offset: 0.4571  time: 0.3539  data_time: 0.0281  lr: 0.00050229  max_mem: 7418M
[12/11 20:41:29 d2.utils.events]:  eta: 0:09:47  iter: 8339  total_loss: 1.988  loss_sem_seg: 0.8552  loss_center: 0.4741  loss_offset: 0.4663  time: 0.3539  data_time: 0.0291  lr: 0.0004969  max_mem: 7418M
[12/11 20:41:36 d2.utils.events]:  eta: 0:09:40  iter: 8359  total_loss: 1.869  loss_sem_seg: 0.7706  loss_center: 0.5521  loss_offset: 0.5386  time: 0.3539  data_time: 0.0313  lr: 0.00049152  max_mem: 7418M
[12/11 20:41:43 d2.utils.events]:  eta: 0:09:33  iter: 8379  total_loss: 2.161  loss_sem_seg: 1.089  loss_center: 0.4861  loss_offset: 0.6032  time: 0.3539  data_time: 0.0295  lr: 0.00048612  max_mem: 7418M
[12/11 20:41:50 d2.utils.events]:  eta: 0:09:26  iter: 8399  total_loss: 1.904  loss_sem_seg: 0.7835  loss_center: 0.4251  loss_offset: 0.5675  time: 0.3539  data_time: 0.0288  lr: 0.00048072  max_mem: 7418M
[12/11 20:41:57 d2.utils.events]:  eta: 0:09:19  iter: 8419  total_loss: 1.955  loss_sem_seg: 0.8139  loss_center: 0.5792  loss_offset: 0.5158  time: 0.3539  data_time: 0.0287  lr: 0.00047531  max_mem: 7418M
[12/11 20:42:04 d2.utils.events]:  eta: 0:09:12  iter: 8439  total_loss: 2.088  loss_sem_seg: 0.9869  loss_center: 0.4829  loss_offset: 0.5436  time: 0.3539  data_time: 0.0291  lr: 0.0004699  max_mem: 7418M
[12/11 20:42:11 d2.utils.events]:  eta: 0:09:05  iter: 8459  total_loss: 1.854  loss_sem_seg: 0.7982  loss_center: 0.5027  loss_offset: 0.4118  time: 0.3539  data_time: 0.0295  lr: 0.00046448  max_mem: 7418M
[12/11 20:42:18 d2.utils.events]:  eta: 0:08:58  iter: 8479  total_loss: 2.128  loss_sem_seg: 0.9715  loss_center: 0.4059  loss_offset: 0.5258  time: 0.3539  data_time: 0.0287  lr: 0.00045905  max_mem: 7418M
[12/11 20:42:25 d2.utils.events]:  eta: 0:08:50  iter: 8499  total_loss: 1.962  loss_sem_seg: 0.8904  loss_center: 0.5335  loss_offset: 0.5546  time: 0.3539  data_time: 0.0274  lr: 0.00045361  max_mem: 7418M
[12/11 20:42:33 d2.utils.events]:  eta: 0:08:43  iter: 8519  total_loss: 1.858  loss_sem_seg: 0.8714  loss_center: 0.4448  loss_offset: 0.4441  time: 0.3539  data_time: 0.0281  lr: 0.00044817  max_mem: 7418M
[12/11 20:42:40 d2.utils.events]:  eta: 0:08:36  iter: 8539  total_loss: 2.006  loss_sem_seg: 0.8993  loss_center: 0.544  loss_offset: 0.5241  time: 0.3539  data_time: 0.0277  lr: 0.00044272  max_mem: 7418M
[12/11 20:42:47 d2.utils.events]:  eta: 0:08:29  iter: 8559  total_loss: 2.03  loss_sem_seg: 1.006  loss_center: 0.5253  loss_offset: 0.5127  time: 0.3539  data_time: 0.0296  lr: 0.00043726  max_mem: 7418M
[12/11 20:42:54 d2.utils.events]:  eta: 0:08:22  iter: 8579  total_loss: 2.055  loss_sem_seg: 0.8798  loss_center: 0.5357  loss_offset: 0.6294  time: 0.3539  data_time: 0.0289  lr: 0.00043179  max_mem: 7418M
[12/11 20:43:01 d2.utils.events]:  eta: 0:08:15  iter: 8599  total_loss: 2.095  loss_sem_seg: 0.8481  loss_center: 0.4304  loss_offset: 0.5158  time: 0.3539  data_time: 0.0281  lr: 0.00042632  max_mem: 7418M
[12/11 20:43:08 d2.utils.events]:  eta: 0:08:08  iter: 8619  total_loss: 1.882  loss_sem_seg: 0.8634  loss_center: 0.4806  loss_offset: 0.5023  time: 0.3539  data_time: 0.0292  lr: 0.00042084  max_mem: 7418M
[12/11 20:43:15 d2.utils.events]:  eta: 0:08:01  iter: 8639  total_loss: 2.247  loss_sem_seg: 0.971  loss_center: 0.5731  loss_offset: 0.4783  time: 0.3539  data_time: 0.0294  lr: 0.00041535  max_mem: 7418M
[12/11 20:43:22 d2.utils.events]:  eta: 0:07:54  iter: 8659  total_loss: 1.912  loss_sem_seg: 0.8989  loss_center: 0.4544  loss_offset: 0.4888  time: 0.3539  data_time: 0.0275  lr: 0.00040985  max_mem: 7418M
[12/11 20:43:29 d2.utils.events]:  eta: 0:07:46  iter: 8679  total_loss: 1.94  loss_sem_seg: 0.9344  loss_center: 0.4381  loss_offset: 0.4376  time: 0.3539  data_time: 0.0285  lr: 0.00040435  max_mem: 7418M
[12/11 20:43:36 d2.utils.events]:  eta: 0:07:39  iter: 8699  total_loss: 2.01  loss_sem_seg: 0.8964  loss_center: 0.5687  loss_offset: 0.5255  time: 0.3539  data_time: 0.0303  lr: 0.00039883  max_mem: 7418M
[12/11 20:43:44 d2.utils.events]:  eta: 0:07:32  iter: 8719  total_loss: 2.097  loss_sem_seg: 0.9267  loss_center: 0.5045  loss_offset: 0.5591  time: 0.3539  data_time: 0.0302  lr: 0.00039331  max_mem: 7418M
[12/11 20:43:51 d2.utils.events]:  eta: 0:07:25  iter: 8739  total_loss: 2.053  loss_sem_seg: 1.071  loss_center: 0.4721  loss_offset: 0.4523  time: 0.3539  data_time: 0.0291  lr: 0.00038778  max_mem: 7418M
[12/11 20:43:58 d2.utils.events]:  eta: 0:07:18  iter: 8759  total_loss: 2.049  loss_sem_seg: 0.9632  loss_center: 0.5533  loss_offset: 0.495  time: 0.3540  data_time: 0.0301  lr: 0.00038224  max_mem: 7418M
[12/11 20:44:05 d2.utils.events]:  eta: 0:07:11  iter: 8779  total_loss: 2.202  loss_sem_seg: 0.8744  loss_center: 0.5978  loss_offset: 0.6072  time: 0.3540  data_time: 0.0293  lr: 0.00037669  max_mem: 7418M
[12/11 20:44:12 d2.utils.events]:  eta: 0:07:04  iter: 8799  total_loss: 2.076  loss_sem_seg: 0.9636  loss_center: 0.5337  loss_offset: 0.5365  time: 0.3540  data_time: 0.0283  lr: 0.00037113  max_mem: 7418M
[12/11 20:44:19 d2.utils.events]:  eta: 0:06:57  iter: 8819  total_loss: 1.983  loss_sem_seg: 0.8273  loss_center: 0.5151  loss_offset: 0.4845  time: 0.3540  data_time: 0.0281  lr: 0.00036557  max_mem: 7418M
[12/11 20:44:26 d2.utils.events]:  eta: 0:06:50  iter: 8839  total_loss: 2.183  loss_sem_seg: 0.989  loss_center: 0.47  loss_offset: 0.5496  time: 0.3540  data_time: 0.0300  lr: 0.00035999  max_mem: 7418M
[12/11 20:44:34 d2.utils.events]:  eta: 0:06:43  iter: 8859  total_loss: 1.954  loss_sem_seg: 1.056  loss_center: 0.4389  loss_offset: 0.4462  time: 0.3540  data_time: 0.0312  lr: 0.0003544  max_mem: 7418M
[12/11 20:44:41 d2.utils.events]:  eta: 0:06:36  iter: 8879  total_loss: 2.147  loss_sem_seg: 0.9444  loss_center: 0.5451  loss_offset: 0.4984  time: 0.3540  data_time: 0.0290  lr: 0.00034881  max_mem: 7418M
[12/11 20:44:48 d2.utils.events]:  eta: 0:06:29  iter: 8899  total_loss: 2.356  loss_sem_seg: 0.9969  loss_center: 0.5108  loss_offset: 0.5573  time: 0.3540  data_time: 0.0309  lr: 0.0003432  max_mem: 7418M
[12/11 20:44:55 d2.utils.events]:  eta: 0:06:22  iter: 8919  total_loss: 1.945  loss_sem_seg: 1.004  loss_center: 0.4238  loss_offset: 0.5092  time: 0.3540  data_time: 0.0290  lr: 0.00033758  max_mem: 7418M
[12/11 20:45:02 d2.utils.events]:  eta: 0:06:15  iter: 8939  total_loss: 2.13  loss_sem_seg: 0.9919  loss_center: 0.5155  loss_offset: 0.5679  time: 0.3540  data_time: 0.0295  lr: 0.00033196  max_mem: 7418M
[12/11 20:45:09 d2.utils.events]:  eta: 0:06:08  iter: 8959  total_loss: 2.023  loss_sem_seg: 0.9406  loss_center: 0.4335  loss_offset: 0.5866  time: 0.3540  data_time: 0.0289  lr: 0.00032632  max_mem: 7418M
[12/11 20:45:16 d2.utils.events]:  eta: 0:06:01  iter: 8979  total_loss: 1.844  loss_sem_seg: 0.8328  loss_center: 0.517  loss_offset: 0.4289  time: 0.3540  data_time: 0.0288  lr: 0.00032067  max_mem: 7418M
[12/11 20:45:23 d2.utils.events]:  eta: 0:05:54  iter: 8999  total_loss: 1.905  loss_sem_seg: 0.8093  loss_center: 0.5134  loss_offset: 0.6015  time: 0.3540  data_time: 0.0292  lr: 0.00031501  max_mem: 7418M
[12/11 20:45:31 d2.utils.events]:  eta: 0:05:46  iter: 9019  total_loss: 2.015  loss_sem_seg: 0.974  loss_center: 0.4545  loss_offset: 0.5345  time: 0.3540  data_time: 0.0312  lr: 0.00030934  max_mem: 7418M
[12/11 20:45:38 d2.utils.events]:  eta: 0:05:39  iter: 9039  total_loss: 1.909  loss_sem_seg: 0.8644  loss_center: 0.4863  loss_offset: 0.5379  time: 0.3540  data_time: 0.0290  lr: 0.00030366  max_mem: 7418M
[12/11 20:45:45 d2.utils.events]:  eta: 0:05:32  iter: 9059  total_loss: 1.775  loss_sem_seg: 0.8341  loss_center: 0.4866  loss_offset: 0.5016  time: 0.3540  data_time: 0.0309  lr: 0.00029797  max_mem: 7418M
[12/11 20:45:52 d2.utils.events]:  eta: 0:05:25  iter: 9079  total_loss: 2.118  loss_sem_seg: 0.947  loss_center: 0.6108  loss_offset: 0.5929  time: 0.3540  data_time: 0.0294  lr: 0.00029226  max_mem: 7418M
[12/11 20:45:59 d2.utils.events]:  eta: 0:05:18  iter: 9099  total_loss: 1.921  loss_sem_seg: 0.9271  loss_center: 0.4758  loss_offset: 0.5043  time: 0.3540  data_time: 0.0298  lr: 0.00028654  max_mem: 7418M
[12/11 20:46:06 d2.utils.events]:  eta: 0:05:11  iter: 9119  total_loss: 2.037  loss_sem_seg: 0.9259  loss_center: 0.5021  loss_offset: 0.521  time: 0.3540  data_time: 0.0308  lr: 0.00028081  max_mem: 7418M
[12/11 20:46:13 d2.utils.events]:  eta: 0:05:04  iter: 9139  total_loss: 2.062  loss_sem_seg: 1.019  loss_center: 0.5436  loss_offset: 0.5228  time: 0.3540  data_time: 0.0300  lr: 0.00027507  max_mem: 7418M
[12/11 20:46:21 d2.utils.events]:  eta: 0:04:57  iter: 9159  total_loss: 1.784  loss_sem_seg: 0.795  loss_center: 0.401  loss_offset: 0.4976  time: 0.3541  data_time: 0.0300  lr: 0.00026931  max_mem: 7418M
[12/11 20:46:28 d2.utils.events]:  eta: 0:04:50  iter: 9179  total_loss: 2.125  loss_sem_seg: 0.9892  loss_center: 0.4833  loss_offset: 0.5721  time: 0.3541  data_time: 0.0305  lr: 0.00026354  max_mem: 7418M
[12/11 20:46:35 d2.utils.events]:  eta: 0:04:43  iter: 9199  total_loss: 2.09  loss_sem_seg: 0.9067  loss_center: 0.4679  loss_offset: 0.452  time: 0.3541  data_time: 0.0296  lr: 0.00025776  max_mem: 7418M
[12/11 20:46:42 d2.utils.events]:  eta: 0:04:36  iter: 9219  total_loss: 1.771  loss_sem_seg: 0.86  loss_center: 0.5723  loss_offset: 0.565  time: 0.3541  data_time: 0.0290  lr: 0.00025196  max_mem: 7418M
[12/11 20:46:49 d2.utils.events]:  eta: 0:04:29  iter: 9239  total_loss: 1.839  loss_sem_seg: 0.8874  loss_center: 0.4483  loss_offset: 0.5007  time: 0.3541  data_time: 0.0282  lr: 0.00024614  max_mem: 7418M
[12/11 20:46:56 d2.utils.events]:  eta: 0:04:22  iter: 9259  total_loss: 2.027  loss_sem_seg: 0.9737  loss_center: 0.4796  loss_offset: 0.5114  time: 0.3541  data_time: 0.0285  lr: 0.00024031  max_mem: 7418M
[12/11 20:47:03 d2.utils.events]:  eta: 0:04:15  iter: 9279  total_loss: 1.998  loss_sem_seg: 0.939  loss_center: 0.7001  loss_offset: 0.5158  time: 0.3541  data_time: 0.0301  lr: 0.00023447  max_mem: 7418M
[12/11 20:47:10 d2.utils.events]:  eta: 0:04:08  iter: 9299  total_loss: 1.677  loss_sem_seg: 0.8095  loss_center: 0.5003  loss_offset: 0.4423  time: 0.3541  data_time: 0.0297  lr: 0.00022861  max_mem: 7418M
[12/11 20:47:18 d2.utils.events]:  eta: 0:04:01  iter: 9319  total_loss: 2.181  loss_sem_seg: 1.005  loss_center: 0.6707  loss_offset: 0.5344  time: 0.3541  data_time: 0.0302  lr: 0.00022273  max_mem: 7418M
[12/11 20:47:25 d2.utils.events]:  eta: 0:03:54  iter: 9339  total_loss: 2.174  loss_sem_seg: 0.967  loss_center: 0.5084  loss_offset: 0.5183  time: 0.3541  data_time: 0.0308  lr: 0.00021683  max_mem: 7418M
[12/11 20:47:32 d2.utils.events]:  eta: 0:03:46  iter: 9359  total_loss: 2.207  loss_sem_seg: 0.8725  loss_center: 0.6092  loss_offset: 0.5688  time: 0.3541  data_time: 0.0288  lr: 0.00021092  max_mem: 7418M
[12/11 20:47:39 d2.utils.events]:  eta: 0:03:39  iter: 9379  total_loss: 2.251  loss_sem_seg: 1.196  loss_center: 0.4217  loss_offset: 0.5283  time: 0.3541  data_time: 0.0300  lr: 0.00020499  max_mem: 7418M
[12/11 20:47:46 d2.utils.events]:  eta: 0:03:32  iter: 9399  total_loss: 1.935  loss_sem_seg: 1.006  loss_center: 0.3689  loss_offset: 0.5453  time: 0.3541  data_time: 0.0297  lr: 0.00019903  max_mem: 7418M
[12/11 20:47:53 d2.utils.events]:  eta: 0:03:25  iter: 9419  total_loss: 2.283  loss_sem_seg: 1.082  loss_center: 0.6186  loss_offset: 0.4674  time: 0.3541  data_time: 0.0280  lr: 0.00019306  max_mem: 7418M
[12/11 20:48:00 d2.utils.events]:  eta: 0:03:18  iter: 9439  total_loss: 1.86  loss_sem_seg: 0.8392  loss_center: 0.4415  loss_offset: 0.4614  time: 0.3541  data_time: 0.0299  lr: 0.00018707  max_mem: 7418M
[12/11 20:48:08 d2.utils.events]:  eta: 0:03:11  iter: 9459  total_loss: 1.97  loss_sem_seg: 0.9014  loss_center: 0.5293  loss_offset: 0.535  time: 0.3541  data_time: 0.0315  lr: 0.00018106  max_mem: 7418M
[12/11 20:48:15 d2.utils.events]:  eta: 0:03:04  iter: 9479  total_loss: 1.994  loss_sem_seg: 0.863  loss_center: 0.5235  loss_offset: 0.3754  time: 0.3541  data_time: 0.0298  lr: 0.00017502  max_mem: 7418M
[12/11 20:48:22 d2.utils.events]:  eta: 0:02:57  iter: 9499  total_loss: 1.901  loss_sem_seg: 0.9502  loss_center: 0.5471  loss_offset: 0.5359  time: 0.3541  data_time: 0.0289  lr: 0.00016896  max_mem: 7418M
[12/11 20:48:29 d2.utils.events]:  eta: 0:02:50  iter: 9519  total_loss: 2.079  loss_sem_seg: 1.015  loss_center: 0.4722  loss_offset: 0.5518  time: 0.3541  data_time: 0.0305  lr: 0.00016288  max_mem: 7418M
[12/11 20:48:36 d2.utils.events]:  eta: 0:02:43  iter: 9539  total_loss: 2.027  loss_sem_seg: 0.8675  loss_center: 0.5419  loss_offset: 0.5415  time: 0.3541  data_time: 0.0286  lr: 0.00015677  max_mem: 7418M
[12/11 20:48:43 d2.utils.events]:  eta: 0:02:36  iter: 9559  total_loss: 1.941  loss_sem_seg: 0.902  loss_center: 0.5423  loss_offset: 0.5037  time: 0.3541  data_time: 0.0290  lr: 0.00015064  max_mem: 7418M
[12/11 20:48:50 d2.utils.events]:  eta: 0:02:29  iter: 9579  total_loss: 2.154  loss_sem_seg: 0.8444  loss_center: 0.5184  loss_offset: 0.5691  time: 0.3541  data_time: 0.0274  lr: 0.00014448  max_mem: 7418M
[12/11 20:48:57 d2.utils.events]:  eta: 0:02:21  iter: 9599  total_loss: 2.047  loss_sem_seg: 1.05  loss_center: 0.4785  loss_offset: 0.5243  time: 0.3541  data_time: 0.0292  lr: 0.00013828  max_mem: 7418M
[12/11 20:49:04 d2.utils.events]:  eta: 0:02:14  iter: 9619  total_loss: 1.904  loss_sem_seg: 0.8894  loss_center: 0.4859  loss_offset: 0.4505  time: 0.3541  data_time: 0.0303  lr: 0.00013206  max_mem: 7418M
[12/11 20:49:12 d2.utils.events]:  eta: 0:02:07  iter: 9639  total_loss: 1.837  loss_sem_seg: 0.8193  loss_center: 0.4268  loss_offset: 0.5029  time: 0.3541  data_time: 0.0303  lr: 0.0001258  max_mem: 7418M
[12/11 20:49:19 d2.utils.events]:  eta: 0:02:00  iter: 9659  total_loss: 1.981  loss_sem_seg: 0.8754  loss_center: 0.3891  loss_offset: 0.5159  time: 0.3541  data_time: 0.0307  lr: 0.00011951  max_mem: 7418M
[12/11 20:49:26 d2.utils.events]:  eta: 0:01:53  iter: 9679  total_loss: 1.778  loss_sem_seg: 0.8934  loss_center: 0.4711  loss_offset: 0.5301  time: 0.3541  data_time: 0.0296  lr: 0.00011319  max_mem: 7418M
[12/11 20:49:33 d2.utils.events]:  eta: 0:01:46  iter: 9699  total_loss: 1.838  loss_sem_seg: 0.8122  loss_center: 0.4602  loss_offset: 0.4314  time: 0.3541  data_time: 0.0331  lr: 0.00010682  max_mem: 7418M
[12/11 20:49:40 d2.utils.events]:  eta: 0:01:39  iter: 9719  total_loss: 2.053  loss_sem_seg: 0.8905  loss_center: 0.4103  loss_offset: 0.5855  time: 0.3542  data_time: 0.0300  lr: 0.00010041  max_mem: 7418M
[12/11 20:49:47 d2.utils.events]:  eta: 0:01:32  iter: 9739  total_loss: 1.738  loss_sem_seg: 0.8704  loss_center: 0.4621  loss_offset: 0.4281  time: 0.3542  data_time: 0.0295  lr: 9.3954e-05  max_mem: 7418M
[12/11 20:49:55 d2.utils.events]:  eta: 0:01:25  iter: 9759  total_loss: 2.163  loss_sem_seg: 0.82  loss_center: 0.5229  loss_offset: 0.4677  time: 0.3542  data_time: 0.0279  lr: 8.7449e-05  max_mem: 7418M
[12/11 20:50:02 d2.utils.events]:  eta: 0:01:18  iter: 9779  total_loss: 2.242  loss_sem_seg: 1.079  loss_center: 0.4856  loss_offset: 0.7088  time: 0.3542  data_time: 0.0299  lr: 8.089e-05  max_mem: 7418M
[12/11 20:50:09 d2.utils.events]:  eta: 0:01:11  iter: 9799  total_loss: 2.022  loss_sem_seg: 0.8283  loss_center: 0.5135  loss_offset: 0.5454  time: 0.3542  data_time: 0.0296  lr: 7.4271e-05  max_mem: 7418M
[12/11 20:50:16 d2.utils.events]:  eta: 0:01:04  iter: 9819  total_loss: 2.034  loss_sem_seg: 0.8742  loss_center: 0.6064  loss_offset: 0.5891  time: 0.3542  data_time: 0.0291  lr: 6.7585e-05  max_mem: 7418M
[12/11 20:50:23 d2.utils.events]:  eta: 0:00:56  iter: 9839  total_loss: 1.954  loss_sem_seg: 0.8305  loss_center: 0.4799  loss_offset: 0.4474  time: 0.3542  data_time: 0.0292  lr: 6.0825e-05  max_mem: 7418M
[12/11 20:50:30 d2.utils.events]:  eta: 0:00:49  iter: 9859  total_loss: 1.834  loss_sem_seg: 0.7912  loss_center: 0.5661  loss_offset: 0.4712  time: 0.3542  data_time: 0.0284  lr: 5.3981e-05  max_mem: 7418M
[12/11 20:50:37 d2.utils.events]:  eta: 0:00:42  iter: 9879  total_loss: 1.926  loss_sem_seg: 0.8905  loss_center: 0.4337  loss_offset: 0.5493  time: 0.3542  data_time: 0.0286  lr: 4.7038e-05  max_mem: 7418M
[12/11 20:50:44 d2.utils.events]:  eta: 0:00:35  iter: 9899  total_loss: 1.842  loss_sem_seg: 0.7385  loss_center: 0.479  loss_offset: 0.4403  time: 0.3542  data_time: 0.0286  lr: 3.9979e-05  max_mem: 7418M
[12/11 20:50:51 d2.utils.events]:  eta: 0:00:28  iter: 9919  total_loss: 1.744  loss_sem_seg: 0.7738  loss_center: 0.4283  loss_offset: 0.4607  time: 0.3542  data_time: 0.0324  lr: 3.2778e-05  max_mem: 7418M
[12/11 20:50:59 d2.utils.events]:  eta: 0:00:21  iter: 9939  total_loss: 1.915  loss_sem_seg: 0.8607  loss_center: 0.4429  loss_offset: 0.4618  time: 0.3542  data_time: 0.0276  lr: 2.5394e-05  max_mem: 7418M
[12/11 20:51:06 d2.utils.events]:  eta: 0:00:14  iter: 9959  total_loss: 1.9  loss_sem_seg: 0.813  loss_center: 0.4535  loss_offset: 0.4457  time: 0.3542  data_time: 0.0315  lr: 1.776e-05  max_mem: 7418M
[12/11 20:51:13 d2.utils.events]:  eta: 0:00:07  iter: 9979  total_loss: 1.869  loss_sem_seg: 0.6852  loss_center: 0.5776  loss_offset: 0.4162  time: 0.3542  data_time: 0.0298  lr: 9.7261e-06  max_mem: 7418M
[12/11 20:51:20 fvcore.common.checkpoint]: Saving checkpoint to ./output/model_0009999.pth
[12/11 20:51:21 fvcore.common.checkpoint]: Saving checkpoint to ./output/model_final.pth
[12/11 20:51:22 d2.utils.events]:  eta: 0:00:00  iter: 9999  total_loss: 1.969  loss_sem_seg: 0.8631  loss_center: 0.61  loss_offset: 0.5017  time: 0.3542  data_time: 0.0302  lr: 6.2797e-07  max_mem: 7418M
[12/11 20:51:22 d2.engine.hooks]: Overall training speed: 9998 iterations in 0:59:01 (0.3542 s / it)
[12/11 20:51:22 d2.engine.hooks]: Total training time: 0:59:10 (0:00:08 on hooks)
[12/11 20:51:23 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=640, sample_style='choice')]
[12/11 20:51:23 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>
[12/11 20:51:23 d2.data.common]: Serializing 5000 elements to byte tensors and concatenating them all ...
[12/11 20:51:23 d2.data.common]: Serialized dataset takes 3.40 MiB
/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
[12/11 20:51:24 d2.evaluation.evaluator]: Start inference on 5000 batches
/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
[12/11 20:51:26 d2.evaluation.evaluator]: Inference done 11/5000. Dataloading: 0.0003 s/iter. Inference: 0.0699 s/iter. Eval: 0.0431 s/iter. Total: 0.1132 s/iter. ETA=0:09:24
[12/11 20:51:31 d2.evaluation.evaluator]: Inference done 61/5000. Dataloading: 0.0012 s/iter. Inference: 0.0654 s/iter. Eval: 0.0351 s/iter. Total: 0.1018 s/iter. ETA=0:08:22
[12/11 20:51:36 d2.evaluation.evaluator]: Inference done 109/5000. Dataloading: 0.0013 s/iter. Inference: 0.0654 s/iter. Eval: 0.0367 s/iter. Total: 0.1034 s/iter. ETA=0:08:25
[12/11 20:51:41 d2.evaluation.evaluator]: Inference done 156/5000. Dataloading: 0.0013 s/iter. Inference: 0.0658 s/iter. Eval: 0.0372 s/iter. Total: 0.1044 s/iter. ETA=0:08:25
[12/11 20:51:46 d2.evaluation.evaluator]: Inference done 204/5000. Dataloading: 0.0014 s/iter. Inference: 0.0657 s/iter. Eval: 0.0372 s/iter. Total: 0.1044 s/iter. ETA=0:08:20
[12/11 20:51:51 d2.evaluation.evaluator]: Inference done 250/5000. Dataloading: 0.0014 s/iter. Inference: 0.0669 s/iter. Eval: 0.0370 s/iter. Total: 0.1053 s/iter. ETA=0:08:20
[12/11 20:51:56 d2.evaluation.evaluator]: Inference done 298/5000. Dataloading: 0.0014 s/iter. Inference: 0.0668 s/iter. Eval: 0.0370 s/iter. Total: 0.1052 s/iter. ETA=0:08:14
[12/11 20:52:01 d2.evaluation.evaluator]: Inference done 345/5000. Dataloading: 0.0014 s/iter. Inference: 0.0668 s/iter. Eval: 0.0372 s/iter. Total: 0.1054 s/iter. ETA=0:08:10
[12/11 20:52:06 d2.evaluation.evaluator]: Inference done 395/5000. Dataloading: 0.0014 s/iter. Inference: 0.0665 s/iter. Eval: 0.0368 s/iter. Total: 0.1048 s/iter. ETA=0:08:02
[12/11 20:52:11 d2.evaluation.evaluator]: Inference done 446/5000. Dataloading: 0.0014 s/iter. Inference: 0.0662 s/iter. Eval: 0.0365 s/iter. Total: 0.1042 s/iter. ETA=0:07:54
[12/11 20:52:16 d2.evaluation.evaluator]: Inference done 496/5000. Dataloading: 0.0014 s/iter. Inference: 0.0659 s/iter. Eval: 0.0364 s/iter. Total: 0.1038 s/iter. ETA=0:07:47
[12/11 20:52:21 d2.evaluation.evaluator]: Inference done 543/5000. Dataloading: 0.0014 s/iter. Inference: 0.0661 s/iter. Eval: 0.0365 s/iter. Total: 0.1040 s/iter. ETA=0:07:43
[12/11 20:52:26 d2.evaluation.evaluator]: Inference done 591/5000. Dataloading: 0.0014 s/iter. Inference: 0.0661 s/iter. Eval: 0.0366 s/iter. Total: 0.1042 s/iter. ETA=0:07:39
[12/11 20:52:31 d2.evaluation.evaluator]: Inference done 640/5000. Dataloading: 0.0014 s/iter. Inference: 0.0661 s/iter. Eval: 0.0366 s/iter. Total: 0.1042 s/iter. ETA=0:07:34
[12/11 20:52:36 d2.evaluation.evaluator]: Inference done 687/5000. Dataloading: 0.0014 s/iter. Inference: 0.0661 s/iter. Eval: 0.0367 s/iter. Total: 0.1044 s/iter. ETA=0:07:30
[12/11 20:52:41 d2.evaluation.evaluator]: Inference done 736/5000. Dataloading: 0.0014 s/iter. Inference: 0.0661 s/iter. Eval: 0.0368 s/iter. Total: 0.1043 s/iter. ETA=0:07:24
[12/11 20:52:46 d2.evaluation.evaluator]: Inference done 785/5000. Dataloading: 0.0014 s/iter. Inference: 0.0659 s/iter. Eval: 0.0368 s/iter. Total: 0.1042 s/iter. ETA=0:07:19
[12/11 20:52:51 d2.evaluation.evaluator]: Inference done 835/5000. Dataloading: 0.0014 s/iter. Inference: 0.0658 s/iter. Eval: 0.0367 s/iter. Total: 0.1040 s/iter. ETA=0:07:12
[12/11 20:52:56 d2.evaluation.evaluator]: Inference done 886/5000. Dataloading: 0.0014 s/iter. Inference: 0.0656 s/iter. Eval: 0.0365 s/iter. Total: 0.1037 s/iter. ETA=0:07:06
[12/11 20:53:01 d2.evaluation.evaluator]: Inference done 937/5000. Dataloading: 0.0014 s/iter. Inference: 0.0655 s/iter. Eval: 0.0365 s/iter. Total: 0.1034 s/iter. ETA=0:07:00
[12/11 20:53:06 d2.evaluation.evaluator]: Inference done 985/5000. Dataloading: 0.0014 s/iter. Inference: 0.0654 s/iter. Eval: 0.0365 s/iter. Total: 0.1035 s/iter. ETA=0:06:55
[12/11 20:53:12 d2.evaluation.evaluator]: Inference done 1035/5000. Dataloading: 0.0014 s/iter. Inference: 0.0654 s/iter. Eval: 0.0365 s/iter. Total: 0.1033 s/iter. ETA=0:06:49
[12/11 20:53:17 d2.evaluation.evaluator]: Inference done 1085/5000. Dataloading: 0.0014 s/iter. Inference: 0.0654 s/iter. Eval: 0.0364 s/iter. Total: 0.1033 s/iter. ETA=0:06:44
[12/11 20:53:22 d2.evaluation.evaluator]: Inference done 1136/5000. Dataloading: 0.0014 s/iter. Inference: 0.0652 s/iter. Eval: 0.0363 s/iter. Total: 0.1030 s/iter. ETA=0:06:38
[12/11 20:53:27 d2.evaluation.evaluator]: Inference done 1185/5000. Dataloading: 0.0014 s/iter. Inference: 0.0652 s/iter. Eval: 0.0363 s/iter. Total: 0.1030 s/iter. ETA=0:06:33
[12/11 20:53:32 d2.evaluation.evaluator]: Inference done 1232/5000. Dataloading: 0.0014 s/iter. Inference: 0.0653 s/iter. Eval: 0.0364 s/iter. Total: 0.1032 s/iter. ETA=0:06:28
[12/11 20:53:37 d2.evaluation.evaluator]: Inference done 1280/5000. Dataloading: 0.0014 s/iter. Inference: 0.0654 s/iter. Eval: 0.0364 s/iter. Total: 0.1033 s/iter. ETA=0:06:24
[12/11 20:53:42 d2.evaluation.evaluator]: Inference done 1330/5000. Dataloading: 0.0014 s/iter. Inference: 0.0653 s/iter. Eval: 0.0364 s/iter. Total: 0.1032 s/iter. ETA=0:06:18
[12/11 20:53:47 d2.evaluation.evaluator]: Inference done 1380/5000. Dataloading: 0.0014 s/iter. Inference: 0.0652 s/iter. Eval: 0.0364 s/iter. Total: 0.1031 s/iter. ETA=0:06:13
[12/11 20:53:52 d2.evaluation.evaluator]: Inference done 1429/5000. Dataloading: 0.0014 s/iter. Inference: 0.0652 s/iter. Eval: 0.0364 s/iter. Total: 0.1031 s/iter. ETA=0:06:08
[12/11 20:53:57 d2.evaluation.evaluator]: Inference done 1477/5000. Dataloading: 0.0014 s/iter. Inference: 0.0652 s/iter. Eval: 0.0365 s/iter. Total: 0.1032 s/iter. ETA=0:06:03
[12/11 20:54:02 d2.evaluation.evaluator]: Inference done 1526/5000. Dataloading: 0.0014 s/iter. Inference: 0.0652 s/iter. Eval: 0.0365 s/iter. Total: 0.1032 s/iter. ETA=0:05:58
[12/11 20:54:07 d2.evaluation.evaluator]: Inference done 1575/5000. Dataloading: 0.0014 s/iter. Inference: 0.0652 s/iter. Eval: 0.0365 s/iter. Total: 0.1032 s/iter. ETA=0:05:53
[12/11 20:54:12 d2.evaluation.evaluator]: Inference done 1622/5000. Dataloading: 0.0014 s/iter. Inference: 0.0652 s/iter. Eval: 0.0366 s/iter. Total: 0.1033 s/iter. ETA=0:05:49
[12/11 20:54:17 d2.evaluation.evaluator]: Inference done 1670/5000. Dataloading: 0.0014 s/iter. Inference: 0.0653 s/iter. Eval: 0.0366 s/iter. Total: 0.1034 s/iter. ETA=0:05:44
[12/11 20:54:22 d2.evaluation.evaluator]: Inference done 1718/5000. Dataloading: 0.0014 s/iter. Inference: 0.0653 s/iter. Eval: 0.0366 s/iter. Total: 0.1034 s/iter. ETA=0:05:39
[12/11 20:54:27 d2.evaluation.evaluator]: Inference done 1769/5000. Dataloading: 0.0014 s/iter. Inference: 0.0652 s/iter. Eval: 0.0366 s/iter. Total: 0.1033 s/iter. ETA=0:05:33
[12/11 20:54:32 d2.evaluation.evaluator]: Inference done 1817/5000. Dataloading: 0.0014 s/iter. Inference: 0.0653 s/iter. Eval: 0.0366 s/iter. Total: 0.1033 s/iter. ETA=0:05:28
[12/11 20:54:37 d2.evaluation.evaluator]: Inference done 1865/5000. Dataloading: 0.0014 s/iter. Inference: 0.0653 s/iter. Eval: 0.0366 s/iter. Total: 0.1034 s/iter. ETA=0:05:24
[12/11 20:54:42 d2.evaluation.evaluator]: Inference done 1914/5000. Dataloading: 0.0014 s/iter. Inference: 0.0653 s/iter. Eval: 0.0366 s/iter. Total: 0.1034 s/iter. ETA=0:05:19
[12/11 20:54:47 d2.evaluation.evaluator]: Inference done 1964/5000. Dataloading: 0.0014 s/iter. Inference: 0.0653 s/iter. Eval: 0.0365 s/iter. Total: 0.1033 s/iter. ETA=0:05:13
[12/11 20:54:52 d2.evaluation.evaluator]: Inference done 2012/5000. Dataloading: 0.0014 s/iter. Inference: 0.0653 s/iter. Eval: 0.0366 s/iter. Total: 0.1033 s/iter. ETA=0:05:08
[12/11 20:54:58 d2.evaluation.evaluator]: Inference done 2060/5000. Dataloading: 0.0014 s/iter. Inference: 0.0653 s/iter. Eval: 0.0366 s/iter. Total: 0.1034 s/iter. ETA=0:05:04
[12/11 20:55:03 d2.evaluation.evaluator]: Inference done 2109/5000. Dataloading: 0.0014 s/iter. Inference: 0.0653 s/iter. Eval: 0.0366 s/iter. Total: 0.1034 s/iter. ETA=0:04:58
[12/11 20:55:08 d2.evaluation.evaluator]: Inference done 2159/5000. Dataloading: 0.0014 s/iter. Inference: 0.0653 s/iter. Eval: 0.0366 s/iter. Total: 0.1034 s/iter. ETA=0:04:53
[12/11 20:55:13 d2.evaluation.evaluator]: Inference done 2207/5000. Dataloading: 0.0014 s/iter. Inference: 0.0653 s/iter. Eval: 0.0366 s/iter. Total: 0.1034 s/iter. ETA=0:04:48
[12/11 20:55:18 d2.evaluation.evaluator]: Inference done 2258/5000. Dataloading: 0.0014 s/iter. Inference: 0.0653 s/iter. Eval: 0.0365 s/iter. Total: 0.1033 s/iter. ETA=0:04:43
[12/11 20:55:23 d2.evaluation.evaluator]: Inference done 2308/5000. Dataloading: 0.0014 s/iter. Inference: 0.0653 s/iter. Eval: 0.0365 s/iter. Total: 0.1033 s/iter. ETA=0:04:38
[12/11 20:55:28 d2.evaluation.evaluator]: Inference done 2360/5000. Dataloading: 0.0014 s/iter. Inference: 0.0652 s/iter. Eval: 0.0364 s/iter. Total: 0.1032 s/iter. ETA=0:04:32
[12/11 20:55:33 d2.evaluation.evaluator]: Inference done 2407/5000. Dataloading: 0.0014 s/iter. Inference: 0.0653 s/iter. Eval: 0.0365 s/iter. Total: 0.1033 s/iter. ETA=0:04:27
[12/11 20:55:38 d2.evaluation.evaluator]: Inference done 2455/5000. Dataloading: 0.0014 s/iter. Inference: 0.0653 s/iter. Eval: 0.0365 s/iter. Total: 0.1033 s/iter. ETA=0:04:22
[12/11 20:55:43 d2.evaluation.evaluator]: Inference done 2504/5000. Dataloading: 0.0014 s/iter. Inference: 0.0653 s/iter. Eval: 0.0365 s/iter. Total: 0.1033 s/iter. ETA=0:04:17
[12/11 20:55:48 d2.evaluation.evaluator]: Inference done 2553/5000. Dataloading: 0.0014 s/iter. Inference: 0.0653 s/iter. Eval: 0.0365 s/iter. Total: 0.1033 s/iter. ETA=0:04:12
[12/11 20:55:53 d2.evaluation.evaluator]: Inference done 2602/5000. Dataloading: 0.0014 s/iter. Inference: 0.0653 s/iter. Eval: 0.0365 s/iter. Total: 0.1033 s/iter. ETA=0:04:07
[12/11 20:55:59 d2.evaluation.evaluator]: Inference done 2650/5000. Dataloading: 0.0014 s/iter. Inference: 0.0654 s/iter. Eval: 0.0365 s/iter. Total: 0.1034 s/iter. ETA=0:04:02
[12/11 20:56:04 d2.evaluation.evaluator]: Inference done 2702/5000. Dataloading: 0.0014 s/iter. Inference: 0.0653 s/iter. Eval: 0.0365 s/iter. Total: 0.1032 s/iter. ETA=0:03:57
[12/11 20:56:09 d2.evaluation.evaluator]: Inference done 2752/5000. Dataloading: 0.0014 s/iter. Inference: 0.0652 s/iter. Eval: 0.0365 s/iter. Total: 0.1032 s/iter. ETA=0:03:51
[12/11 20:56:14 d2.evaluation.evaluator]: Inference done 2801/5000. Dataloading: 0.0014 s/iter. Inference: 0.0652 s/iter. Eval: 0.0365 s/iter. Total: 0.1032 s/iter. ETA=0:03:46
[12/11 20:56:19 d2.evaluation.evaluator]: Inference done 2850/5000. Dataloading: 0.0014 s/iter. Inference: 0.0652 s/iter. Eval: 0.0365 s/iter. Total: 0.1032 s/iter. ETA=0:03:41
[12/11 20:56:24 d2.evaluation.evaluator]: Inference done 2900/5000. Dataloading: 0.0014 s/iter. Inference: 0.0652 s/iter. Eval: 0.0364 s/iter. Total: 0.1031 s/iter. ETA=0:03:36
[12/11 20:56:29 d2.evaluation.evaluator]: Inference done 2949/5000. Dataloading: 0.0014 s/iter. Inference: 0.0652 s/iter. Eval: 0.0365 s/iter. Total: 0.1031 s/iter. ETA=0:03:31
[12/11 20:56:34 d2.evaluation.evaluator]: Inference done 3000/5000. Dataloading: 0.0014 s/iter. Inference: 0.0651 s/iter. Eval: 0.0364 s/iter. Total: 0.1031 s/iter. ETA=0:03:26
[12/11 20:56:39 d2.evaluation.evaluator]: Inference done 3051/5000. Dataloading: 0.0014 s/iter. Inference: 0.0651 s/iter. Eval: 0.0364 s/iter. Total: 0.1030 s/iter. ETA=0:03:20
[12/11 20:56:44 d2.evaluation.evaluator]: Inference done 3103/5000. Dataloading: 0.0014 s/iter. Inference: 0.0650 s/iter. Eval: 0.0364 s/iter. Total: 0.1029 s/iter. ETA=0:03:15
[12/11 20:56:49 d2.evaluation.evaluator]: Inference done 3151/5000. Dataloading: 0.0014 s/iter. Inference: 0.0650 s/iter. Eval: 0.0364 s/iter. Total: 0.1029 s/iter. ETA=0:03:10
[12/11 20:56:54 d2.evaluation.evaluator]: Inference done 3201/5000. Dataloading: 0.0014 s/iter. Inference: 0.0650 s/iter. Eval: 0.0364 s/iter. Total: 0.1029 s/iter. ETA=0:03:05
[12/11 20:56:59 d2.evaluation.evaluator]: Inference done 3248/5000. Dataloading: 0.0014 s/iter. Inference: 0.0650 s/iter. Eval: 0.0364 s/iter. Total: 0.1029 s/iter. ETA=0:03:00
[12/11 20:57:04 d2.evaluation.evaluator]: Inference done 3295/5000. Dataloading: 0.0014 s/iter. Inference: 0.0651 s/iter. Eval: 0.0364 s/iter. Total: 0.1030 s/iter. ETA=0:02:55
[12/11 20:57:09 d2.evaluation.evaluator]: Inference done 3343/5000. Dataloading: 0.0014 s/iter. Inference: 0.0651 s/iter. Eval: 0.0364 s/iter. Total: 0.1030 s/iter. ETA=0:02:50
[12/11 20:57:14 d2.evaluation.evaluator]: Inference done 3391/5000. Dataloading: 0.0014 s/iter. Inference: 0.0651 s/iter. Eval: 0.0365 s/iter. Total: 0.1031 s/iter. ETA=0:02:45
[12/11 20:57:19 d2.evaluation.evaluator]: Inference done 3439/5000. Dataloading: 0.0014 s/iter. Inference: 0.0651 s/iter. Eval: 0.0365 s/iter. Total: 0.1031 s/iter. ETA=0:02:40
[12/11 20:57:24 d2.evaluation.evaluator]: Inference done 3491/5000. Dataloading: 0.0014 s/iter. Inference: 0.0650 s/iter. Eval: 0.0365 s/iter. Total: 0.1030 s/iter. ETA=0:02:35
[12/11 20:57:29 d2.evaluation.evaluator]: Inference done 3540/5000. Dataloading: 0.0014 s/iter. Inference: 0.0650 s/iter. Eval: 0.0365 s/iter. Total: 0.1030 s/iter. ETA=0:02:30
[12/11 20:57:34 d2.evaluation.evaluator]: Inference done 3588/5000. Dataloading: 0.0014 s/iter. Inference: 0.0651 s/iter. Eval: 0.0365 s/iter. Total: 0.1030 s/iter. ETA=0:02:25
[12/11 20:57:39 d2.evaluation.evaluator]: Inference done 3638/5000. Dataloading: 0.0014 s/iter. Inference: 0.0651 s/iter. Eval: 0.0365 s/iter. Total: 0.1030 s/iter. ETA=0:02:20
[12/11 20:57:44 d2.evaluation.evaluator]: Inference done 3688/5000. Dataloading: 0.0014 s/iter. Inference: 0.0650 s/iter. Eval: 0.0365 s/iter. Total: 0.1030 s/iter. ETA=0:02:15
[12/11 20:57:50 d2.evaluation.evaluator]: Inference done 3739/5000. Dataloading: 0.0014 s/iter. Inference: 0.0650 s/iter. Eval: 0.0364 s/iter. Total: 0.1030 s/iter. ETA=0:02:09
[12/11 20:57:55 d2.evaluation.evaluator]: Inference done 3787/5000. Dataloading: 0.0014 s/iter. Inference: 0.0651 s/iter. Eval: 0.0365 s/iter. Total: 0.1030 s/iter. ETA=0:02:04
[12/11 20:58:00 d2.evaluation.evaluator]: Inference done 3837/5000. Dataloading: 0.0014 s/iter. Inference: 0.0650 s/iter. Eval: 0.0364 s/iter. Total: 0.1030 s/iter. ETA=0:01:59
[12/11 20:58:05 d2.evaluation.evaluator]: Inference done 3887/5000. Dataloading: 0.0014 s/iter. Inference: 0.0650 s/iter. Eval: 0.0364 s/iter. Total: 0.1030 s/iter. ETA=0:01:54
[12/11 20:58:10 d2.evaluation.evaluator]: Inference done 3937/5000. Dataloading: 0.0014 s/iter. Inference: 0.0650 s/iter. Eval: 0.0364 s/iter. Total: 0.1029 s/iter. ETA=0:01:49
[12/11 20:58:15 d2.evaluation.evaluator]: Inference done 3988/5000. Dataloading: 0.0014 s/iter. Inference: 0.0650 s/iter. Eval: 0.0364 s/iter. Total: 0.1029 s/iter. ETA=0:01:44
[12/11 20:58:20 d2.evaluation.evaluator]: Inference done 4036/5000. Dataloading: 0.0014 s/iter. Inference: 0.0650 s/iter. Eval: 0.0364 s/iter. Total: 0.1029 s/iter. ETA=0:01:39
[12/11 20:58:25 d2.evaluation.evaluator]: Inference done 4083/5000. Dataloading: 0.0014 s/iter. Inference: 0.0650 s/iter. Eval: 0.0365 s/iter. Total: 0.1030 s/iter. ETA=0:01:34
[12/11 20:58:30 d2.evaluation.evaluator]: Inference done 4133/5000. Dataloading: 0.0014 s/iter. Inference: 0.0650 s/iter. Eval: 0.0364 s/iter. Total: 0.1030 s/iter. ETA=0:01:29
[12/11 20:58:35 d2.evaluation.evaluator]: Inference done 4179/5000. Dataloading: 0.0014 s/iter. Inference: 0.0651 s/iter. Eval: 0.0365 s/iter. Total: 0.1031 s/iter. ETA=0:01:24
[12/11 20:58:40 d2.evaluation.evaluator]: Inference done 4228/5000. Dataloading: 0.0014 s/iter. Inference: 0.0651 s/iter. Eval: 0.0365 s/iter. Total: 0.1030 s/iter. ETA=0:01:19
[12/11 20:58:45 d2.evaluation.evaluator]: Inference done 4276/5000. Dataloading: 0.0014 s/iter. Inference: 0.0651 s/iter. Eval: 0.0365 s/iter. Total: 0.1031 s/iter. ETA=0:01:14
[12/11 20:58:50 d2.evaluation.evaluator]: Inference done 4323/5000. Dataloading: 0.0014 s/iter. Inference: 0.0651 s/iter. Eval: 0.0365 s/iter. Total: 0.1031 s/iter. ETA=0:01:09
[12/11 20:58:55 d2.evaluation.evaluator]: Inference done 4375/5000. Dataloading: 0.0014 s/iter. Inference: 0.0651 s/iter. Eval: 0.0364 s/iter. Total: 0.1030 s/iter. ETA=0:01:04
[12/11 20:59:00 d2.evaluation.evaluator]: Inference done 4422/5000. Dataloading: 0.0014 s/iter. Inference: 0.0651 s/iter. Eval: 0.0364 s/iter. Total: 0.1031 s/iter. ETA=0:00:59
[12/11 20:59:05 d2.evaluation.evaluator]: Inference done 4471/5000. Dataloading: 0.0014 s/iter. Inference: 0.0651 s/iter. Eval: 0.0364 s/iter. Total: 0.1031 s/iter. ETA=0:00:54
[12/11 20:59:11 d2.evaluation.evaluator]: Inference done 4522/5000. Dataloading: 0.0014 s/iter. Inference: 0.0651 s/iter. Eval: 0.0364 s/iter. Total: 0.1030 s/iter. ETA=0:00:49
[12/11 20:59:16 d2.evaluation.evaluator]: Inference done 4570/5000. Dataloading: 0.0014 s/iter. Inference: 0.0651 s/iter. Eval: 0.0364 s/iter. Total: 0.1031 s/iter. ETA=0:00:44
[12/11 20:59:21 d2.evaluation.evaluator]: Inference done 4620/5000. Dataloading: 0.0014 s/iter. Inference: 0.0651 s/iter. Eval: 0.0364 s/iter. Total: 0.1030 s/iter. ETA=0:00:39
[12/11 20:59:26 d2.evaluation.evaluator]: Inference done 4668/5000. Dataloading: 0.0014 s/iter. Inference: 0.0652 s/iter. Eval: 0.0364 s/iter. Total: 0.1031 s/iter. ETA=0:00:34
[12/11 20:59:31 d2.evaluation.evaluator]: Inference done 4717/5000. Dataloading: 0.0014 s/iter. Inference: 0.0652 s/iter. Eval: 0.0364 s/iter. Total: 0.1031 s/iter. ETA=0:00:29
[12/11 20:59:36 d2.evaluation.evaluator]: Inference done 4767/5000. Dataloading: 0.0014 s/iter. Inference: 0.0651 s/iter. Eval: 0.0364 s/iter. Total: 0.1031 s/iter. ETA=0:00:24
[12/11 20:59:41 d2.evaluation.evaluator]: Inference done 4814/5000. Dataloading: 0.0014 s/iter. Inference: 0.0652 s/iter. Eval: 0.0364 s/iter. Total: 0.1031 s/iter. ETA=0:00:19
[12/11 20:59:46 d2.evaluation.evaluator]: Inference done 4864/5000. Dataloading: 0.0014 s/iter. Inference: 0.0652 s/iter. Eval: 0.0364 s/iter. Total: 0.1031 s/iter. ETA=0:00:14
[12/11 20:59:51 d2.evaluation.evaluator]: Inference done 4913/5000. Dataloading: 0.0014 s/iter. Inference: 0.0652 s/iter. Eval: 0.0364 s/iter. Total: 0.1031 s/iter. ETA=0:00:08
[12/11 20:59:56 d2.evaluation.evaluator]: Inference done 4962/5000. Dataloading: 0.0014 s/iter. Inference: 0.0652 s/iter. Eval: 0.0364 s/iter. Total: 0.1031 s/iter. ETA=0:00:03
[12/11 21:00:00 d2.evaluation.evaluator]: Total inference time: 0:08:35.199753 (0.103143 s / iter per device, on 1 devices)
[12/11 21:00:00 d2.evaluation.evaluator]: Total inference pure compute time: 0:05:25 (0.065162 s / iter per device, on 1 devices)
[12/11 21:00:00 d2.evaluation.panoptic_evaluation]: Writing all panoptic predictions to /tmp/panoptic_evaloqfox4hf ...
[12/11 21:00:26 d2.evaluation.panoptic_evaluation]: Panoptic Evaluation Results:
|        |   PQ   |   SQ   |   RQ   |  #categories  |
|:------:|:------:|:------:|:------:|:-------------:|
|  All   | 19.484 | 64.629 | 25.382 |      133      |
| Things | 19.691 | 67.609 | 25.705 |      80       |
| Stuff  | 19.172 | 60.131 | 24.895 |      53       |
[12/11 21:00:26 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...
[12/11 21:00:26 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json
[12/11 21:00:26 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...
Loading and preparing results...
DONE (t=0.04s)
creating index...
index created!
[12/11 21:00:27 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*
[12/11 21:00:35 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 8.73 seconds.
[12/11 21:00:35 d2.evaluation.fast_eval_api]: Accumulating evaluation results...
[12/11 21:00:36 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.68 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.078
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.149
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.072
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.012
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.065
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.132
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.095
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.128
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.129
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.101
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.225
[12/11 21:00:36 d2.evaluation.coco_evaluation]: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:-----:|:------:|:------:|:-----:|:-----:|:------:|
| 7.828 | 14.917 | 7.224  | 1.189 | 6.512 | 13.231 |
[12/11 21:00:36 d2.evaluation.coco_evaluation]: Per-category bbox AP: 
| category      | AP     | category     | AP     | category       | AP     |
|:--------------|:-------|:-------------|:-------|:---------------|:-------|
| person        | 14.873 | bicycle      | 5.652  | car            | 6.483  |
| motorcycle    | 9.562  | airplane     | 24.517 | bus            | 31.494 |
| train         | 32.504 | truck        | 4.922  | boat           | 2.333  |
| traffic light | 1.763  | fire hydrant | 29.973 | stop sign      | 27.361 |
| parking meter | 4.593  | bench        | 1.937  | bird           | 6.870  |
| cat           | 13.274 | dog          | 15.980 | horse          | 15.355 |
| sheep         | 11.202 | cow          | 9.027  | elephant       | 28.681 |
| bear          | 36.981 | zebra        | 34.717 | giraffe        | 29.858 |
| backpack      | 0.000  | umbrella     | 7.989  | handbag        | 0.000  |
| tie           | 0.099  | suitcase     | 2.583  | frisbee        | 14.679 |
| skis          | 1.253  | snowboard    | 0.264  | sports ball    | 4.426  |
| kite          | 8.270  | baseball bat | 0.301  | baseball glove | 2.688  |
| skateboard    | 5.269  | surfboard    | 3.932  | tennis racket  | 8.098  |
| bottle        | 0.767  | wine glass   | 0.255  | cup            | 2.057  |
| fork          | 0.062  | knife        | 0.020  | spoon          | 0.000  |
| bowl          | 2.427  | banana       | 1.575  | apple          | 0.933  |
| sandwich      | 0.788  | orange       | 2.559  | broccoli       | 1.356  |
| carrot        | 0.537  | hot dog      | 0.594  | pizza          | 5.431  |
| donut         | 3.729  | cake         | 0.679  | chair          | 2.632  |
| couch         | 14.712 | potted plant | 0.848  | bed            | 18.458 |
| dining table  | 2.503  | toilet       | 20.131 | tv             | 18.621 |
| laptop        | 13.016 | mouse        | 4.450  | remote         | 0.488  |
| keyboard      | 4.476  | cell phone   | 2.408  | microwave      | 7.264  |
| oven          | 4.374  | toaster      | 0.000  | sink           | 3.699  |
| refrigerator  | 9.304  | book         | 0.471  | clock          | 9.750  |
| vase          | 0.782  | scissors     | 0.000  | teddy bear     | 4.309  |
| hair drier    | 0.000  | toothbrush   | 0.000  |                |        |
Loading and preparing results...
DONE (t=0.47s)
creating index...
index created!
[12/11 21:00:37 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*
[12/11 21:00:47 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 9.95 seconds.
[12/11 21:00:47 d2.evaluation.fast_eval_api]: Accumulating evaluation results...
[12/11 21:00:48 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.74 seconds.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.081
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.160
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.073
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.064
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.173
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.095
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.123
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.124
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.008
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.100
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.228
[12/11 21:00:50 d2.evaluation.coco_evaluation]: Evaluation results for segm: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:-----:|:------:|:------:|:-----:|:-----:|:------:|
| 8.068 | 15.988 | 7.331  | 0.541 | 6.368 | 17.268 |
[12/11 21:00:50 d2.evaluation.coco_evaluation]: Per-category segm AP: 
| category      | AP     | category     | AP     | category       | AP     |
|:--------------|:-------|:-------------|:-------|:---------------|:-------|
| person        | 10.647 | bicycle      | 2.760  | car            | 6.451  |
| motorcycle    | 6.509  | airplane     | 18.011 | bus            | 34.026 |
| train         | 36.665 | truck        | 4.721  | boat           | 2.095  |
| traffic light | 2.401  | fire hydrant | 31.928 | stop sign      | 33.854 |
| parking meter | 9.081  | bench        | 1.470  | bird           | 4.332  |
| cat           | 18.135 | dog          | 16.346 | horse          | 11.217 |
| sheep         | 9.684  | cow          | 7.669  | elephant       | 26.862 |
| bear          | 38.367 | zebra        | 24.668 | giraffe        | 21.423 |
| backpack      | 0.000  | umbrella     | 11.650 | handbag        | 0.000  |
| tie           | 0.099  | suitcase     | 3.559  | frisbee        | 11.895 |
| skis          | 0.019  | snowboard    | 0.000  | sports ball    | 4.608  |
| kite          | 2.946  | baseball bat | 0.493  | baseball glove | 2.911  |
| skateboard    | 1.883  | surfboard    | 3.227  | tennis racket  | 14.878 |
| bottle        | 1.521  | wine glass   | 0.240  | cup            | 4.482  |
| fork          | 0.012  | knife        | 0.006  | spoon          | 0.000  |
| bowl          | 4.741  | banana       | 1.203  | apple          | 1.222  |
| sandwich      | 2.707  | orange       | 4.041  | broccoli       | 2.309  |
| carrot        | 0.360  | hot dog      | 0.990  | pizza          | 7.147  |
| donut         | 7.032  | cake         | 1.143  | chair          | 1.852  |
| couch         | 12.798 | potted plant | 0.940  | bed            | 15.398 |
| dining table  | 0.193  | toilet       | 29.682 | tv             | 24.175 |
| laptop        | 16.837 | mouse        | 6.888  | remote         | 0.706  |
| keyboard      | 8.226  | cell phone   | 2.858  | microwave      | 8.068  |
| oven          | 4.064  | toaster      | 0.000  | sink           | 5.556  |
| refrigerator  | 8.236  | book         | 0.135  | clock          | 11.762 |
| vase          | 3.247  | scissors     | 0.064  | teddy bear     | 6.816  |
| hair drier    | 0.000  | toothbrush   | 0.297  |                |        |
[12/11 21:00:55 d2.engine.defaults]: Evaluation results for coco_2017_val_panoptic in csv format:
[12/11 21:00:55 d2.evaluation.testing]: copypaste: Task: panoptic_seg
[12/11 21:00:55 d2.evaluation.testing]: copypaste: PQ,SQ,RQ,PQ_th,SQ_th,RQ_th,PQ_st,SQ_st,RQ_st
[12/11 21:00:55 d2.evaluation.testing]: copypaste: 19.4843,64.6291,25.3821,19.6910,67.6093,25.7049,19.1722,60.1308,24.8949
[12/11 21:00:55 d2.evaluation.testing]: copypaste: Task: bbox
[12/11 21:00:55 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl
[12/11 21:00:55 d2.evaluation.testing]: copypaste: 7.8278,14.9166,7.2237,1.1893,6.5115,13.2308
[12/11 21:00:55 d2.evaluation.testing]: copypaste: Task: segm
[12/11 21:00:55 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl
[12/11 21:00:55 d2.evaluation.testing]: copypaste: 8.0680,15.9881,7.3306,0.5411,6.3683,17.2676